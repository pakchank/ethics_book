# 설명 가능한 알고리즘

박찬경 (경북대학교)

인간이 많은 결정을 인공지능의 자동적인 판단에 더 많이 의존하게 될 수록, 해당 판단이 문제를 발생시켰을 때, 그 도덕적, 법적 책임을 어떻게 분배할 것인가라는 문제가 첨예하게 제기된다. 어떤 입장은 새로운 주체로서 인공지능 스스로가 가지는 자체의 권리와 책임에 대해 논의하기도 하지만, EU의 신뢰할 수 있느 인공지능(Trustworthy AI)와 같이 현실주의적 접근에서는 인공지능을 설계하고, 배포하며, 이용하는 인간에게 그 책임성을 부과하는 입장을 보이기도 한다. 어떠한 입장을 취하든, 우리가 인공지능과 더욱 더 안정적으로 공존할 수 있기 위해서는, 우리는 인공지능의 판단 실패에 대해 어떻게 윤리적, 법적으로 대처해야 하는가 하는 문제를 해결해야만 한다. 단적으로, 자율주행 자동차에서 사고가났을 때, 그리고 무인 공격 무기가 오인 사격했을 때..이러한 문제는 결코 인공지능의 판단이 인간보다 오류를 덜 발생시켜서 문제가 되는 것이 아니다 (오히려 현실은 그 반대에 가깝다). 그러한 실패가 일어났을 때, 우리는 인간 사이에, 그리고 인간과 인공지능 사이에 책임을 어떻게 분배하는 것이 옳은지에 대한 합의에 도달하지 못하였다. 

그런데, 책임의 분배에 앞서 먼저 해결되어야 하는 문제가 있다. 즉, 어디서 문제가 발생했는지를 알아야 한다. 일단 '인식'하여야 '도덕 판단'을 할 수 있다는 것은 자명하다. '인식'의 문제는 '도덕 판단'만큼인 어려운 문제다. 왜냐하면 예측력이 높을 수록 모형은 복잡해지기 때문이다. 따라서, 인공지능의 판단을 어떻게 설명할 것인가 하는 문제는 충분조건은 아닐지라도 필요조건이다.

사실 어떠한 판단, 또 자동 판단의 설명 요구는 새로운 일이 아니다. 왜냐하면, 예전에도 어떤 판단이 이루어졌을 때, 그에 대해 설명하라는 요구는 있었기 때문이다. 하지만, 인공지능은 그에 더하여 새로운 문제를 만들어낸다 (어떤..??) 이를 해결하는 위한 분야를 XAI라고 한다.
또 다시 원래 인간세계에 있었던 문제를 반복하면서 이를 더 복잡하게 만드는 패턴이 반복되고 있는 것 (차이와 반복.) -> 학제간 사고 방식
 
이 장에서는 XAI를 설명하고, 투명성, accountability, 기존의 설명 요구 법안과의 연관성 등이 설명될 것이다. 

## 설명이 쉬운 알고리즘. 
먼저, 인공지능의 근간이 되는 기계학습, 그 중에서도 지도학습(supervised learning) 방법은 분류(classification) 문제에 대해 복습해보도록 하자. 분류의 문제는 어떠한 특성(feature)을 이용해 타겟(target)을 예측하는 문제를 의미한다. 기계학습을 처음 배울 때 하는 이미지 특성을 이용해 개와 고양이를 자동으로 나누는 알고리즘, 손글씨 이미지를 이용해 숫자를 파악하는 알고리즘 같은 것들이 분류 알고리즘이다. 설명으 편의를 위해, 여기서는 그 보다 더 간단한 알고리즘을 생각해보자. 즉, 두 가지의 특성을 이용해 0 또는 1의 판단을 하는 알고리즘이다. 첫번째 특성을 $X_1$, 두번째 특성을 $X_2$라고 하자. 그리고 예측하고자 하는 $Y$는 0 또는 1의 값을 갖는데, 이를 예측하고자 하는 것이다. 비근한 예로, 소득($X_1$)과 신용점수($X_2$)를 이용해 대출을 상환할 수 있는지($Y=1$), 그렇지 않은지($Y=0$)를 판단하는 알고리즘을 만드는 상황 그런 경우이다.

이러한 경우에 적용할 수 있는 가장 간단한 분류 알고리즘 중 하나로 이야기되는 것 중 하나는 '로지스틱 회귀'라는 방식이다. 이는 선형성을 갖는다.

**로지스틱 회귀 그림**

이를 수식으로 표현하면 다음과 같다. 그림으로 봐도, 수식으로 봐도, 즉시 설명이 가능하다. 그림에서 기울기에 해당하니까..


그런데, 이런 간단한 분류 방법이 늘 현실적이지는 않을 것이다. 가장 먼저, 저 경계선이 곡선인 경우도 허용하고 싶을 것이다. 다음과 같이... 
**그림** 

또, 다음과 같이 1로 예측되는 영역이 분리되어 있는 경우도 있을 수 있다. 

**그림**

이는 로지스틱 회귀 분석과 같은 방법으로는 불가능하고, 이를 조금 더 복잡한 함수에 집어넣는다.

**수식**

이러한 방식 중 하나가 바로 우리가 잘 알고 있는 '인공신경망(Artificial Neural Networks; ANN)이다. 이제 저 안에 들어가 있는 모수는 기울기라는 단순한 해석을 가지지 않는다. 정확히 말하면, 저 모수를 설명할 방법이 없다! 이를 blackboxedness라고 한다. 

결론은 더 나은 예측을 위한 모형의 '유연성(flexibility)'를 얻기 위해 모형은 더 복잡해져야 하며, 그 댓가로 설명가능성이 떨어진다는 것이다. 즉, 모형의 예측력과 설명가능성 사이에는 트레이드오프 관계가 성리하는 것이다!

분류 알고리즘 중에는 로지스틱 회귀 말고도 본질적으로 설명하기 쉬운 모형들이 있다. 예컨대 의사결정 나무 같은 것이 그렇다.

**Decision Tree 그림**

의사결정 나무 같은 것은 에전 인공지능 시대의 근간 이기도 했다. 그러나 현대적인 인공적인 인공지능에 사용하는 분류 알고리즘은 이를 믹스한 것. Random Forest같은 경우... 따라서 다음과 같은 트레이드 오프를 보일 수 있다. 

**일본 트레이드 오프 그림**



## 설명 가는성을 높이는 기술: XAI
그렇다면, 예측력을 위해 우리는 설명을 포기해야 하는가? 그렇지 않다. 많은 인공지능 연구자들은 예측력과 복잡도를 보존하면서도 여전히 설명하기 위한 기술들을 개발하고 있다. 여기서 핵심적인 생각은, 에측을 위한 모형을 그대로 둔 채로, '설명을 위한 장치'를 개발하는 것이다.

model specific v.model agnostic
-> 이 장에서는 model agnostic한 애들만 다룰 것이다.

global v. local
-> 

## 설명가능성을 높이는 기술 
분류

### Global
### Local

## 더 읽을거리
- 인터넷 교재
- 인공지능 개론인가? (그 검은 책...)

인공지능의 실패 -> 예외상태 (슈미트, 발리바르, 등등등) -> 예외상태가 주체성을 결정하는 것이 아닐까.
노동. 

책임의 분배. 

인공지능의 책임성...도덕적으로나, 피해 보상으로나, 책임성을 따지는 것은 매우 어렵다.
하지만, 그 전에 전제되어야 하는 것이 있다. 어떠한 판단을 했는가 하는것. 그것이 책임있는 인공지능, 혹은 책임있는 인공지능과 인간의 네트워크의 충분조건은 될 수 없을 지언정, 필요조건인 것은 사실.


책임성 있는 인공지능... 

결정을 인공지능에 맡길 수록....
원래 결정은 모두 인간이 하지 않았다 - 사실은 이에 대해서 많은 법들이 있어왔다. 인간이 하든, 하지 않든...
하지만, 문제는 더욱 복잡해졌다는 것.

미리 모니터링...ex ante의 접근법.
global - 유지 보수를 위해서... security의 문제를 monitor하기 위해서...

또, 인간과의 협업을 위해서...



설명 가능한 인공지능 v. 설명을 위한 인공지능.