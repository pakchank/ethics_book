# 인간을 위한 인공지능의 기술적 탐구

*윤장렬 (국립부경대학교 미디어커뮤니케이션학과)*

AI 윤리는 인공지능 기술이 발전함에 따라 이해관계자들이 준수해야 할 보편적인 사회 규범을 말한다. 여기서 말하는 윤리(ethics)는 성격이나 관습을 뜻하는 그리스어인 에토스(ethos)에서 유래했다. 에토스의 사전적 의미는 “도덕적 가치에 대한 인식을 특징으로 하는 태도 또는 책임 있는 태도의 기초가 되는 윤리적 인식”이다.[^j-1] AI 기술이 발전하면서 도덕적 가치나 윤리적 문제가 대두되는 것은 AI 기술을 공정하게 사용하기 위한 것이다. AI 기술의 발전은 긍정적일 수 있지만, 상용화되면서 부작용과 함께 비판적 시각도 대두되고 있다. 이를테면, AI가 생산한 콘텐츠의 정확성이나, 저작권의 이익과 이해관계의 충돌, 그리고 기술 혁신과 저널리즘 사이에서 인간의 역할 등 다양한 물음이 제기되고 있다. 따라서 유럽연합(EU)뿐 아니라 미국, 중국, 일본 등 세계 각국은 AI 기술을 규제하기 위한 법안 마련에 속도를 내는 한편, AI 기술을 공정하게 사용하기 위해 각 기업, 학계, 정부에서 다양한 가이드라인을 만들어 공유하고 있다. 

[^j-1]: Duden online, https://www.duden.de/rechtschreibung/Ethos

AI 기술을 공정하게 사용하기 위해서는 더욱 객관적인 원칙이 필요하다. 왜냐하면, 공정이란 특정 사안을 판단하는 데 작용하는 정의와 불의에 관한 확고하고 즉각적 통찰이기 때문이다. 다시 말해 무엇이 공정하다고 판단하는 것은 지극히 주관적인 통찰과 판단에 근거하게 된다. 프랑스 철학자 앙드레 랄랑드(André Lalande)는 공정성의 개념을 위와 같이 정의하면서, 우리 사회의 공정은 산술적 평등을 포기하고 도덕적 철학의 가르침을 따르게 한다고 비판한 바 있다. 랄랑드에 따르면, 공정한 세상을 구현하기 위해서는 단순히 드러나는 역기능과 불의를 해소하는 것이 아니라 객관적인 원칙에서 동등한 권리가 요구되어야 한다. 즉 AI 윤리는 새로운 기술이 가져오는 혜택과 부작용 사이에서 즉각적인 통찰과 판단이 아니라, 보다 근본적이고 객관적인 원칙이 제시될 필요가 있다. 이러한 원칙은 새로운 기술에 대한 분명한 이해와 탐구에서 출발하며, 다양한 관점과 학제 간의 연구에서 인간과 인공지능이 어떻게 상호작용해야 하는지, 더욱 공정한 AI 윤리가 마련될 것이다.

## 미디어 법학자들의 인공지능 기술 탐구

AI 기술은 최근 몇 년간 “전통적인 미디어”에서 다양한 미디어 상품의 집합체로서 발전하고 있는 미디어 변화를 인식하게 한다. 다시 말해, ‘미디어의 융합’은 모든 미디어 콘텐츠를 디지털화하여 품질 저하 없이 생산, 교환할 수 있는 기술적 가능성에서 비롯된다. 이는 미디어 콘텐츠 생산자(제공자) 간의 경쟁을 고려하고, 동시에 미성년자, 개인 권리 및 지적 재산권의 보호가 적절하게 보장되는 규제의 틀을 국가가 어떻게 만들 수 있는지에 대한 물음으로 확대된다.

이런 점에서 독일의 미디어 법학자들은 새로운 기술이 가진 잠재력과 위험을 무엇보다 기본권 차원에서 다루고 있다. 독일의 기본법은 “언론의 자유와 방송 및 영화를 통한 보도의 자유를 보장”하면서, 방송과 영화의 자유와 함께 언론의 자유를 보장하고 있다. 독일연방헌법재판소는 출판물의 특정 형태와 무관하게 의사소통 과정에서 출판물이 관련되는 한, 언론의 자유를 보호한다(BVerGE 66, 134: 107, 280). 따라서 새로운 전자적 배포의 형태, 즉 로봇 저널리즘이나 AI 저널리즘도 기본법상 언론의 보호 대상에 포함된다(Weberling, 2018). 하지만 AI 자체는 인간도 아니고 법적 실체도 아니므로 기본권을 행사할 수 없다. 물론 AI 운영자는 기본권을 행사할 수 있으며, 직업의 자유나 커뮤니케이션 권리가 보장된다. 그러나 가장 중요한 것은 AI를 기반으로 하는 저널리즘 제공자가 언론의 자유에 의존할 수 있는지다. 왜냐하면, 기본법상의 기본권인 ‘언론의 자유’는 언론 종사인 ‘사람’을 위한 인권이기 때문이다.

물론 AI 저널리즘은 저널리스트의 개입하에 콘텐츠가 제작된다. 거대 인공지능 모델이 사람의 도움 없이 초소형 AI를 만드는 시대가 왔고, 이제는 거대 모델이 알아서 소형 모델 개발부터 배포까지 하고 있다. 인공지능 과학자들에 따르면, 거대 AI 모델은 스스로 데이터 세트를 통해 사람의 동작 데이터까지도 수집하고 분석할 수 있다(Business Insider, 2023. 12.). 하지만 인간 저널리스트는 기사 작성과 콘텐츠 제작과정에서 모든 유형의 출판물에 중요한 요인이 무엇인지 선별하고 결정하고 있다. 따라서 편집자나 저널리스트가 AI 소프트웨어를 보조도구로써 사용하고, 이러한 보조도구에 의해 제작된 콘텐츠에 대한 윤리적, 법적 책임은 여전히 사람에게 부여된다. 아래 <표 1>은 독일의 미디어 법학자들이 AI 저널리즘과 관련해 논의하고 있는 주요 이슈들을 개괄적으로 나열하고 있다. 


| |주요 이슈|관련 내용|참고|
|:----|:----|:----|:----|
|기본법|·언론의 개념|·의사 소통 과정에 출판물이 관련되어 있다면 언론의 개념으로 보호의 대상|·인간의 개입 없이 독자적으로 저널리즘 작업을 수행할 수 없음|
| |·언론자유에 의미와 보호 범위|·전자적 배포 형식의 로봇 저널리즘도 언론의 보호 대상이 됨| |
| | |·기본법상의 기본권인 ‘언론의 자유’는 언론 종사인 ‘사람’을 위한 인권임| |
|노동법|·노동법과의 관련성|·저널리즘 로봇은 자연인이 아니므로 노동법과는 무관함|·노동권은 사람을 위한 인권임|
| | |·보조적 도구가 저널리스트의 인격권을 침해하는지 법적 책임| |
|저작권법|·저작물의 보호 대상|·저작권법은 인간의 창작 활동에 의한 저작물을 보호의 대상으로 함|·저널리즘에 사용되는 기계나 컴퓨터의 활용은 보조수단임|
| | |·저널리즘에 사용되는 로봇은 보조수단이므로 저작권법상 저작자에서 제외됨| |
|EU|·AI 개념 정의|·로봇 저널리즘과 AI 저널리즘의 개념 구분|·세계 첫 AI 법|
|AI 법|·규제의 범위|·국가 개입의 정도| |
| | |·위험등급을 구분하고,| |
| | |라벨링의 필요성 제시| |
| | |·산업적 개발 필요| |

: <표 1> 미디어 법학자들의 AI 저널리즘에 대한 논의와 견해   

<표 1>에서 살펴본 바와 같이 AI가 생성한 텍스트는 아직　저작권으로 보호되지 않는다. 왜냐하면, 저작권법은 인간의 창작 활동에 의한 저작물을 보호의 대상으로 하기 때문이다. AI에게 프롬프트를 사용해 텍스트를 생성해 달라고 요청한 사람도 자동으로 결과물의 작성자가 되는 것이 아니다. 이는 AI가 생성한 텍스트 및 이미지를 제한 없이 복사하고 다시 게시할 수 있음을 의미한다. 따라서 인공지능을 사용하여 맞춤형 콘텐츠를 생성하는 언론사나 광고 대행사에 문제가 될 수 있다. 이들은 자신의 창작물이 법적 결과 없이 다른 사람에 의해 복제될 수 있음을 예상해야 한다. 

최근 AI 기술을 이해하고, 적절한 법안을 제시하기 위한 법학자들의 노력 가운데 ‘텍스트와 데이터 마이닝’에 대한 기술적 해명이 있다. 우선, 언어 중심의 생성형 AI는 거대언어모델(LLM, Large Language Model)을 통해 데이터를 학습한다. 다시 말해, 이미지나 영상 생성 AI도 그림과 비디오 등을 배우지만, 학습한 데이터는 학습이 끝난 뒤 거대언어모델 내부에 저장이 되거나 원본 파일이 따로 없다. 즉, 거대언어모델은 데이터를 갖고 머신러닝을 돌리면 수식이 저절로 생기는 방식이다. 따라서 데이터는 머신러닝에서 가중치를 최적화하는 재료로 사용되지만, 데이터가 최종 수식에 보존되지 않는다. 이런 특성 때문에 생성 AI 업계에서는 개발사가 소설, 기사, 만화 등 저작물을 법적 제약 없이 학습할 수 있다는 인식이 지배적이다. 지금의 저작권법에서는 원작을 저장하고 재현하는 행위를 저작권 위반으로 보는데, 생성 AI는 둘 다 해당 사항이 없다는 견해이다. 이런 점에서 뉴욕타임스와 오픈AI 간의 법적 갈등은 평행선을 달려왔다. 뉴욕타임스는 생성 AI 학습에 기사를 쓴 대가로 거액을 요구했지만, 오픈AI는 이런 저작물을 마음대로 학습할 수 있다고 주장해 왔다. 따라서 미국의 AI 지지자들은 저작권법 제107조에 따라, ‘공정이용(fair use)’을 주장하는 반면, 유럽에서는 ‘텍스트 및 데이터 마이닝’을 위해 저작권이 있는 작품의 특정 사용을 허용하는 DSM 지침[^j-2]을 인용해 왔다. 

[^j-2]: DSM 지침이란 유럽의회가 2019년 의결한 ‘유럽연합 디지털 단일시장의 저작권 및 저작인접권 지침’을 말한다. 동 지침 제4조(1)항에 따르면, 저작권자가 명시적으로 제한하지 않는 한 텍스트 및 데이터 마이닝 목적으로 합법적으로 접근 가능한 작품의 복제 및 추출을 허용할 의무를 규정하고 있다. 이러한 복제는 텍스트 및 데이터 마이닝 목적으로 필요한 기간 보관될 수 있다. 그런데 2024년 9월, 베를린에서 발표된 인공지능 과학자와 법학자 간의 공동 연구에 따르면, 생성형 AI 모델 훈련이 현행 저작권법과 호환되지 않는다는 결론에 도달하고 있다(Tim W. Dornis & Sebastian Stober, 2024).  
 
이런 상황에서 독일의 법학자 팀 도어니스(Tim W. Dornis)와 인공지능 과학자 세바스티안 스토버(Sebastian Stober)는 생성형 AI 모델의 기술을 살펴보면서, 이 기술의 훈련은 텍스트 및 데이터 마이닝의 경우가 아니라고 주장한다. 이들에 따르면, “훈련 데이터의 일부는 현재 생성모델(LLMs and latent diffusion models)에서 전체 또는 부분적으로 기억할 수 있기 때문에 최종 사용자가 적절한 프롬프트로 다시 생성하여 재생산할 수 있다”고 지적한다(Initiative Urheberricht, 2024). 따라서 이는 명백한 저작권 침해가 된다. 다시 말해 이 연구는 지금까지 지배적인 유럽의 법률적 견해, 즉 DSM 지침에 반대하는 이론적 근거가 된다.

텍스트와 데이터 마이닝에 대한 기술적 해명에서 제시하는 주요 주장은 크게 세 가지로 구분된다. 첫째, 텍스트와 데이터 마이닝에 대한 예외는 생성형 AI 훈련에 적용되어서는 안 된다. 왜냐하면, 이 기술은 근본적으로 다르기 때문이다. 하나는 의미 정보(semantic information)을 처리하는 반면, 다른 하나는 구문 정보(syntactic information)를 추출한다. 둘째, 생성형 AI의 훈련 중에 발생하는 대규모 침해를 정당화할 적절한 저작권 예외 또는 제한은 없다. 예를 들면, 데이터 수집 중에 보호된 작품을 복사하는 것, AI 모델 내에서 전체 또는 부분적으로 복제하는 것, 챗GPT와 같은 AI 시스템의 최종 사용자가 시작한 훈련 데이터에서 작품을 재생산하는 것과 관련된다. 셋째, AI 훈련이 유럽 외부에서 이루어지더라도 개발자는 유럽의 저작권법을 완전히 피할 수 없다. 저작물이 AI 모델 내부에서 복제되는 경우, 유럽에서 모델을 공개하면 InfoSoc 지침 제3조[^j-3]에 따라 ‘공개할 권리’를 침해할 수 있다. 따라서 유럽에서 AI 서비스를 제공하는 것은 궁극적으로 개발자를 유럽 저작권법과 유럽 법원의 관할권에 종속시킨다. 이런 점에서 이 연구는 지금까지 우리가 지적재산의 도난을 다루고 있다는 사실을 기술적, 이론적으로 증명한 것이며, 인간의 창의성 보호와 AI 혁신 간의 더 나은 균형이 필요하다는 사실을 분명히 밝고 있다. 독일기자협회 미카 보이스터 (Mika Beuster) 회장은 “이 연구를 통해 빅테크 기업의 AI 훈련 관행이 불법이라는 점을 확인”하는 한편, “앞으로 깨끗한 ‘그린(Green) AI’는 우리 사회의 가치와 규범을 반영하고 법적 위반을 사전에 방지해야 한다”고 주장했다(DJV, 2024. 9).  

[^j-3]: InfoSoc Dirctive는 정보 사회에서 저작권 및 관련 권리의 특정 측면의 조화에 관한 지침이다. (2001년 5월 22일, 2001/29/EC) 동 지침 제3조, 대중에 대한 전달권에 따르면, 회원국은 저작자에게 유선 또는 무선 수단을 통해 자신의 작품을 대중에게 전달하는 것을 허가하거나 금지하는 독점적 권리를 부여해야 하며, 여기에는 대중이 개별적으로 원하는 장소에서 원하는 시간에 작품에 접근할 수 있도록 한다.

## 미디어 경제학자들의 인공지능 기술 탐구
   
미디어 경제학에서는 디지털 미디어의 산업 구조와 자원을 근본적으로 변화시키고 판매 시장에 영향을 미치는 디지털 플랫폼에 주목한다(Caraway, 2023). 디지털 플랫폼은 다양한 행위자 간의 중개자 역할을 하는 플랫폼 경제를 의미한다. 플랫폼 경제에서 중요한 요소는 직간접적인 행태로 발생하는 네트워크 효과이다. 직접적인 네트워크 효과는 네이버와 같이 사용자가 많아질수록 사용자에 대한 플랫폼 가치가 높아질 때 발생한다. 간접적인 네트워크 효과는 관련된 사용자의 행위가 많을수록 이들의 활동이 플랫폼의 가치를 높일 때 발생한다.[^j-4] 이러한 역동성은 비용 우위를 통해 지배적인 플랫폼이 등장하여 사용자 기반의 상당 부분을 통제하고 시장을 지배하는 독점으로 이어진다(Peitz, 2006; Fuchs, 2015; Yun, 2024). 디지털 미디어 환경의 독점화는 시장 점유율이 소수의 대기업에 집중되는 것을 의미한다. 따라서 소수의 강력한 플레이어가 시장을 지배하고, 잠재적으로 경쟁 부족 및 독점이윤으로 이어질 수 있다(Litschka & Tschulik, 2019; Yun, 2024). 

[^j-4]: 미디어 경제학에서는 디지털 미디어 환경에서 생산과 소비의 활동을 동시에 하는 플랫폼 경제를 연구하면서, 디지털 노동과 착취의 구조를 설명하고 있다(Fuchs, 2015). 이러한 현상과 구조는 접근 방식의 따라, 네트워크 효과(Caraway, 2023), 독점이윤(Yun, 2024)으로 설명한다.  

오늘날 우리 생활에서 데이터가 차지하는 비중은 상당하다. 미디어 경제학에서는 디지털 미디어 환경을 고찰하면서 데이터의 특성과 데이터의 생산, 유통 및 소비 환경에 주목한다. 이때 데이터는 지식과 정보가 포함된 결과물로서 디지털 미디어 환경에서 한번 생산되고 나면 매우 쉽고, 간단히 전달되는 재화(good)이다. 이처럼 한번 생산된 데이터는 정치, 사회, 경제, 문화 등 관련 영역에서 주체들이 고유한 활동을 하면서 발생시키는 결과물이며, 동시에 다른 주체들에게 필요한 (리)소스가 된다. 따라서 ‘시장’에서는 지식과 정보가 포함된 데이터가 ‘상품’으로 생산, 유통 및 소비되는 반면, ‘공적 영역’에서는 공개와 협력 및 교류를 통해 전체 사회를 위한 데이터 ‘재화’가 생산, 분배되고 있다. 이런 점에서 미디어 경제학, 특히 미디어 정치경제학[^j-5]에서는 디지털 미디어 상품의 가치와 가격에 대한 분석을 통해 디지털 미디어 환경에서 한번 생산된 데이터가 상품화, 독점화되는 시장의 구조를 검토하는 한편, 시장이 아닌 공공 영역에서 한번 생산된 데이터가 공공재화로 생산, 분배되고 있는 데이터 저널리즘의 사례들을 제시하고 있다(Fuchs, 2015; Yun, 2024). 다시 말해, 미디어 정치경제학에서는 디지털 미디어 상품의 가치와 가격에 대한 분석을 통해, 데이터가 디지털 환경에서 추가 생산비용 없이 재생산된다는 점에서 데이터의 경제적 가치를 고찰하고, 이를 통해 데이터가 상업적인 방식이 아닌 공공재화로서 생산, 분배될 수 있는 가능성을 주장하고 있다.

[^j-5]: 이 글에서 말하는 미디어 정치경제학이란 독일어권의 비판적 정치경제학의 미디어와 커뮤니케이션 연구(Die Kritische Politische Oekonomie der Medien und der Kommunikation)와 영미권의 미디어 정치경제학(Political Economy of the Media)을 모두 아우른다. 이들은 주요하게 맑스주의 정치경제학의 기본 개념들을 수용하고, 비판적인 관점에서 미디어 커뮤니케이션 연구를 진행하고 있다.
        
한편, 지식과 정보가 포함된 데이터 분석은 데이터와 저널리즘이 어떻게 관련되는지 데이터 저널리즘의 형태로 확대, 연결된다. 이는 빅데이터, 알고리즘, 또는 AI 기술이 컴퓨터공학이나 정보관리 시스템, 그리고 도서관학 연구자들의 연구 대상을 넘어 미디어 커뮤니케이션 연구 일반에서 주요하게 고찰하고 있는 연구 대상, 즉 데이터 저널리즘, 컴퓨테이션 저널리즘 및 AI 저널리즘을 말한다. 예를 들면, 뉴스가 하나의 데이터로 생산, 전달되는 데이터 저널리즘의 생태계에서 시장의 지배력이 막강한 빅테크 플랫폼 기업은 이미 뉴스 데이터를 상업적인 알고리즘으로 설계, 운영하면서 독점이윤을 획득하고 있다. 하지만 다른 한편에서는 높은 사용가치가 있는 뉴스 정보를 생산하기 위해 일정 부분 투여됐던 최소한의 생산비용이나 노동비용조차 제대로 회수하지 못하는 경우가 많다. 간단히 말해, 한번 생산된 뉴스 데이터는 인터넷 공간에서 기술적인 차단이나 법률적인 개입, 예를 들어 저작권이나 불법이라는 인식이 없다면 누구나 쉽고, 간단히 사용할 수 있는 공공재화가 된다. 하지만 인터넷과 디지털 기술이 발전하면서 뉴스 콘텐츠와 같은 생산물을 사적으로 소유하고, 지배하기 위해 기술적 차단 방식이 개발되고, 강력한 법률적 개입이 증가하고 있다. 물론 뉴스 콘텐츠와 같은 데이터를 생산하려면, 어느 정도의 생산비용이 요구되고, 창작자를 위한 합당한 분배가 보장되어야 한다. 그런데 현실 세계에서 확인되는 미디어 시장의 상품화, 독점화 현상은 시장의 지배력을 지닌 빅테크 플랫폼 기업이나 대형 언론사 또는 관련 시장에서 나름대로 크고, 작은 영향력을 행사하는 기업들이 본래의 생산비용이나 생산에 투여된 노동보다 높은 이윤을 획득하고 있다. 반면, 디지털 미디어 환경에서 아무런 영향력이 없거나 법적 권리를 주장하기 힘든 창작자나 영세 기업들은 최소한의 생산비용조차 회수하기 불가능한 것이 현실이다.

따라서 미디어 정치경제학자들이 데이터 기반의 컴퓨테이션 저널리즘 또는 AI 저널리즘에 주목하는 이유는 미디어 기업이 보다 많은 작업에서 데이터를 활용하고 있고, 이러한 데이터를 더 많이 공유할수록 전체 사회 또는 인류에 더욱 많은 혁신과 복리가 증대될 것으로 예상하기 때문이다(Diakopoulos, 2016). 실례로 디지털 미디어 환경에서 데이터를 공유하는 것은 미디어 기업의 경영 측면에서도 새로운 수익원이 된다. 이는 리눅스 운영체제 관련 기업에서 오픈 데이터인 리눅스를 통해 수익성이 증가한 사실에서 확인된다. 오픈 데이터(소스)란 어떤 소프트웨어의 설계도라 할 수 있는 소스 코드와 디자인 문서, 그리고 제품의 콘텐츠 등을 누구나 사용할 수 있도록 공개하는 것을 말한다. 이렇게 공개된 소스 코드는 디지털 환경에서 누구나 접근할 수 있으며, 또 다른 사람이 공개된 소스 코드 위에 새로운 코드를 만들어 발전된 소프트웨어의 코드로 재공개된다. 

디지털 미디어 환경에서 데이터의 경제적 가치를 고찰하는 미디어 정치경제학자들은 상업화되고 독점화되는 인터넷 플랫폼이 우리 사회의 민주주의를 위협한다며 비판적 시각에서 공공서비스 인터넷과 공공서비스 알고리즘, 나아가 공공서비스 미디어의 가능성을 제시하고 있다(Fuchs & Unterberger, 2021). 공공서비스 알고리즘이란 공개와 협력 및 교류를 통해 전체 사회를 위한 알고리즘을 상품이 아닌 ‘재화’로써 생산, 분배하는 것이다. 이를테면, 독일의 제2 공영방송 체데에프(ZDF)는 자사의 추천 알고리즘 방식을 YouTube나 Google의 상업적 알고리즘과 차별화하고 있다. 체데에프에서 추천 알고리즘을 개발하고 있는 안드레아스 그륀(Andreas Gruen)에 따르면, 체데에프의 알고리즘은 매우 유사한 콘텐츠의 수를 제한하고 몇 가지 프로그램의 동영상만 제공하지 않는다(Krei, 2022). “우리는 최신 선호도와 인기 선호도를 대응하기 위해 더 긴 시간과 더 많은 사용 데이터를 샘플링한다. 그리고 우리가 만든 알고리즘을 공개하고, 이를 통해 사람들에게 새로운 주제에 관심을 두게 만든다.” 이것이 공영방송 체데에프가 상업 미디어와 차별화하는 방식인데, 직접 알고리즘을 개발하고, 알고리즘의 운영 방식을 공개할 때 투명성을 유지하는 것이다. 즉, 데이터와 알고리즘이 공개되고, 서로 협력하여 교류할 때, 새로운 가치를 창출하게 된다는 미디어 정치경제학의 설명은 인류가 이룩한 과학기술의 성과는 소수 기업이 독점할 수 있는 배타적인 성과가 아니다. 이들의 주장은 데이터와 알고리즘이 지닌 객관적인 기술적 특성에서 기인하며, 시장이 아닌 공적 영역에서 더욱 실현 가능하다는 사실로 제시되고 있다. 다음 <표 2>는 미디어 경제학에서 데이터와 알고리즘 그리고 AI 기술을 설명하는 주요 내용을 개괄하고 있다.    

| |기술적 활용|데이터의 특성과 활용|저널리즘의 활용|참고|
|:----|:----|:----|:----|:----|
|데이터|·빅데이터|·지식과 정보가 포함|·뉴스 콘텐츠는 하나의 데이터이며 소스이자 코드|·공공데이터|
| |·오픈 데이터|·한번 생산하고 나면 쉽고, 간단히 전송|·데이터 저널리즘|·상업데이터|
| | |·저장이 가능함|·빅데이터 활용(생산, 유통, 판매): 데이터 시각화|·사적 소유 가능|
| | | |·뉴스 데이터 기반의 플랫폼 서비스 증가(포털, 블로그, SNS, 검색엔진)| |
|알고리즘|·콘텐츠 기반 필터링|·데이터 기반의 추천 알고리즘|·필터버블: 편향성|·공공 알고리즘|
| |·협업 필터링|·선별된 데이터 서비스|·개인화 서비스|·협력, 공개, 투명|
| | | |·편향성, 차별성 우려|·신뢰, 만족|
| | | | |·양극화 우려|
|생성 AI|·머신러닝|·데이터는 주요한 원료|·AI 저널리즘|·사적 소유 불가|
| |·텍스트 & 데이터 마이닝|·사용(최적화) 후 원재료는 불필요|·로봇 저널리즘|·공정 이용|
| |·거대언어모델(LLM)| |·뉴스 데이터 기반의 빅테크 기업의 개인화 서비스 극대화|·개인화,독점화 고조|

: <표 2> 미디어 경제학에서 데이터, 알고리즘 및 AI를 설명하는 내용



## 독일 저널리스트들의 인공지능 기술 탐구

저널리즘 분야에서 인공지능을 활용하는 것은 의견과 여론형성, 나아가 국가와 사회에 영향을 미칠 수 있는 잠재력이 있다. 또한, 언론인의 업무에 심각한 영향을 미칠 것이다. 따라서 저널리즘에 인공지능을 활용하려면 자유민주주의와 저널리즘 직업을 위태롭게 하지 않도록 적절한 가이드라인이 절대적으로 필요하다(DJV, 2024.4). 이러한 지침, 즉 객관적인 원칙은 도로 위의 자율주행차로부터 사람들을 보호해야 하는 것처럼, 저널리즘 분야에서도 인공지능의 무분별한 사용을 막을 수 있게 한다.

독일기자협회(DJV)는 AI 저널리즘은 한 신문사에 국한된 문제가 아니라 소통과 의견형성에 신기술이 활용되는 문제로서 국가와 사회에 영향을 미칠 수 있다는 견해를 밝힌다. 무엇보다 AI 응용 프로그램은 윤리 및 가치 체계와 거리가 멀게 작동하므로 언론사와 저널리스트가 항상 가지고 있던 비판적 감시 기능을 수행할 수 없으며, 이익을 추구하는 미디어 기업은 저널리즘적 품질보다 AI의 경제적 잠재력을 더 높이 평가할 수 있다는 게 협회의 입장이다. 나아가 협회는 악셀 슈프링거사(Axel Springer)와 같은 거대 언론사가 오픈AI사와 긴밀한 협력 관계에서 언론 활동을 할 때, 저널리스트를 위한 AI 수익의 공정한 분배를 요구한 바 있다.[^j-6] 이는 2024년 1월, 협회 성명서에서 확인되는데, “악셀 슈프링거가 판매하는 것은 저자의 지적 작품이기 때문에 언론인들은 적절한 몫을 받아야 한다”는 입장을 분명히 했다(DJV, 2024. 1.). 아래 <표 3>은 저널리즘 분야에서 인공지능 활용에 대한 독일기자협회의 공식적인 입장을 개괄적으로 나열하고 있다. 

[^j-6]: Axel Springer (2023. 9. 6.). Axel Springer startet bei BILD deutsches ChatGPT-Angebot Hey_ https://www.axelspringer.com/de/ax-press-release/axel-springer-startet-bei-bild-deutsches-chatgpt-angebot-hey_

한편, 독일의 제1공영방송 아아르데(ARD) 산하의 바이에른 공영방송(BR)은 AI 프로그램 개발을 위해 10가지 원칙을 제시한 바 있다(BR, 2024.7.). 여기에서는 AI 활용의 윤리적 문제가 포함되는데, 투명성과 자원에 대한 책임감, 협력과 토론이 수반되는 평가, 그리고 알고리즘 편향에 대응하고 사회적 다양성을 반영하기 위한 노력이 강조된다. 바이에른 공영방송은 더 나은 저널리즘을 수행하기 위해 인공지능의 잠재력 이점과 위험을 고려하면서, 인간과 인공지능 사이의 건설적인 상호작용의 가능성을 모색하고 있다. 이때 새로운 기술이 가져다줄 투명성과 다양성, 그리고 지역성은 기자와 사용자를 위한 이익과 가치가 된다. 

바이에른 공영방송은 기술이 그 자체로 끝나는 것이 아니라 더 나은 저널리즘을 수행하는 데 도움이 되는 도구로 사용됨을 강조한다. “우리는 바이에른 공영방송과 사용자에게 실질적인 부가가치를 제공하는 기술만을 사용한다”는 목적을 분명히 밝히고 있다. 이때 새로운 기술은 인공지능과 기타 모든 형태의 자동화 기술까지 확대, 적용된다. 그리고 이와 같은 새로운 기술의 적용은 다음 <표 4>와 같은 윤리 지침을 토대로 운영된다. 

|주요 입장|내용 요약|참고|
|:----|:----|:----|
|인간의 성과를 대체할 수 없음|·미디어 회사는 언론인의 일자리를 대처하기 위한 목적으로 인공지능을 사용할 수 없음|새로운 기술은 인간을 위해 개발함|
| |·인공지능과 사람 간의 상호작용은 여전히 중요함| |
| |·완전히 자동화된 메시지 생성과 배포는 피하고, 콘텐츠의 책임은 사람에 있음| |
|콘텐츠에 대한 책임|·자동화 및 AI 사용을 통해 생성된 게시물이 인간 기자의 참여나 개입 없이 게시되면 안 됨|기계가 아닌 사람에게 책임이 있음|
| |·편집팀은 저널리즘적 책임을 짐| |
| |·언론사는 자사의 AI 행동 강령을 준수하고 불만 사항을 담당하는 책임자를 임명해야 함| |
|데이터 자료의 책임 있는 처리|·편집팀은 데이터에 관한 책임과 정확성을 보장함|상업용 빅테크기업과 독립성 보장함|
| |·데이터의 불완전성, 왜곡 및 기타 오류는 즉시 수정되고, 개인 데이터는 관련 보호법을 준수해야 함| |
| |·미디어 회사는 자체적으로 데이터를 구축하고, 당국과 정부 기관의 공개데이터 프로젝트를 지원해야 함| |
|투명성 및 라벨링|·인공지능의 생성 콘텐츠는 표시해야 함|입법부에 라벨링 법제화를 요구함|
| |·자동화 기술은 일부 공개되어야 함| |
| |·저작자 보상을 위해 데이터 출처를 명시함| |
|책임 있는 개인화|·필터버블을 피함|사회적 다양성과 투명성을 보장함|
| |·편집팀이 선별한 콘텐츠 제공을 보장함| |
| |·사용된 알고리즘을 사용자에게 공개함| |
| |·사용자는 알고리즘 기준을 변경하고, 개인화된 배포를 선택, 취소할 수 있어야 함| |
|인증된 AI 시스템 사용|·각 언론사에서 활용되는 AI시스템에 대한 인증 개발을 지지하고, 지원함|저널리즘의 품질 개선과 표준화|
| |·품질, 균형, 차별 금지, 데이터보호 및 보안이 특정 표준을 충족해야 함| |
|지속적인 검토|·미디어 기업은 저널리즘 분야에서 인공지능 활용과 영향을 지속해서 검토해야 함|언론사 및 미디어 기업의 사회적 책임 강화|
|평생 교육|·새로운 기술에 대한 언론인 훈련과 교육 제공|저널리스트를 위한 기술 활용|
| |·인공지능에 대한 인식과 활용의 기회 확대| |
|적절한 보상|·제작자인 언론인에게 적절한 보상이 필요함|저널리스트를 위한 보상|
| |·언론인의 AI 시스템 개발과 사용을 위해, 보상을 의무화하도록 입법부에 요구하고 있음| |

: <표 3> AI 저널리즘에 대한 독일기자협회의 입장(2024년 4월 기준)

(자료출처: 독일기자협회 홈페이지, https://www.djv.de/medienpolitik/kuenstliche-intelligenz/)


한편, 독일의 제1공영방송 아아르데(ARD) 산하의 바이에른 공영방송(BR)은 AI 프로그램 개발을 위해 10가지 원칙을 제시한 바 있다(BR, 2024.7.). 여기에서는 AI 활용의 윤리적 문제가 포함되는데, 투명성과 자원에 대한 책임감, 협력과 토론이 수반되는 평가, 그리고 알고리즘 편향에 대응하고 사회적 다양성을 반영하기 위한 노력이 강조된다. 바이에른 공영방송은 더 나은 저널리즘을 수행하기 위해 인공지능의 잠재력 이점과 위험을 고려하면서, 인간과 인공지능 사이의 건설적인 상호작용의 가능성을 모색하고 있다. 이때 새로운 기술이 가져다줄 투명성과 다양성, 그리고 지역성은 기자와 사용자를 위한 이익과 가치가 된다. 

바이에른 공영방송은 기술이 그 자체로 끝나는 것이 아니라 더 나은 저널리즘을 수행하는 데 도움이 되는 도구로 사용됨을 강조한다. “우리는 바이에른 공영방송과 사용자에게 실질적인 부가가치를 제공하는 기술만을 사용한다”는 목적을 분명히 밝히고 있다. 이때 새로운 기술은 인공지능과 기타 모든 형태의 자동화 기술까지 확대, 적용된다. 그리고 이와 같은 새로운 기술의 적용은 다음 <표 4>와 같은 윤리 지침을 토대로 운영된다. 

|윤리 지침|주요 내용|참고|
|:----|:----|:----|
|1. 사용자를 위한 부가가치|·AI 시스템은 작업 효율을 위해 사용하고, 기여자가 위임한 리소스를 책임감 있게 사용함|AI 도입과 사용 목적|
|2. AI에 대한 정확한 설명|·인공 생명체에 대한 오해를 줄이고, 기술의 기능을 허위적으로 묘사하는 것을 피함|활용 기술의 범주 구분|
|3. 편집 통제 및 투명성|·생성된 콘텐츠 책임은 사람과 편집팀에 있음|책임과 의무|
| |·사용하는 기술, 허용 가능한 위험과 제한, 데이터의 역할 및 담당 편집팀에 대한 공개| |
| |·생성된 콘텐츠에 표시(라벨링)하고, 접근 방식을 게시함| |
|4. 영향 평가|·AI 사용의 효과와 부작용을 사전에 평가함|관리 가능한 시스템 사용|
| |·언론 기준과 해당 법률에 부합하는지 평가함| |
| |·가능한 작업에 접근하도록 공개하고, 오픈소스 소프트웨어를 사용함| |
| |·타사 소프트웨어 사용 시 윤리적 고려 사항과 영향 평가를 통합함| |
|5. 다양성과 지역성|·콘텐츠에 대한 더 큰 포용과 장벽 없는 접근의 기회로 AI를 활용함|공영방송의 역할 확대|
| |·잠재적이고 차별적인 고정관념을 고려하면서 데이터를 다루고, 지역성 강화를 위해 노력함| |
|6. 의식적인 데이터 문화|·신뢰 가능한 데이터를 평가, 사용하며, 데이터 보호 규정의 틀에서 개발, 활용됨을 직원들에게 교육함|데이터 관리|
|7. 책임 있는 개인화|·개인화는 사회적 교류와 결속력을 훼손하지 않고, 원치 않는 필터버블 효과를 방지함|상업 미디어 기업과 차별화함|
| |·공공 서비스 추천 알고리즘 개발에 전념함| |
|8. 학습 문화|·제품과 정책 개발을 위해 파일럿 프로젝트와 프로토타입을 통한 학습 환경 보장|개발, 교육|
| |·언론인, 개발자, AI 전문가와 경영진으로 구성된 연구팀에서 정기적인 경험과 윤리적 경계선을 모색함| |
|9. 교류 및 파트너십|·대학과 협력하고, 과학연구소 및 AI 윤리 전문가와의 교류를 모색함|교류, 홍보|
| |·과학기술 및 지역 신생 기업과 협력하고, 전문 지식을 활용해서 프로젝트로 연결, 홍보함| |

: <표 4> 바이에른 공영방송의 AI 및 자동화 처리에 대한 윤리 지침(2024년 7월 버전)

(자료출처: 바이에른 공영방송(BR) 홈페이지, https://www.br.de/extra/ai-automation-lab/ki-ethik-100.html)


위에서 살펴본 바이에른 공영방송의 윤리 지침은 2024년 7월 개정된 내용으로서, 2020년 11월에 작성되었던 첫 번째 버전에서 몇몇 내용을 수정, 보완한 것이다. 대표적으로 위에서 살펴본 지침서 2번에 해당하는 AI에 대한 분명한 개념 정의이다. 지침서에 따르면, 

> “AI(인공지능)라는 용어는 상황에 따라 매우 다른 의미가 있을 수 있다. AI는 지능적인 행동과 유사한 컴퓨터의 기능 집합으로 이해될 수 있습니다. "지능형"이 무엇을 의미하는지, 어떤 기술이 사용되는지는 명시되어 있지 않다. 실제적인 이유로 우리는 특정 작업을 수행하기 위해 훈련이 필요한 컴퓨터 시스템으로 정의를 제한한다. "머신러닝" 또는 독일어로 "기계 학습"이라고 한다”(BR, 2024. 7.).


바이에른 공영방송과 달리 베를린-브란데부르크 지역에 있는 공영방송 에르베베(rbb: Rundfunk Berlin-Brandenburg)에서는 인공지능을 다루기 위한 원칙을 비교적 간단, 명료하게 제시하고 있다(2024년 7월 버전 기준). 에르베베에 따르면, AI를 사용하는 목적은 자사의 공적 의무를 이행하기 위함인데, 이를 위해 저널리즘 연구의 가능성을 확대하고, 다양한 콘텐츠를 현대적이고 매력적인 방식으로 생산, 배포하며 동시에 행정 업무 프로세스를 더욱 효율적으로 만들기 위해 AI를 사용한다(rbb, 2024). 이때 저널리즘의 의무와 데이터 및 출처 보호는 가장 주요하게 고려되며, AI를 사용하기 전에 발생 가능한 위험들은 주의 깊게 검토되어야 한다. 한편, 모든 콘텐츠가 공개되기 전에 사람의 검토와 승인이 필요하며, 담당 편집팀이 모니터링하면서 저널리즘적 관리가 더욱 요구된다. 나아가 아직 라벨링과 관련하여 정확한 기준을 마련한 것은 아니지만, 에르베베는 인공지능 사용의 투명성은 자사에 대한 사용자의 신뢰에 연결되기 때문에 AI 기술을 통해 생성된 콘텐츠에는 사용자가 이해할 수 있는 표시(라벨링)를 분명히 할 계획이다. 이와 같은 향후 계획은 일상적인 편집과 운영 과정에서 인공지능 기술을 올바로 이해하고 투명하게 활용하기 위한 규칙, 즉 법적 틀에 관한 지속적인 논의로 확대될 전망이다. 

지금까지 살펴본 바와 같이 독일의 언론 종사자들은 AI와 관련된 가이드라인을 만들기 위해 다음과 같은 내용에 주목하고 있다. 

> 하나, 저널리즘에 인공지능을 활용하려면 적절한 지침, 즉 가이드라인이 필요한데, 무엇보다 투명성의 의무가 강조된다. 여기서 투명성의 의무는 다시 두 가지 측면에서 확인된다.: 첫째, AI를 사용해서 저널리즘 콘텐츠를 생산할 때, 생산자와 이용자는 이를 인식할 필요가 있고, 둘째, AI가 어떤 데이터 소스를 사용했는지, 어떤 자료를 사용해서 훈련했는지 명확해야 공개할 필요가 있다. 
> 
> 둘, 작성된 콘텐츠가 AI 교육을 목적으로 사용될 경우라도 원 작성자와 소통을 하는 한편, 적절한 보상을 할 수 있도록 해야 한다.
> 
> 셋, 저널리즘 분야에서 인공지능의 활용은 품질, 균형, 차별 금지, 데이터 및 출처 보호, 그리고 저작권 및 보안 측면에서 최소한의 기준을 충족하는 사회적으로 인증된 AI 시스템이 사용되어야 한다. 
> 
> 넷, 인공지능의 활용은 무엇보다 자체적으로 제어가 가능한 기술을 저널리즘 분야에서 활용하고, 유럽연합 기반의 자체적인 인프라를 보유하는 것이 바람직하다. 
>
> 다섯, 나아가 신속하고 단호한 규제가 AI 저널리즘 분야에 필요하다. 실례로 디지털 단일시장 저작권지침(DSM-RL: Digital Single Market Copyright Directive)이 제정된 후 유럽의회에서 채택되기까지 10년 이상이 걸렸고, 그 이후 국내법으로 전환되는데 별도의 시간이 소요되었다. 따라서 AI를 규제하기 위해서는 시기적절한 규제 정책이 필요하다.

독일의 저널리즘과 독일 사회는 AI 기술을 최대한 신속하고 현명하게, 그리고 건전하게 사용하기 위한 원칙을 요구하고 있다. 이러한 원칙은 입법부와 행정부가 제안하는 법률안과 규제 정책, 나아가 앞에서 살펴본 바와 같이 언론사 내부에서 설정, 제시하는 가이드라인을 통해 구체화한다. AI 기술이 저널리즘과 민주주의를 발전시킬지 아니면 파괴할지 ​​여부는 이제 입법부의 손에 달려 있다(DJV, 2024. 3).  


## 마치며: 인간을 위한 인공지능 기술

우리 사회가 어떤 AI 윤리를 마련할 것인지 이제 공은 정치인들의 법정에 있다. 정치인들은 기업이 아닌 사람에게 필요한 결론을 도출하고, 언론인과 다른 창작자들의 희생을 끝내야 할 것이다. 물론 정치인들은 급변하는 AI 기술에 대해 전체 시민들이 무엇을 인식하고, 어떤 문제를 제기하며, 시민들이 어떤 미래를 꿈꾸는지 상호작용하며 필요한 결론을 도출해 갈 것이다. 따라서 인간에게 필요한 정치적 결론은 역설적으로 인간이 현실 세계를 분명히 인식하고, 자신에게 필요한 사안을 정치적으로 요구할 때 실현할 수 있다. 이런 점에서 빠르게 발전하는 AI 기술을 공정하게 사용하려면, 무엇보다 AI 기술에 관한 객관적인 이해와 탐구가 필요하다. 지금까지 살펴본 바와 같이 다양한 관점과 학제 간의 연구들은 AI 기술이 기업이 아닌 사람, 즉 인간을 위한 인공지능 기술이라는 대원칙을 제시하고 있다.     


## 더 생각해볼 문제

1. AI 기술이 보편화하는 오늘날 인공지능 윤리와 도덕적 가치가 논의되는 이유는 무엇인가?

2. 인공지능 윤리와 도덕 외에 AI 기술을 공정하게 사용하려는 우리의 원칙은 무엇인가?

3. 미디어 커뮤니케이션 연구 일반, 특히 저널리즘 연구에서 ‘공정’과 ‘공익’, 그리고 ‘공공’에 관한 개념 정의는 AI 기술의 ‘공정’과 ‘공익’으로 어떻게 확대, 적용될 수 있는가?

4. AI 기술의 불공정성이 상업화된 생산방식에서 기인한다면, 비상업화된 생산방식에서 AI 기술은 자본이 아닌 인간을 위해 사용될 수 있는가?


## 더 읽을거리

김태균, 권영전, 성서호, 박주현 (2023). <챗GPT와 생성 AI 전망>. 커뮤니케이션북스

로버트 W. 맥체스니. 전규찬 옮김 (2014). <디지털 디스커넥트: 자본주의는 어떻게 인터넷을 민주주의의 적으로 만들고 있는가>. 삼천리.

이재신 (2022). <인공지능 알고리즘과 다양성 그리고 편향>. 커뮤니케이션북스

Bettig, Ronald (1996). Copyrighting Culture: The Political Economy of Intellectual Property. Taylor & Francis.

Yun, Jang-Ryol (2024). The Value and Price of Digital Media Commodities. Media, Culture & Society, 46(2), 219-234. https://doi.org/10.1177/01634437231188464



## 참고문헌

Bayerischer Rundfunk (2024. 7. 12.). Umgang mit Kuenstlicher Intelligenz: Unsere KI-Richtlinien im Bayerischen Rundfunk. https://www.br.de/extra/ai-automation-lab/ki-ethik-100.html

Business Insider (2023. 12. 17.). Large AI models can now create smaller AI tools without humans and train them like a 'big brother,' scientists say https://www.businessinsider.com/large-models-can-create-new-smaller-ai-tools-scientists-2023-12

Caraway, Brett (2023). Digital Media Economics: A Critical Introduction. SAGE Publications. 

Coddington, Mark (2015). Clarifying Journalism’s Quantitative Turn. Digital Journalism, 3(3), 331-348.

Dahm, M. H. & Twesten, N. (2023). Der Artificial Intelligence Act als neuer Maßstab für künstliche Intelligenz. Das Spannungsfeld zwischen Regulatorik und Unternehmen. Wiesbaden. https://doi.org/10.1007/978-3-658-42132-8

Deutscher Journalisten-Verband (2024). Positionspapier bezueglich des Einsatzes Kuenstlicher Intelligenz im Journalismus. 

Deutscher Journalisten-Verband (2024. 1. 3.). KI-Erlöse fair verteilen https://www.djv.de/news/pressemitteilungen/press-detail/ki-erloese-fair-verteilen/

Deutscher Journalisten-Verband (2024. 4. 24). DJV fordert klare Regeln https://www.djv.de/news/pressemitteilungen/press-detail/djv-fordert-klare-regeln/

Deutscher Journalisten-Verband (2024. 8. 28). Inhalte nicht an KI verschenken https://www.djv.de/news/pressemitteilungen/press-detail/inhalte-nicht-an-ki-verschenken/

Deutscher Journalisten-Verband (2024. 9. 30). DJV fordert Green-KI https://www.djv.de/news/pressemitteilungen/press-detail/djv-fordert-green-ki/

Dornis, Tim W. & Sebastian Stober (2024). Urheberrecht und Training generativer KI-Modelle: technologische und juristische Grundlagen.  

Diakopoulos, Nicholas (2016). Computational journalism and the emergence of news platforms. In B. Franklin & S. Eldridge II (Eds.), The Routledge companion to digital journalism studies (pp. 176–184). Abingdon: Routledge. 

Fuchs, Christian (2015). Reading Marx in the Information Age. Routledge. 

Fuchs, Christian and Klaus Unterberger (2021). The Public Service Media and Public Service Internet Manifesto. University of Westminster Press. pp. 7-18.

Hagendorff, Thilo (2020). The Ethics of AI Ethics: An Evaluation of Guidelines. Minds and Machines. 30, 99-120. 

Initiative Urheberrecht (2024. 10. 1.). Interdisziplinäre Studie belegt Urheberrechtsverletzungen beim Training generativer KI https://urheber.info/diskurs/interdisziplinare-studie-belegt-art-und-umfang-der-urheberrechtsverletzungen-beim-training-generativer-ki

Krei, Alexander (2022). DWDL.de-Interview mit Eckart Gaddum und Andreas Gruen: Den öffentlich-rechtlichen Auftrag in Algorithmen übersetzen. https://www.dwdl.de/interviews/88806/den_oeffentlichrechtlichen_auftrag_in_algorithmen_uebersetzen/?utm_source=&utm_medium=&utm_campaign=&utm_term=

Litschka, Michael & Sebastian Tschulik (2019). Der Social Choice der Selbstregulierung–ein vertragstheoretischer Versuch in Zeiten ökonomisierter und mediatisierter conditio humana. Der Mensch im digitalen Zeitalter: Zum Zusammenhang von Ökonomisierung, Digitalisierung und Mediatisierung, 87-102

Litschka, Michael & Tassilo Pellegrini (2019). Considerations on the Governance of Open Data – an Institutional Economic Perspective. International Journal of Intellectual Property Management, 9(3-4), 247-263.

Peitz, Martin (2006). Marktplätze und indirekte Netzwerkeffekte. Perspektiven der Wirtschaftspolitik, 7(3), 317-333.

Rundfunk Berlin-Brandenburg (2024). RBB KI GRUNDSAETZE: GRUNDSAETZE ZUM UMGANG MIT KUENSTLICHER INTELLIGENZ IM RBB. 

Weberling, Johnnes (2018). Medienrechtliche Bedingungen und Grenzen des Roboterjournalismus, NJW 2018, 735-739. 

Yun, Jang-Ryol (2024). The Value and Price of Digital Media Commodities. Media, Culture & Society, 46(2), 219-234. https://doi.org/10.1177/01634437231188464


