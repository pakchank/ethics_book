[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "인공지능 시대의 윤리 또는 윤리적 인공지능 - 기술, 정책, 철학",
    "section": "",
    "text": "Preface\n공학자들도, 인공지능을 조금 더 윤리적으로 만들기 위해 나름의 해결책을 제시하기 위해 노력하고 있다. 이들의 목적은 인공지능의 판단 결과를 조금 더 공정하고, 설명가능하고, 프라이버시 보호하는 방향으로 만들기 위해 데이터 수집, 처리 방법, 사후 처리, 알고리즘 설계 등에 개입하는 것. 이러한 작업을 하기 위해서는 위와 같은 개념들은 수학적으로 정의할 수 있어야 한다. 실제로 상당한 성과를 만들어냈으며, XAI는 협업을 더 잘 가능하게 하고, FML은 법적 처방과, 차분 프라이버시는 실제 거대 테크기업들의\n또한, 이러한 해법의 부산물로, 우리는 윤리적 입을 위한 개념들, 특히 윤리성을 의미하는 공정성 등의 용어 안에 얼마나 다층의 개념들이 모호하게 뒤섞여 있었는지를 알게될 것이다…\n물론 이러한 방법들이 만병통치약은 아니다. 앞선 챕터들의 논의한 윤리적 문제들과 화합되지 않는 측면들을 살펴보자.\n인공지능, 빅데이터, 기계학습의 관계에 대해서 쓰기…"
  },
  {
    "objectID": "index.html#예상-챕터",
    "href": "index.html#예상-챕터",
    "title": "인공지능 시대의 윤리 또는 윤리적 인공지능 - 기술, 정책, 철학",
    "section": "예상 챕터",
    "text": "예상 챕터\n1부: 인공지능 윤리의 철학적 기초\n\n인공지능 윤리의 모색을 위한 철학적, 기술학적 기초 (경북대 김상호)\n인과성과 책임있는 인공지능 (협의중)\n인공지능과 정보철학/윤리 (전남대 유용민)\n포스트휴머니즘과 윤리적 인공지능 (조선대 박선희)\n자본주의 권력/인공지능 권력 (협의중)\n\n2부: 인공지능 윤리의 적용\n\n신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인 (선문대 박대민)\n인공지능과 미디어 윤리 (협의중)\n인공지능과 사회 불평등 (중앙대 이정현)\n인공지능 시대의 노동 (성균관대 이창준)\n인공지능 서비스와 저작권 (법무법인 지평 이혜온)\n인공지능과 리터러시 (협의중)\n도덕심리와 인공지능 서비스의 개발과 활용 (서강대 사영준)\n\n3부: 인공지능 윤리의 기술적 접근\n\n설명가능한 인공지능 (경북대 박찬경)\n공정한 기계학습 (경북대 박찬경)\n차분프라이버시를 통한 개인정보 보호 (경북대 박찬경)"
  },
  {
    "objectID": "basic.html",
    "href": "basic.html",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "",
    "text": "2 기술(인공지능)-인간의 관계적 존재론\n분리되고 고정되며, 자기 완결적인 존재로서의 개체를 파악하게 된다면 기술적인 대상들의 존재양식에 대한 올바른 이해에 도달하지 못하게 된다. 지금 논의하고 있는 인공지능과 같은 새로운 미디어는 이런 점에서 전혀 다른 방향에서 숙고를 요하는 것이라고 할 수 있다. 왜냐하면 기술과 인간의 관계는 주고받음의 관계 설정보다 주어진 상황에 따른 변화에 의해서 주-객 혹은 대상과 주체의 관계가 변환되는 측면이 더 강하다. 기술적 미디어와 인간의 관계에서 기술적 미디어가 지닌 잠재적 특성(포텐셜)은 여전히 소진되지 않는 채로 새로운 개체화로 진행하는 과정에 있을 것이기 때문이다. 쉽게 말해, 어떤 미디어가 처음 등장해서 인간들과 관계를 맺을 때 현실화되는 것들이 시간의 경과와 다른 미디어들을 비롯한 주변 환경과의 연합 속에서 전혀 다른 미디어로 잠재성을 실현해 갈 수 있다는 말이다. 인공지능이라는 미디어 혹은 기술적 요소는 지금도 새로운 관계맺음의 양상에서 새로운 양상을 보여주고 있는데, 이를 하나의 기술적 대상으로 지각하거나 파악하게 되면, 그 대상과 인간의 단순한 대립 관계에서 윤리적 고민을 모색하게 되기 때문에 적절한 질문과 대답을 찾기 힘들어진다.\n그런 측면에서 매클루언은 미디어 개체들의 성격을 파악하기 위한 개념틀로, 성좌(constellation)라는 것이 오히려 적절하다고 생각한다. 즉 미디어가 지닌 연대기적-위상학적 앙상블이 중요하다는 것이다. 이런 맥락에서 시몽동이 말하는 존재의 특성, 즉 존재는 “단일성 그 이상이자 동일성 그 이상인 것”으로서, 차이를 지양하는 변증법적 종합이 아니라 매번 불일치한 것들의 새로운 소통 관계를 창출하며 “변환적 통일성(transductive unity)”을 갖는다는 말은 관계론적 미디어론을 사고하는 핵심적인 것이 된다.\n시몽동에 의하면 개체 발생의 조건들과 동시에 개체 개념 그 자체도 바뀐다는 것인데, 이는 개체의 특성을 ’정태적 형상(닫힌 체제)’이 아니라, 매개와 소통의 ’관계적 기능’에서 찾는다는 것을 의미한다. 미디어는 개별적 존재로서는 아무런 의미가 없고 어떤 미디어 짜임관계(constellation)에 존재하느냐에 따라 개별적 미디어의 특성이 생겨난다는 매클루언의 미디어론 역시 관계론적 미디어 존재론의 특성을 잘 보여준다. 과도한 일반화의 오류를 범할 수도 있지만, 이를 나름대로 도식화 해보면, 매클루언의 미디어론과 시몽동적 개체발생론의 개념을 결합한 이해가 될 수도 있다고 본다. 여기서 ’[ ]’는 개체화된 기술적 대상을 의미한다. 이런 도식화는 인공지능이라는 기술적 대상의 전개과정을 파악하는데도 유용하다고 생각된다.\n신문이라는 기술적 대상은 신문이 만들어지기 이전의 퍼텐셜들로 가득한 전개체적 준안정적 상태(문자, 종이, 독해력 등등의 결합에 의한 상태)에서 개체화된다. 라디오라는 미디어의 등장은 신문 미디어의 연합환경을 변화시키고, 라디오에 의해 이전 신문과는 다른 개체로 변환(transduction)되며 상전이한다. 이 상전이(dephase)된 신문은 이전의 신문과는 다른 기술적 대상이며 새로운 개체인 [신문(라디오)]가 된다. 이런 과정은 새로운 미디어들의 등장 이후에도 계속 진행된다. 신문은 애초 자신의 퍼텐셜을 모두 소진하지 않은 상태에서 계속 다른 개체들과의 앙상블을 이루는 변환과정에서, 자신을 변환시켜나간다. 인터넷이 등장한 이후에도 신문은 애초 자신이 지닌 퍼텐셜을 모두 소진하지 않으면서 상전이를 시도하고, 이런 변환의 과정은 다른 기술적 대상들도 동일한 과정을 통해 개체초월적인 대상으로 미디어 앙상블 속에 존재한다. 신문이나 라디오 등 모든 미디어는 독자적이고 완결된 형태의 개별적인 존재로 다른 미디어와 관계를 맺는 것이 아니라 위와 같은 과정 속에서 개체화되며 끊임없이 상전이를 하고 있는 것이다.\n매클루언은 “이전에 존재하던 모든 미디어는 새롭게 등장하는 미디어의 내용이 된다.”는 말을 통해, 미디어의 짜임관계에 진입하는 모든 미디어는 다른 미디어들과의 관계 속에서 자신의 존재를 드러낼 수밖에 없다는 점을 밝히고 있다. 이를 볼터와 그루신(Bolter & Grusin, 2000)은 ’재매개(remediation)’라고 개념화하고 있는데, 그들이 말하는 재매개의 과정은 미디어의 변환과 상전이를 말하고 있는 것이라고 생각한다. 매클루언이 관계론적으로 그리고 발생론적이고 순환적인 측면에서 미디어를 바라보고 있다는 점은 그의 《미디어의 법칙》(McLuhan & McLuhan, 1988)에서 ’테트라드(tetrad)’라는 개념을 통해 자세히 밝히고 있다(김상호, 2004).\n앞서 살펴본 것처럼 시몽동은 이미 주어져 있는 존재들에 의해서 관계가 만들어지는 것이 아니라, 존재는 관계를 통해 발생하고 발생된 존재 역시 관계망에 새로운 구조를 부여하며 지속적으로 변환해나가는 것임을 밝힌바 있다. 즉 관계맺음이라는 활동에 의해 존재가 구성되는 과정을 보여준다. 이는 생성이 곧 존재의 ’구성적(constitutive) 원리’임을 보여주는 것이다. 시몽동의 개체화 이론에서 생성은 매번 독특한 관계에 의해, 더 정확히 말하면 관계맺음이라는 활동에 의해 존재를 구성한다. 관계맺음은 존재자들 사이의 이차적 연관이 아니라 존재자를 발생시키는 근본적이면서도 구체적인 작용이다.4\n시몽동과 마찬가지로 매클루언 역시 사회 속에 존재하는 하나의 사물을 자기완결적이고 단일성을 지닌 실체로 바라보았다기보다는 지속적인 전개과정을 지닌 하나의 과정적 실체로 파악했다. 사람들이 만들어내는 인공적인 산물들의 그물망, 즉 다른 사물들과 제도 그리고 그것과 관련된 관념 등의 그물망(앙상블) 속에서 그 존재의 성격이 형성되는 것으로 파악했다고 볼 수 있다. 시몽동은 앞에서 살펴보았듯이 전개체적 상태에서 생성된 존재인 개체는 현실화한 잠재성이다. 이 개체는 지속적으로 상전이 하며 변환한다는 것은 앞서 살펴본 바와 같다. 따라서 시몽동에게 지금 우리 현실 속에서 만나는 개체로서의 미디어는 연합환경과 앙상블 속에 존재하고 변환 중에 있는, 일종의 잠정적인 현실태라고 할 수 있다. 이것이 실체론적 존재론에 반대하는 그의 관계론적 존재론의 핵심이라고 할 수 있다면, 매클루언의 미디어론 역시 관계론적 미디어 존재론이라고 부를 수 있을 것이다. 왜냐하면 매클루언의 핵심 주장인 “미디어가 메시지이다”라는 말이 관계론적 존재론을 잘 표현하고 있다.\n매클루언은 우리에게 어떤 새로운 사물 혹은 미디어가 주어질 때, 그 미디어는 독자적으로 혹은 새로운 단일 미디어만 주어지는 것이 아니라 새로운 미디어를 둘러싼 연합환경도 동시에 새롭게 주어진다는 점을 강조하고 있다. 즉 항상-그리고-이미(always already) 서로 연결되어 있는 미디어들 간의 관계가 동시에 주어진다는 것이다. 그리고 우리에게 개별적인 존재처럼 보이는 미디어들이 개별적으로 존재하는 것이 아니라 눈에 보이지 않는 힘들의 작동과정이 존재하고 그 과정의 결과물로서, 그것도 준안정적인 형태로서 우리에게 주어진다는 것이다. 보이는 구체적 미디어와 보이지 않는 미디어라는 이러한 두 층위의 미디어가 작동하는 과정을 그는 미디어의 엔텔레키라고 부르고 있다. 즉 어떤 현존하는 미디어도 그 미디어의 잠재적 퍼텐셜 에너지가 소진된 상태가 아니라 다른 항들과의 결합에 의해서 다른 것으로 상전이가 일어날 수 있는 상태라는 것이다. 앞서든 예로 보면, 신문의 등장 초기와 라디오라는 매체의 등장 이후, 그리고 텔레비전과 인터넷이라는 연합환경들의 결합은 준안정적으로 존재하던 신문의 형태가 지속적으로 다른 개체로 개체화하는 변환의 과정을 계속한다는 것이다. 그래서 우리가 지금 논의하고 있는 인공지능이라는 대상이 어떤 상태 혹은 관계론적인 지평에 놓여있는가를 검토하는 것은 그것에 대한 윤리적 검토에 있어 근본적인으로 요청되는 질문이라고 할 수 있을 것이다.\n인공지능의 윤리적인 문제를 검토함에 있어 가장 흔하게 제기되는 비판은 차별과 편향성에 관한 것이라고 할 수 있다. 인공지능을 가능하게 하는 기계학습 알고리즘은 분석의 정확성을 확보하고자 하는 것을 늘 최우선 과제로 두지만, 그것이 늘 ‘옳은’ 방향으로 나아가는 것을 담보하지는 않는다. 알고리즘을 ’규정된 코드의 집합이라기보다는 사회적으로 구성되어 제도적으로 운영되는 체계’로 이해되어야 한다고 주장하기도 하고(오세욱, 2018), 인간과 비인간이라는 이분법에 갇혀 알고리즘을 객관적이고 중립적인 기술적 대상이라고 보는 관점은 환상이라고 지적하기도 한다(허유선, 2018). 이들은 공통적으로 알고리즘이 구성하는 힘이 있다고 간주하며, 바로 그것 때문에 알고리즘에 편향성과 차별의 가능성이 내재한다고 주장한다. 차별과 편향에 대한 비판적 주장의 요지는 크게 두 가지다. 첫째, 알고리즘의 기술적 특성 때문에 분석 결과물이 내재적으로 편향되어 있다. 알고리즘의 내재적 편향의 원인은 크게 데이터에 의한 편향과 알고리즘 설계 혹은 모델에 의한 편향으로 구분된다. 먼저 데이터에 의한 편향에 대해 살펴보도록 하자. 자동화 및 예측을 목표로 하는 모든 알고리즘은 훈련 데이터를 수집하고, 이를 통해 반복적인 기계학습을 요한다. 이 과정에서 데이터는 광범위한 그룹으로부터 추출되어야 하며, 충분한 다양성을 보장할 수 있도록 데이터 클리닝 과정을 거쳐야 한다. 그러나 데이터가 현실을 대표하지 못하고 일부만을 표상할 경우, 알고리즘 편향이 발생한다. 즉, 알고리즘의 내적 논리로 인한 편향성은 데이터에 의한 편향에 비해 보다 적극적으로 ’인간의 개입’이 문제시된다. 여기서 중요한 것은 인간의 그릇된 판단이며, 따라서 알고리즘의 문제는 ’기술의 문제’라기보다는 알고리즘을 개발하고 설계하며 학습시키는 인간 행위자의 가치체계, 판단 과정, 나아가서는 사회적이고 역사적인 맥락이 문제가 된다. 이 경우 최우선 과제는 의사결정에 관여하는 인간의 불합리한 판단, 역사적 차별, 행위에 의한 편향 등을 제거하는 일이 된다.\n둘째, 알고리즘이 제시하는 내용들이 편향되어 있다. 데이터나 알고리즘 설계 구조를 비판적으로 검토한 연구들은 어떤 결과물이 도출되는 ‘과정’을 비판하고 있다면, 알고리즘의 제시 내용을 비판하는 연구들은 사용자가 누리는 ‘콘텐츠의 내용’을 문제시한다. 여기서는 주로 추천 알고리즘의 핵심적인 비판 대상이 된다. 현재 많은 웹사이트와 거대 플랫폼은 추천 알고리즘을 콘텐츠 구성 및 제공의 기본 형식으로 채택하고 있다. 미디어 이용자들의 정보에의 접근과 선택은 추천 알고리즘의 강력한 영향 아래에 있으며, 추천 알고리즘은 정보의 게이트웨이로 기능하면서, 과거 매스미디어와는 다른 방식으로 정보의 흐름을 재구성, 통제한다. 인공지능의 차별이나 편향성에 대한 비판은 근본적으로 인공지능이 재현하는 현실이 왜곡되어 있음에 대한 비판이다. 그것이 편향되어 있건, 기존의 차별을 강화하든 간에 문제는 인공지능이라는 기술이 현실을 ‘제대로’ 반영하지 못한다는 게 요지다. 그러면 여기서 재현의 문제는 어떻게 해결될 수 있는가? 다수 연구들이 공통적으로 요구하는 것은 ’제도적 대응’이다. 제도적 대응은 국가적 차원에서 알고리즘 편향성 문제를 해결하기 위한 법을 제정하거나, 기업이 자체적으로 알고리즘 편향성을 방지하기 위한 제도나 위원회를 설립하는 것을 의미한다. 그리고 제도적 대응을 통해 달성해야 하는 바는 알고리즘의 투명성, 설명가능성, 다양성이다.\n차별이나 편향에 비해 중립성은 (상대적으로) 더 나은 가치이며, 기업이나 국가 기관에 대한 이용자의 감시 또한 어느 정도의 효과를 산출할 것이다. 그러나 문제는 객관적 재현이나 중립적 채널의 문제는 유일한 해결책이 아닐뿐더러, 단 한 번도 현실화된 적이 없다는 것이다. 앞서 살펴본 내용에서 보듯이, 기술의 가치를 투명성과 중립성으로 설정할 때, 우리는 투명한 것이 곧 좋은 것이라는 함정에 빠지게 된다. 그러나 모든 미디어는 기술-인간간의 관계론적 파악에서 비판적으로 검토했듯이, 언제나 특정한 방식으로 지각과 경험을 구성하기 때문에 투명할 수 없을뿐더러, 투명함은 좋음을 보장하지 않는다. 오히려 바로 이 지점에서 인공지능에 대한 윤리적 검토를 시작해야 한다."
  },
  {
    "objectID": "basic.html#인간-기술의-관계를-바라보는-관점",
    "href": "basic.html#인간-기술의-관계를-바라보는-관점",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "1.1 인간-기술의 관계를 바라보는 관점",
    "text": "1.1 인간-기술의 관계를 바라보는 관점\n인공지능이 우리 미디어 환경의 근본적인 변화를 가져올 것이라는 전망은 거의 확실해 보인다. 실제 우리의 삶에서 인공지능이라는 미디어가 전면에 등장한 것은 알파고를 통해 처음 시작되었다면, 이제 일상적 삶의 일부를 구성하는 요소가 되었음을 부정하기는 힘들 정도로 인공지능은 모든 미디어와 결합 되고 있다. 그런데 인공지능과 관련된 새로운 상황에 대면할 때마다 우리는 과거 기술 발전에 대해 열광하던 모습들과는 달리 인간의 지위와 존재 조건에 대한 심각한 우려를 동시에 제기하고 있다. 새 기술을 바라보는 인간의 시선은 항상 기대와 우려가 교차했지만, 지금 던지는 질문은 그 기술로부터 파생될 각종 사회경제적 문제만이 아니라 ’인간이라는 존재는 무엇인가’라는 보다 근본적인 존재 물음이라고 할 수 있다.\n그런 의문과 질문 속에는 오랜 기술과 미디어의 변화과정에서 제기되었던 기술과 인간의 관계에 관한 오래된 형이상학적 구도가 여전히 개입하고 있음을 알 수 있다. 가장 대표적인 것으로 ’주인으로서의 인간과 노예 혹은 도구로서의 기술’이라는 아주 오래된 도식이 그것이다. 최근 주인을 누르고 해방된 노예 혹은 주인을 압도하는 노예처럼 비춰진 ’알파고’라는 한 기술적 대상을 바라보는 시선도 이런 구도의 연장선에 있으며, 각종 추천 알고리즘으로 무장한 각종 미디어들이 우리의 선택을 도와준다는 외형을 가장하며 우리의 취향에까지 개입하고 있는 양상은 이런 경향성의 현재적 모습이다.\n여기에 더해 검토해야 할 또 다른 철학적 쟁점은 기술적 대상 혹은 미디어를 하나의 대상으로 파악하여 인간과 대결하는 구도로 지각하는 방식이다. 즉 기술 혹은 미디어를 독립적으로 존재하는 실체적 대상으로 간주하고, 그 독립된 실체와 인간의 관계를 상정하고 진행하는 생각의 방식이다. 이런 구도는 기술적 대상들 특히 지금 논의의 대상인 인공지능을 하나의 실체적이고 독립적인 대상으로 파악함으로써 인간과 대립시키는 방식으로 파악하는 방향으로 나아가게 한다. 이는 기술에 대한 단순한 의인화를 넘어 서양 철학의 오래된 실체론적 인식구조가 밑바탕에 깔려있기 때문이다.\n인간과 기술의 관계에 대한 인식변화는 변화하는 기술 환경 속에서 인간의 위치, 즉 인간-기술의 관계에 대한 재검토는 물론, ’인간’과 ’기술’에 대한 존재론적인 차원의 근본적인 문제를 제기하고 있다. ’포스트휴먼(post-human)’이라는 인간에 대한 새로운 개념을 주장하는 사람들에 따르면, 당연하게 우리에게 주어진 실체처럼 보이는 ’인간’이 사실은 하나의 역사적 인식들과 실천들 속에서 구성된 것이며, 우리가 지금까지 지녀온 인간이라는 개념이 현재와 같은 기술 환경에서는 더 이상 인간을 설명하는데 적절하지 않다고 주장한다(Hayles, 1999. 참조). 지금 우리에게 일반적으로 받아들여지는 ’인간’이라는 개념, 즉 알파고와 이세돌처럼 기술의 대척점에 놓인 바로 그 인간이라는 개념이 사실은 역사적, 사회적으로 구성된 개념이기 때문에 새로운 상황에서는 새로운 인간 개념이 생겨날 수 있다는 것이다. 물론 이들 사이에서도 미묘한 차이가 존재한다. 그러나 보다 정확하게 말하자면 인간의 개념규정이나 그 개념이 포섭하는 범위의 문제가 아니라 인간과 기술 혹은 인간과 그를 둘러싼 환경의 관계에서 인간이 파악되는 방식이 변화하는 것이라고 보는 것이 더 타당하다. 왜냐하면 인간은 어떤 방식으로든지 주변 환경과 관계를 맺어 왔고, 그 속에서 인간의 본질 혹은 존재론적 지위를 규정 받아왔기 때문이다. 인간을 중심으로 생각하면 인간을 어떻게 파악하느냐에 따라 그를 둘러싼 환경이 정의될 수 있겠지만, 인간은 환경과 더불어, 그 관계 속에서 환경을 정의하는 존재이자 환경에 의해서 정의 당하는 존재이기도 하다는 점을 받아들인다면 이야기는 달라진다.\n새로운 기술 환경에 대한 또 다른 시선으로 포스트휴머니즘과는 약간 결을 달리하는 트랜스휴머니스트(trans-humanist)를 주창하는 이들이 있다. 이들은 현재 인간을 넘어선 포스트휴먼으로 이행하는 특이점을 향해가는 인간향상기술의 발전을 긍정적으로 파악하며, 인간의 사이보그화로 정의되는, 즉 새로운 형태를 지닌 인간종(種)을 종적인 진화로 간주한다. 그들은 이처럼 변화된 인간을 자율적인 주체로서의 인간이 역량을 확장시켜나가는 근대적 계몽의 일환으로 간주한다. 근대 이후 인간이 추구해온 인간 해방의 연장선에서 포스트휴먼 현상을 바라보는 것이다. 이런 생각을 대표하는 이들로, 보스트롬(Bostrom), 커즈와일(Kurzweil, 2005), 모라벡(Moravec, 1999) 등을 들 수 있는데, 이들은 자연적으로 태어난 인간의 육체적 한계를 현대 기술력으로 극복할 수 있고, 이것은 이전의 인간과는 다른 포스트휴먼을 약속한다고 믿는다. 이들의 시선은 앞서 말한 서구의 형이상학적 입장이 극단화된 것으로 볼 수 있는데, 특히 인간의 육체적 변환이 인간의 본질과 인간과 그 주변 환경과의 관계를 변화시킬 것이라는 점은 고려하지 않는 듯 보인다.\n이들이 말하는 새로운 인간종은 인간의 육체를 철저히 영혼에 종속적인 것으로 보는 테카르트적 인간의 극단적 형태라고 볼 수 있다. 이런 점에 대해 비판적인 시선을 보내고 있는 캐서린 헤일스(Hayles, 2013), 배드밍턴(Badmington, 2000), 캐리 울프(Wolfe, 2010), 스테판 헤어브레히터(Herbrechter, 2013) 등과 같은 비판적 포스트휴머니스트라고 불리는 사람들은 트랜스휴머니즘의 주창자들의 사고가 인간에 대한 다소간의 몰이해에 기초해 있다고 비판한다. 이들은 기술문화의 급진적 변화에 대해 인정하고 그것을 받아들이면서도 트랜스휴머니즘이 근본적으로 전제하고 있는 데카르트적 인간중심주의 또는 자유주의적 휴머니즘에 대해서는 비판적 태도를 취한다. 이들은 인간과 비인간(포스트휴먼)의 경계 자체가 지닌 불확실성과 서로의 존재에 깊숙이 스며든 상대의 영향에 주목하며, 포스트휴머니즘 안에서 여전히 작동하고 있는 서구의 오래된 형이상학적 편향인 인간중심주의의 유령을 끄집어내어 해체하고자 한다."
  },
  {
    "objectID": "basic.html#인간-기술인공지능의-관계-문제-인간중심주의anthropocentrism",
    "href": "basic.html#인간-기술인공지능의-관계-문제-인간중심주의anthropocentrism",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "1.2 인간-기술(인공지능)의 관계 문제 : 인간중심주의(anthropocentrism)",
    "text": "1.2 인간-기술(인공지능)의 관계 문제 : 인간중심주의(anthropocentrism)\n우리는 인간이라는 종적인 틀 속에서 지각하고 판단하며, 그런 구속성 속에서 무엇인 옳고 바람직한 것인가를 판단할 수 밖에 없다. 우리가 다른 존재가 아니기 때문에 우리가 생각하는 모든 것들은 우리 인간 중심적인 판단이 될 수 밖에 없다고 생각할 수 있을 것이다. 그렇기 때문에 우리 인간을 중심에 두고 기술과 자연을 파악하는 방식은 지극히 당연한 것처럼 보인다. 그러나 사실 이런 인간중심적 사고는 근대를 관통하며 만들어진 구도이며, 서양 형이상학이 끝없이 이런 인식에 동력을 공급한 결과이지, 인간의 존재 일반의 당연한 인식으로 받아들일 수는 없다. 특히 기술에 관한한 이런 인간중심주의적 경향은 더 강력한데, 기술은 인간이 사용하기 위해 고안했고 만들었으며, 따라서 그 기술은 주인인 인간을 위해 사용되는 도구이며, 인간의 목적에 부합하는 중립적인 대상으로 파악하는 것이다.\n기술에 대한 인간중심주의가 보여온 인식을 가장 잘 드러내 주는 것이 ’기술의 도구성’에 대한 인식과 더불어 근대 이후 과학기술의 본질을 둘러싸고 지속적으로 주장되어온 가치중립성이라는 개념이다. 이는 자연적 사실을 탐구하고 발견하며, 이를 체계적으로 설명하는 활동인 과학과 그에 기반한 기술은 그 자체로는 ’가치중립적’이라는 주장이다. 이런 주장들은 기술이란 수단일 뿐 그 자체는 선도 아니고 악도 아니며, 인간이 그것에서 무엇을 만들어 내는지, 기술이 인간에게 어떻게 기여하는지, 인간이 기술을 어떤 조건 아래 두는지가 중요하다고 본다. 이들이 보기에는 기술의 가치중립성은 과학기술자에게도 객관적이며 중립적인 태도를 보장하는데, 그들은 대상을 있는 그 자체로 파악하며 오로지 객관성과 보편성의 원리만을 추구한다는 입장을 가능하게 한다. 이 같은 이념에는 근대가 표방하는, 과학기술이야말로 가장 합리적이고 이성적 사유의 표준이라는 신념과 신뢰가 전제되어 있다.\n기술 발전의 자율성 또한 근대인의 사유를 부단히 사로잡아온 개념이다. 그것에 따르면 과학기술의 발전은 자가발전적 메커니즘처럼 그 속에 내재된 자율적 원리에 따라 움직인다. 곧, 기술 발전이란 인간의 의지나 사회적 관계에 의해 결정되는 것이 아니라 기술의 자체 발전적 메커니즘을 따른다는 것으로, 이는 기술 선택의 자동성, 기술의 자율적 증식, 기술들 사이의 필연적 연결로 이어질 수 있다. 이 같은 기술발전의 자율성과 그것을 정당화하는 객관성의 이념은 기술 발전이 사회변동의 일차적 요소로 정치, 경제, 사회, 문화를 주도한다는 기술결정론으로 이어진다.\n그런데 우리가 기술을 중립적이라고 여길 때, 우리는 최악의 경우에 처해질 수 있다. 왜냐하면 오늘날 사람들이 인간을 중심에 두고, 기계의 주인으로서 인간을 설정하기 위해 도입한 기술의 도구성과 가치중립성은 우리를 기술의 본질에 대해 완전히 맹목적이게 할 수 있기 때문이다. 가치중립적인 기술은 오로지 사람들에 의해 선하게도 악하게도 사용될 수 있다는 양면적 입장은 언뜻 보기에는 타당해 보이지만, 정작 기술이 미치는 영향력을 논의하기에는 지나치게 단순한 감이 있다. 이는 선한 기술 혹은 악한 기술이란 구분과는 별개로 기술은 자신을 둘러싼 모든 환경에 이미 영향을 미치고, 삶의 맥락 전체를 바꾸어 버릴 수도 있기 때문이다.\n하이데거는 기술중립주의가 궁극적으로는 기술결정론으로 환원되어 기술 개발과 그것의 산업적 이용을 무제한적으로 허용함으로써 초래할 수 있는 엄청난 위험의 가능성을 경고하고 있다. 그는 서구의 인식론적 틀을 구성해온 기술의 가치중립적 본질과는 거리를 두고 있는 것인데, 그의 기술철학적 질문은 현대 기술의 위험에 대한 지적으로 이어진다. 그는 기술이 도전적이고 총체적방식으로 기술에 내재된 고유한 방식으로 인간을 비롯한 모든 존재들을 닦달하고 몰아세우고(ge-stell) 있다고 비판한다. 기술은 스스로의 본질을 드러내지 않고 위장한다. 인간은 스스로 기술을 통제하고 있다고 착각하지만, 기술의 힘은 이미 인간의 통제를 벗어나 있다는 것이다. 이것은 모든 사물을 대상화, 부품화할 뿐 아니라 심지어 인간마저도 대상화, 부품화한다. 지금 거의 모든 회사에 설치되어 있는 인사부의 영어 명칭은 ’Department of Human Resources’다. 즉 인간도 자원이며, 필요한 곳에 사용되는 부품이요 대상이라는 말이다. 그럼에도 사람들은 여전히 기술을 도구처럼 장악한다고 믿고 자신의 욕망대로 다루려 하는데, 하이데거는 기술이 인간의 의지를 벗어날 가능성이 커질수록 기술을 지배하려는 사람들의 욕구 또한 절박해질 것으로 보았다.\n인간의 활동은 마침내 기술 발전과 짝하여 존재하는 모든 것들을 착취하는 행위에 이른다. 하지만 모순적이게도 기술의 위장술은 끝내는 인간 스스로를 착취의 대상으로 전락시킨다. 즉 인간중심주의에서 발현한 인식틀, 즉 기술을 인간을 위한 도구로 파악하는 것과 인간의 목적에 부응하는 중립적인 대상으로 파악한 것과 같은 기술과의 이상적인 관계맺음은 본질적으로 가능하지 않은 것이라고 비판하는 것이다. 당연하게도 기술에 대한 우리의 이해 방식은 최종 국면에서는 항상 윤리적인 질문으로 끝나게 되는 이유가 바로 이것이며, 인간-기술의 관계에 대한 철학적이고 근본적인 접근방식에 대한 이해가 중요한 이유도 여기에 있다."
  },
  {
    "objectID": "basic.html#인간과-인공지능에-대한-관계론적-접근",
    "href": "basic.html#인간과-인공지능에-대한-관계론적-접근",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "1.3 인간과 인공지능에 대한 관계론적 접근",
    "text": "1.3 인간과 인공지능에 대한 관계론적 접근\n트랜스휴먼으로 대표되는 기술이 초래하는 미래에 대한 낙관론은 인간과 기술에 대한 특정한 관점에 기초하고 있다. 즉 이들은 기술과 인간의 관계를 파악하는데 있어서 ‘실체론’적이라고 불릴 수 있는 관점을 드러내고 있다. 실체론은 관계를 맺는 두 실체가 이미 존재하고 있고 그것들이 만들어내는 것이 관계라고 생각하는 입장이다. 비록 그것이 역사적 과정 속에서 변화를 거치면서 만들어진 구성적인 요소라고 하더라도, 이들의 입장은 ‘인간’과 ‘기계’ 혹은 ’인간’과 ’미디어’라는 두 실체를 먼저 상정하고 논의를 진행한다.\n한편 이와 다른 시각도 존재하는데, 이 실체라는 것들은 관계가 만들어내는 전체 앙상블(ensemble)을 통해서만 그 현실적인 존재의 특성이나 모습을 지닐 수 있다고 생각하는 방향도 있다. 이를 관계론적인 접근이라고 할 수 있다. 이 관점에 따르면, 실제 혹은 존재는 생성되는 것이지 이미 주어진 것이 아니라는 것이며, 생성된 존재 역시 외부와 단절적으로 존재하는 것이 아니라 끊임없이 외부 환경(milieu)과 소통하며 변환되어 가는 존재라고 할 수 있다. 이런 존재에는 생명체만이 아니라 기술적 대상들도 포함된다고 할 수 있다.1 지금 우리가 관심을 가지고 있는 인공 지능이라는 기술적 대상 역시 하나의 실체적인 존재로서 바라보기 보다 전체 미디어 환경 속에서 이 기술이 다른 존재들과 맺고 있는 관계의 양상 속에서 기술적 대상으로서 인공지능은 존재하게 된다는 것이다.\n이 점에 대해 기술철학자 시몽동(Simondon)은 분명하게 언급하고 있는데, 그는 “기술적 대상은 인공적인 존재로 고찰되어서는 안 된다.”고 말한다(Simondon, 1989/2011, 383쪽). 왜냐하면 그는 우리가 흔히 생각하는 방식으로 자연적인 것과 인공적인 것으로 구분하지 않기 때문이다. 시몽동은 “인공성은 자연 생산물의 자발성에 대립하는 것으로서 기술적 대상의 제작된 기원을 가리키는 특성이 아니라, 인간의 인공화하는 행동에 내재하는 것”이라고 주장하는데, “(인공화하는) 이 행동은 자연적인 대상에 대해서든 완전히 제작된 대상에 대해서든 다 개입한다.”고 본다. 그는 예를 들어 온실 속의 꽃은 자연적인 대상처럼 보이지만 원래 자연적인 대상에 대한 조절들이었던 것이 온실 속의 인공적인 조절들이 되었다고 본다. 인간의 개입에 의해 인간과 관계맺음의 관계 양상으로서의 인공화가 진행되는 것이지, 그것이 애초에 인간에 의해 제작되었기 때문에 인공적인 것은 아니라는 것이다. 즉 기술적 대상들은 만들어졌다는 이유로 인간에게 단순히 도구적 대상으로 사용되는 존재가 아니며, 생명체로서의 현실적 모습을 띠고 있다고 해서 자연적인 것만은 아니라는 것이다. 여기서 중요한 지점은 인간의 개입에 의해 탄생하는 존재론적인 차원 즉 관계맺음의 문제이다.2\n기술적 대상과 인간의 관계 양상이 기술에 대한 본질을 파악할 수 있는 근본적인 것이라는 점은 “기술의 본질은 전혀 기술적인 것이 아니다”라고 말한 하이데거에서도 찾아 볼 수 있듯이, 기술적 대상에 대한 이해는 기술 그 자체에 대한 탐색으로는 불가능하다. 시몽동이 지적하고 있는 점 역시 이 부분이라고 할 수 있다. 기술적 대상에 대한 탐구는 그것이 발생하기 이전과 그 개체가 발생되는 과정의 여러 요소들 그리고 개체 외부의 요소들과 맺게 되는 앙상블들 속에서 계속 다른 존재로 변환을 진행하는 전체 과정을 함께 고려해야 기술적 개체 혹은 대상(여기에서는 인공지능)의 의미를, 특히 인간과 더불어 지니는 의미를 파악할 수 있다.\n그러나 기술적 대상들을 파악하는데 있어 이런 시각을 확보하는 것은 ‘기술적인 것’ 또는 ’인공적인 산물’에 대한 문화적 편견으로 말미암아 쉬워 보이지 않는다. 이를 시몽동은 다음과 같이 비판적으로 지적하고 있는데, “예술을 예술의 대상들로 환원시키는 것, 인간성을 단지 성격적 특징들을 지니고 있을 뿐인 일련의 개인들로 환원시키는 것, 이와 같은 것이 바로 기술적 실재를 기계들의 집합으로 환원시킬 때 취하는 태도다. 전자는 세련되지 못한 것으로 쉽게 판명되는 대신에 후자의 경우에는 마찬가지로 파괴적인 환원이 실행되고 있음에도 불구하고 문화의 가치들에 부합하는 것으로 간주된다.”는 것이다(Simondon, 1989/2011, 211쪽).\n우리가 인공지능이 무엇이고 그것이 우리에게 던지는 질문이 무엇인가를 파악하기 위해서는, 흔히 진행되는 방식인 기술적 대상 혹은 미디어로서의 인공지능을 인간과 분리하여 그 존재의 특성과 구성 요소 그리고 작동방식을 인공지능의 가장 줌심적 문제로 사고하는 것이 지닌 문제점을 살펴보고, 이런 파악방식만으로는 기술적 대상이나 미디어로서의 인공지능이 지닌 의미를 알 수가 없다는 점을 잘 알고 있어야 한다. 인공지능이라는 미디어의 존재론적 성격 역시 결국 생성적이며, 다른 미디어들과의 앙상블을 통해 계속 변환되는 과정에 놓여있다는 것을 인식한다는 점은 이 미디어에 대한 우리의 윤리적 접근에도 중요한 영향을 미치는 문제이다. 왜냐하면 개별적 미디어로서의 인공지능에 대한 선호나 부정 혹은 환호나 두려움이라는 것이 사실은 윤리적 문제를 고려할 때 아무런 도움을 주지 못하는 전제에 입각해서 전개되는 논의이기 때문이다.\n미디어 철학자 매클루언은 미디어와 인간이 맺는 관계를 미디어와 인간이라는 두 실체 사이의 일로 보지 않는다. 또한 어떤 하나의 미디어도 개별적으로 존재하는 독립된 실체로 여기지 않으며, 이전의 미디어들이 새롭게 등장하는 미디어들에 의해 변환된 구조에 참여한다는 점을 지적한다. 이런 그의 기본적인 입장은 미디어의 발생과 존재 방식을 입체적이고 순환적인 구조로 파악하는 것에 근거하고 있다. 시몽동과 매클루언의 주장의 유사성은 단순한 우연이 아니라, 그들이 기술적 대상을 바라보는 관점 자체가 발생론적 존재론이며, 인간과 기술의 관계를 파악하는 방식도 거의 유사한 지점을 확보하고 있기 때문이다.\n시몽동의 이른바 ’관계의 존재론’은 서양철학의 전통에서는 현저하게 예외적인 개념적 구도를 보여준다. 그는 존재냐 생성이냐의 이분법을 거부할 뿐만 아니라 전통적인 사상과는 다르게 생성으로부터 존재를 구성하려는 시도를 하고 있다(황수영 2015, 118쪽). 또한 매클루언이 생각하는 미디어와 인간의 관계는 미디어의 측면에서 보면 미디어는 ’인간의 확장된 몸’이며, 인간의 측면에서 미디어를 보면 인간의 몸이란 ’스며든 기술’로 파악될 수 있다(김상호, 2009). 다시 말하자면, 매클루언과 시몽동이 공유하는 기본적인 생각은 생성과정을 중심에 두고 그 생성된 대상으로서 기술적 대상-미디어를 파악하고, 인간과 기술적 대상들은 상호 협력적 관계 속에서 그 존재론적 의미를 획득한다는 것이다. 따라서 이 두 사상가의 사고 속에서 주인으로서의 인간의 지위와 노예(혹은 도구)로서의 기술적 대상-미디어의 지위는 더 이상 존재할 수가 없다. 매클루언이 그의 책 속에서 보여준 기술적 대상이나 미디어의 이와 같은 지위의 상승 혹은 존재론적 재설정은 기존의 형이상학적 입장이나 인간중심주의적 입장에서 보았을 때, 그의 입장을 기술결정론적이라고 비판할 수밖에 없을 것이다.\n기술에 대한 인간중심주의적 접근을 벗어나 새로운 관계 설정을 주문하는 철학자들의 주장은 보기에 따라 사람들에게 충격적으로 다가갈 수 있다. 그러나 이들의 이런 주장을 살펴보면 새로운 미디어 환경에서 벌어지는 존재들의 앙상블에서는 새로운 인식이 요청되고, 여기에는 결과적으로 새로운 윤리적 질문과 요청이 생겨날 수 밖에 없다. 다음 주장들은 인간-기술 간 새로운 관계 설정의 요청 사항들이다.\n\n“인간이 기술적 대상들보다 열등하거나 우월하지 않아야 한다.”(Simondon, 1989/2011, 129-130쪽).\n“기술(또는 다양한 방식으로 확장된 신체)을 정상적으로 사용하는 사람은 그 기술에 의해 끊임없이 변형되고, 다시 그의 기술을 새롭게 변형시키는 방법들을 찾아내게 된다. 마치 벌이 식물의 생식기이듯이 인간은 말하자면 기계 세계의 생식기로서 언제나 새로운 형태들을 수태시키고 진화시키는 것이다.”(McLuhan, 1964/2011, 107쪽).\n“인간의 진정한 본성은 연장들의 운반자, 그래서 기계의 경쟁자가 아니라, 기술적 대상들의 발명가이며 앙상블 안에 있는 기계들 사이의 양립가능성의 문제를 해결할 수 있는 생명체다. 기계들의 수준에서, 기계들 사이에서, 인간은 그 기계들을 조정하고 그것들의 상호 관계를 조직화한다. 인간은 기계들을 다스리기보다는 양립가능하게 만들며, 정보를 수용할 수 있는 열린 기계의 작동이 내포되어 있는 비결정성의 여지에 개입하여 기계로부터 기계로 정보를 번역해주고 전달해 주는 자다. 인간은 기계들 사이의 정보 교환이 갖는 의미작용을 구축한다. 인간이 기술적 대상에 대해 갖는 적합한 관계 맺음은 생명체와 비생명체 사이의 접속으로 파악되어야만 한다.”(Simondon, 1989/2011, 385쪽).\n\n기술적 대상들/미디어는 각기 떨어져서 존재하는 요소가 아니며, 다른 수준에서 다른 형태로 존재하는데, 그것에 개입하는 중요한 요인이 바로 인간이라는 사실이 시몽동과 매클루언의 사상의 공통점이다. 시몽동에 따르면 기술적 대상들은 요소-개체-앙상블의 세 수준들에서 존재하며, 기술성은 이 세 수준들을 따라 변환하며 역사적으로 발전해 나가고, 이 기술성의 발달 정도에 따라 기술적 대상들과 인간 사이의 관계 양상도 달라진다는 것이다.\n이런 발생론적 혹은 생성의 존재론과는 달리 기술에 대해 그리고 미디어에 대해 전통적인 접근들이 지니고 있는 공통적인 전제는 기술이나 미디어의 존재 양태를 ’형상-질료’라는 서양 형이상학의 전통적인 이분법에 근거하여 파악하고 있는 것이다. 기술이나 미디어를 인간의 의지나 필요에 따라 사용 혹은 처분 가능한 대상이라고 보는 관념은 주인으로서의 인간과 노예로서의 기술이라는 관념이 여전히 투사, 적용되고 있다고 볼 수 있다.3 이는 단순히 기술/미디어와 인간의 관계를 파악하는 것에 국한된 것이 아니라 어떤 의미에서는 서양 철학사 전반에 걸쳐 형성된 일반적인 사고의 방식과 관련이 있다. 흔히 ’생성의 철학(혹은 관계론적 접근)’과 ’존재의 철학(개체성의 철학)’으로 대비되는 이 긴장은 매클루언과 시몽동의 발생론적 존재론을 파악하는 시작 지점이라고 할 수 있을 것이고, 인공지능을 파악함에 있어 철학적으로 대비되는 두 접근방식을 점검해볼 수 있는 기본적인 지점이 될 수 있을 것이다."
  },
  {
    "objectID": "basic.html#더-생각해볼-내용",
    "href": "basic.html#더-생각해볼-내용",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "3.1 더 생각해볼 내용",
    "text": "3.1 더 생각해볼 내용\n과거에 이루어진 의도적 불공정, 예컨대 유색인종, 여성, 장애인, 성소수자 등에 대한 차별들은 데이터 학습 과정에서 배제되지 못함으로써 보호 속성을 지닌 인구 집단에 대해 차별로 이어질 가능성이 존재한다. 단적인 예가 2020년 한국의 모 스타트업 기업에서 개발한 AI 챗봇 ’이루다’이다. 이루다는 20대 여성을 컨셉으로 하고 있었는데, 이용자들이 성소수자에 대한 질문을 했을 때 혐오성 발언을 내뱉을 뿐만 아니라 이루다에 대한 성희롱성 발언에 적극적으로 동조하고 그것을 수용하는 대답을 보냈다. 결국 이루다는 차별과 혐오를 증폭시킬 수 있다는 우려와 함께 이용이 중단되었다.\n\n새로운 기술에 대한 윤리적인 질문과 답변은 기술이 가진 기술적인 특성이나 사용과 관련된 구체적인 특성에 국한되지 않고, 기술의 전체적인 앙상블을 고려해야 한다. 위의 사례를 보면 인공지능 기술이 맺고 있는 관계맺음의 양상은 어떻게 고찰될 수 있고 또 그 속에서는 윤리적 질문은 어떻게 제기될 수 있는가?"
  },
  {
    "objectID": "basic.html#더-읽으면-좋은-자료",
    "href": "basic.html#더-읽으면-좋은-자료",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "3.2 더 읽으면 좋은 자료",
    "text": "3.2 더 읽으면 좋은 자료\nMcLuhan, M. (1964). Understanding Media: The Extension of Man. Gingko Press. 김상호 (역) (2011), 《미디어의 이해》. 커뮤니케이션북스.\nSimondon, Gilbert (1989). Du mode d’existence des objects techniques. Editions Aubier. 김재희 (역) (2011). 《기술적 대상들의 존재양식에 대하여》, 그린비."
  },
  {
    "objectID": "basic.html#참고문헌",
    "href": "basic.html#참고문헌",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "3.3 참고문헌",
    "text": "3.3 참고문헌\n김상호(2002). 기술논의의 전개과정에 관한 비판적 고찰, 《언론과 사회》 10권(4호), 58-89쪽\n김상호 & 이호규(2003). 해롤드 이니스의 커뮤니케이션 사상: 편향을 중심으로, 《언론과 사회》 11권(3/4호), 78-107쪽.\n김상호(2004). 엔텔레키를 중심으로 해석한 맥루한의 미디어 개념, 《언론과 사회》 12권 4호, 79-116쪽.\n김상호(2008). 맥루한 매체이론에서 인간의 위치: 기술우선성을 중심으로, 《언론과학연구》 8권 2호, 84-121쪽.\n김상호(2009). 확장된 몸, 스며든 기술: 맥루한 명제에 관한 현상학적 해석, 《언론과학연구》 9권 2호, 167-206쪽.\n김상호(2011). 한국 텔레비전 테크놀로지의 사회적 수용-매클루언의 접근방식을 중심으로 《방송문화연구》 제23권 1호, 73-107쪽.\n김재희(2008). 베르그손에서 잠재성과 물질의 관계, 《시대와 철학》 제19권 2호, 9-44쪽.\n김재희(2011). 물질과 생성 : 질베르 시몽동의 개체화론을 중심으로. 《철학연구》 제93집, 231-260쪽.\n김재희(2014). 포스트휴먼 사회를 사유하기 위한 하나의 청사진-질베르 시몽동의 기술-정치 학. 《범한철학》 제 72권, 387-414쪽.\n김화자 (2011). 질베르 시몽동의 기술철학에 나타난 ’기술성의 의미: 현대 정보기술문화 이해 를 위한 소고. 《철학과 현상학연구》 제 51집, 35-66쪽.\n브루노 라투르 외(2010). 《인간 • 사물 동맹》, 홍성욱 엮음. 이음.\n연효숙(2013). 들뢰즈의 기관 없는 신체와 개체성의 문제. 《헤겔연구》 제 34호, 259-280쪽.\n이정우(2004). 《개념-뿌리들》, 철학아카데미.\n이지훈(2002). 시몽동: 생명의 자연철학, 《과학과 철학》 제13집, 139-157쪽.\n황수영(2003). 《베르그손, 생명과 지속의 형이상학》, 이룸.\n황수영(2005). 《근현대 프랑스철학，데까르뜨에서 베르그손까지》, 철학과 현실.\n황수영(2009). 시몽동의 개체화 이론: 프랑스 생성철학의 맥락에서, 《동서철학연구》 제53호, 199-224쪽.\n황수영(2014). 《베르그손, 생성으로 생명을 사유하기》, 갈무리.\n황수영(2015). 시몽동의 생성의 존재론에서 물질과 생명의 연속성과 불연속성. 《철학연구》 제 111집. 93-121.\nAristotle (1957). Metaphysica. Oxford University Press. 김진성(역) (2007), 《형이상학》, 이제이북스\nBadmington, Neil. ed.(2000). Posthumanism : A Reader, London：Palgrave.\nBarthelemy, J. & Norman, Barnaby (2015). Life and Technology : An Inquiry into and beyond Simondon. Luneburg : Meson Press\nBergson, Henri(1907). (L’)evolution creatrice. 황수영(역) (2005), 《창조적 진화》, 아카넷.\nBradley, Arthur.(2011). Originary Technicity: The Theory of Technology from Marx to Derrida, London- Palgrave. Macmillan.\nDeleuze, Gilles(1968). Difference et Repetition. 김상환 (역) (2004), 《차이와 반복》, 민음사.\nDe Boever, A. Murray, J. Roffe & A. Woodward (2012). Gilbert Simondon : Being and Technology. Edinburgh University Press.\nGow, Gordon(2001). “Spatial Metaphor in the work of Marshall McLuhan”, Canadian Journal of Communication, 26. 63-80.\nKurzwil, Ray(2005). The Singularity Is Near. 김명남•장시형 (역)(2007), 《특이점이 온다》, 김영사.\nLatour, Bruno(1991). We Have Never Been Modern. 홍철기 역(2009), 《우리는 근대인이었던 적이 없다》, 갈무리.\nHall, Edward(1959). The Silent Language. 최효선(역) (2000), 《침묵의 언어》, 한길사.\nHerbrechter, Stefan (2013). Posthumanism： a Critical Analysis, London- Bloomsbury.\nInnis, H. A. (1950). Empire and Communications. Oxford: The Clarendon Press.\nInnis, H. A. (1951). The Bias of Communication. Toronto: University of Toronto Press.\nMcLuhan, Marshall (1987). Letters of Marshall McLuhan. Molinaro, Matie, Corinne McLuhan & William Toye (eds.) NY: Oxford University Press.\nMcLuhan, M. (1964). Understanding Media: The Extension of Man. Gingko Press. 김상호 (역) (2011), 《미디어의 이해》. 커뮤니케이션북스.\nMcLuhan, Marshall, (1962). Gutenberg Galaxy: The Making of Typographic Man. Toronto: University of Toronto Press.\nMcLuhan, M & Eric McLuhan (1988). Laws of Media: The New Science.\nToronto :University of Toronto Press.\nMoravec, Hans (1999). Robot: Mere Machine to Transcendent Mind, Oxford- Oxford University Press.\nSimondon, Gilbert (1989). Du mode d’existence des objects techniques. Editions Aubier. 김재희 (역) (2011). 《기술적 대상들의 존재양식에 대하여》, 그린비.\nSimondon, Gilbert (1993). ‘The Genesis of the Individual’, in J. Crary & S. Kwinter (Eds.) Incorporations, NY: Zone. pp. 297-317.\nStiegler, Bernard (2009). Technics and Time 2: Disorientation, Stephen Barker tr.. California: Stanford University Press. Wolfe, Cary (2010). What is Posthumanism?, Minneapolis: University of Minnesota"
  },
  {
    "objectID": "basic.html#footnotes",
    "href": "basic.html#footnotes",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "",
    "text": "물론 시몽동은 기술적(물리적) 대상과 생명체의 존재론적 차이를 분명히 하고 있지만, 기술적인 대상이든 생명체든 그것이 개체라는 존재의 발생론적 관점에서 이들 모두를 포괄하고 있다. 그가 예를 드는 결정(結晶)의 생성과 같은 경우가 가장 대표적인 예가 될 것이다. 이를 시몽동은 “기술적 대상들은 단순히 사용도구로서 취급될 수 있는 것들이 아니라 구체화의 정도에서 차이가 나는 특수한 개별자들로서 자기 나름의 발생과 진화를 겪는 것들이다.”라고 지적하고 있다(Simondon, 1989/2011, 9쪽).↩︎\n“열린 기계들의 앙상블은 인간을 상설 조직자로 기계들을 서로 연결시켜주는 살아있는 통역자로 상정한다. 노예집단의 감시자이기는커녕, 인간은 마치 연주자들이 오케스트라의 지휘자를 필요로 하듯이 그를 필요로 하는 기술적 대상들 모임의 상설 조직자다.” (Simondon, 1989/2011, 13쪽) 또한 “지휘자는 모든 연주자들을 모두에게 서로서로 연결시켜주는 통역자인 것이다. 이와 같이 인간은 자기 주위에 있는 기계들의 상설 발명가이자 조정자로 존재하는 기능을 갖는다. 인간은 자신과 함께 작용하는 기계들 가운데 존재한다.” (Simondon, 1989/2011, 14쪽)↩︎\n이런 점에 대해 시몽동은 다음과 같이 지적한다. “힘을 얻기 위해 기계들을 활용하는 장소로서 기술적 앙상블을 취급하는 철학은 기술에 대한 독재적인 철학이라고 부를 수 있을 것이다. 여기서 기계는 단지 수단일 뿐이다. 목적은 자연의 정복, 즉 일차적인 예속화를 이용해서 자연의 힘들을 지배하는 것이다. 그러니까 기계는 다른 노예들을 만들어 내는데 쓰이는 노예인 것이다. 이와 같은 정복의 영감과 노예제 주창자는 인간을 위한 자유의 요청이라는 명분으로 서로 만날 수 있다. 그러나 노예를 다른 존재자들, 즉 인간들, 동물들, 또는 기계들에로 이전시키면서 자유로워지기란 어려운 것이다.” (Simondon, 1989/2011, 183-4쪽).↩︎\n시몽동이 파악하는 생성의 관점은 굳이 메를로-퐁티를 다시 상기시키지 않더라도, 현상학적 입장에서 사태를 파악하는 방식과 아주 유사하다. 특히 상호-구성적(co-constitutive)이라는 말은 현상학적 접근을 하는 사람들이 하나의 대상을 인식하는 과정에서 대상과 인식하는 주체 사이에는 어느 한쪽의 결정적인 중요성이나 근본성이 있다는 것이 아니라 양쪽의 존재 모두가 함께 만들어내는 과정이라는 점을 강조하기 위해서 사용하는 용어이다. 기술과 그것을 사용하는 사람들 사이의 관계를 파악하는 데에도 중요한 지점이다. 특히 앙상블을 통해서나 생성으로 나타나게 되는 존재의 특성을 구성적(constitutive)이라고 파악할 때, 이 말은 구조적 결정성을 강조하는 구성(construction)이라는 개념과는 다르다. 이 두 지점의 차이를 놓치면 매클루언의 논의를 기술결정론적인 것으로 파악할 여지가 있다.↩︎"
  },
  {
    "objectID": "info.html#서론",
    "href": "info.html#서론",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "2.1 서론",
    "text": "2.1 서론\n1990년대 후반 인터넷과 컴퓨터 혁명의 진전으로 우리 한국사회에서 정보사회(information society)라는 개념이 도입된지도 적지 않은 시간이 지났다. 새로운 기술이 도입될 때마다 그 기술을 어떻게 이해해야 할지? 그리고 그러한 이해를 바탕으로 해당 기술을 어떻게 활용해야 할지에 관한 철학적, 윤리적 논쟁은 늘 반복되어 왔다. 마찬가지로 인터넷 윤리, 컴퓨터 윤리, 사이버 윤리, 데이터 윤리 같은 용어들 옆에 놓고 보면, 정보철학이나 정보윤리라는 말은 크게 새롭게 와닿지 않을 수 있다. 일반인들이나 전문가들도 이런 용어들을 쭉 펼쳐 놓고 보면 과연 이 용어들의 의미를 어떻게 구별해야 할지? 즉 정보윤리란 무엇인가?에 대해 명확한 답을 얻기 어렵다. 비슷비슷해 보이고, 별 차이 없는 이야기를 하자는 것 아닌가 하는 결론에 도달하기 쉽기 때문이다. 정보든 데이터든, 정보통신기술이든 최근에 새롭게 부상한 지능정보기술이든 윤리적으로 바람직하게 활용하자는 주장으로 들리기 때문이다.\n이런 현실의 바탕에는 정보를 협소하게 이해하는 현대 지식이 배경에 놓여 있다. 예를 들어 정보를 잘 활용하는 것의 중요성과 가치를 설파하는 기존의 광범위한 정보윤리 논의들은 대부분 정보를 정보통신기술(ICT) 발달을 통해서 정보에 대한 의존이 절대적으로 커진 현대사회의 윤리학의 한 응용(applied) 영역으로 간주하는 경향이 있다. 물리적 실체가 없이 정보통신기술을 기반으로 정보의 생산, 유통, 소비 과정에서 발생하는 권리와 책임, 사회적 해악, 긴장 갈등 같은 실천적인 문제들을 윤리적으로 풀어야 한다는 도덕적 사고 체계가 이러한 정보윤리론의 골자이자 거의 전부였다. 이러한 자기충족적인 정보윤리론의 철학적 근저에는 정보적 주체(정보를 활용하는 인간)와 정보적 대상(혹은 존재) 사이의 이원론이 자리잡고 있다. 이러한 이원론적 사고 안에서는 정보든, 미디어든, 데이터든 그 대상(target)이 바뀌는 것이 별다른 의미가 없다. 어차피 그것들을 사용하는 인간 주체의 자기 이해가 달라지지 않고, 따라서 윤리적 주체로서의 사람과 기술적인 대상들 사이의 관계도 달라지지 않으며, 결과적으로 인간에게 정보가 갖는 의미 또한 변화가 없게 된다. 그 대상의 이름이 정보든, 데이터든, 알고리즘이든 인공지능이든, 또는 초거대 인공지능이든, 그저 ’잘 쓰면 그만인 것을’이라는 말이다.\n하지만 최근 인공지능 기술의 눈부신 발전 상에서 목격되고 있듯, 기술의 본질적인 특성이나 형식이 달라질 경우에도 그럴 수 있을까를 질문해 본다면, 답은 간단치 않다. 기술의 특성 변화로 인하여 인간과 기술이 관계맺는 양상이 달라진다면, 기술과 인간의 상호작용 과정과 그 결과가 갖는 의미도 달라지기 때문이다. 예컨대, 기존의 미디어는 인간의 직간접적 통제에 의해 주입된 정보(input data/information)를 송신자의 의도나 목적에 따라 정해진 흐름에 전달(transmit)하고, 이용자는 이를 수신받아 활용하는 도구라는 범주적 특성 하에 인간과 관계를 맺었다. 그러나 근래 등장한 챗GPT 같은 생성형 인공지능 기술은 물론이거니와 심지어 단순한 알고리즘조차 인간으로부터 위임을 받아 스스로 행위함으로써, 과거의 기술과는 전혀 다른 존재의 가능성을 보여주고 있다. 이런 변화의 양상을 고려하면, 정보를 이해하는 방식과 그러한 이해를 정보 현상에 적용하는 과정은 과거와 같은 방식으로 접근해서는 곤란한 측면이 있다. 인공지능 기반 미디어 서비스가 보급되면서 인공지능을 법적 또는 윤리적인 행위 주체로 볼 것이냐 아니냐 같은 논쟁이 제기되는 이유도 바로 이런 맥락 때문이다.\n인공지능 시대를 맞이하여 이런 맥락은 더욱 더 전면화되고 보편화될 것이란 전망이 있다. 정보윤리도 정보를 활용하는 인간윤리 혹은 정보 생산, 유통, 소비 과정에서 발생하는 도덕적 해악이나 피해의 문제를 다루는 인간윤리 또는 사회윤리라는 기존의 관점으로 설명하기 어려운 문제들이 속속들이 등장하고 있기 때문이다. 그래서 정보 현상 혹은 정보 개념을 이해하는 방식을 근본적인 수준에서, 즉 철학적으로 재구성하고, 그러한 철학적 원리 하에서 정보와 관련한 복잡다양한 현실적 문제들에 대처해 가야 한다는 문제의식들이 최근 서구 학계로부터 제기된 바 있다. 정보철학(information philosophy: PI) 분야가 대표적이다. 여기서 말하는 정보철학이란 정보를 세계를 이해하는 근본 개념으로 상정하고, 정보에 관한 철학적 원리와 이론을 개발하려는 철학적 논의 분야를 지칭한다. 이 용어를 공식적으로 창안하여 널리 보급한 학자는 이탈리아 출신으로 옥스퍼드대학에서 인터넷, 정보 윤리에 관한 학술적 작업을 주도하고 있는 정보철학자 루치아노 플로리디(Luciano Floridi)다. 플로리디에 따르면 정보철학은 사물의 제1원인과 제1원리를 취급하는 철학(Philosophia Prima)이다.1 즉 정보 또는 정보를 통해 구성되는 세계 자체를 이해하는 것이 목표인 셈이다. 이러한 정보철학은 근본적으로 정보가 어떤 역할을 수행하며, 점점 더 확장되고 있는 정보의 중요성을 어떻게 이해할 수 있을지에 관한 원리와 토대를 찾는데 도움이 될 수 있다. 쉽게 말하면, 과거의 ’계급이론’이 계급이라는 렌즈로 세계와 역사를 이해하고자 했던 것처럼, 정보라는 차원을 통해서 사회와 세계를 이해해 보려는 철학 분야로 이해할 수 있다.\n이 장에서는 루치아노 플로리디의 정보철학을 토대로 인공지능과 알고리즘 같은 최근의 기술적인 대상들과 인간이 관계를 맺어가는 현대사회에서 정보를 둘러싼 문제를 어떻게 이해해야 할지에 관한 논의를 제공하고, 나아가 인공지능 시대에 정보철학적 접근에 주목해야 할 이유에 대해서도 살펴보고자 한다. 이러한 논의를 통해 이 장은 인공지능 같은 비인간 존재들과 인간이 대등한 관계를 복잡다양하게 맺어가는 오늘날 기존의 미시윤리학적이고 인간윤리학적인 접근이 가진 한계를 비판적으로 진단하면서, 정보에 대한 조금 더 보편적인 사고가 필요하다는 문제의식을 공유하고자 한다."
  },
  {
    "objectID": "info.html#footnotes",
    "href": "info.html#footnotes",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "",
    "text": "정보철학의 기원은 거슬러 올라가면 1940년대 후반 노르베르트 위너가 개념적 틀을 정초한 사이버네틱스(cybernetics) 이론에서 찾아볼 수 있다. 사이버네틱스 이론은 신호와 신호가 어떻게 관련을 맺는지를 설명하는 수학적 이론으로 제안되었지만, 1990년대 중후반 정보통신기술의 발달로 인해 등장한 정보화 물결과 맞물려 정보이론의 발전에 중요한 영향력을 행사했다.↩︎"
  },
  {
    "objectID": "post.html#우리-모두는-포스트휴먼",
    "href": "post.html#우리-모두는-포스트휴먼",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.1 우리 모두는 포스트휴먼",
    "text": "3.1 우리 모두는 포스트휴먼\n새로운 인공지능 기술이 등장할 때마다 어김없이 나타나는 반응은 놀라움과 두려움이다. 2017년 5월 인공지능프로그램 알파고가 이세돌 9단과 대국을 벌였을 때도, 2022년 11월 ChatGPT라는 생성형 인공지능기술이 등장했을 때도 인공지능을 둘러싼 반응은 크게 이 두 가지였다. 놀라움이라는 반응은 인공지능기술을 활용하여 수익을 내고자 하는 산업과 경제정책을 중심으로 형성되는데, 이런 반응은 온갖 미디어를 통해 확장되면서 인공지능을 급속히 받아들이는 기제로 작동한다. 이에 비해 두려움이라는 반응은 인공지능이 인간의 일자리를 빼앗거나 인간과 경쟁하는 맥락에서 주로 나타나며, 극단적으로는 인간의 생존을 위협하거나 인류를 파멸시키는 기술로까지 묘사된다.\n우리는 미디어담론을 지배하는 기술숭배와, SF영화들에서 지속적으로 재생되어온 기술공포라는 양극단의 어디쯤에서 인공지능기술을 신기해하면서도 두려워하는 양가적인 마음을 가지고 인공지능기술이 만들어내는 환경에서 살고 있다.\n스마트폰은 아침에 우리의 잠을 깨우고 날씨가 어떤지, 오늘 일정이 무엇인지 알려준다. 밤새 친구들이 전해온 메시지와 친구들의 일상을 들여다보게 해주고, 세상에 어떤 일들이 벌어졌는지를 전해준다. 버스와 지하철이 언제 오는지, 도로상황은 어떤지 알려주고, 택시를 불러주고 퀵보드와 자전거의 위치를 보여주기도 한다. 음악과 영화와 드라마를 골라주고, 관심과 취향에 맞는 콘텐츠를 찾아주고, 다양한 사람들을 연결해주며, 음식을 배달하고, 음식점을 추천하며, 쇼핑을 돕고 여행계획을 짜주기도 한다. 잠이 들면 혈압과 맥박, 수면상태를 살펴보고 건강상태를 체크한다. 코로나19 같은 팬데믹 상황에서는 QR코드로 동선을 보고하고, 확진자 동선알림 앱으로 안전을 확인하며, 공공앱으로 백신접종을 예약하고, 전자백신접종증명서로 출입을 허락받는 등 미디어와 함께 재난을 관리하기도 한다. 스마트폰이 없었다면 팬데믹 상황에서 우리는 생존할 수 없었을 것이다. 스마트폰이 없는 삶은 이제 상상할 수도, 가능하지도 않다. 2021년 10월 25일 발생한 KT 인터넷 서비스 중단사고는 지능화미디어가 인간과 공생하는 사회임을 단적으로 보여주었다. 인터넷 접속이 중단되자 음식주문과 배달앱의 불통으로 식사라는 기본 생존이 위협받았고, 백신접종과 병원진료가 불가능했으며, 비대면 수업 등 교육에 차질을 빚었고, 지도앱이 작동하지 않아 사람들을 만나러가는 데 어려움을 겪었다.\n이처럼 우리의 일상과 문화적 취향, 건강상태 등을 관리하는 지능화미디어는 우리와 ‘함께 산다’. 지능화미디어는 단순히 일상의 편리를 도와주는 도구가 아니라 우리의 삶 속에 스며들어 우리와 분리될 수 없는 존재가 된 것이다. 우리의 삶은 인간과 미디어 어느 한편의 일방적 지시나 통제로 이루어지는 것이 아니라 인간과 미디어가 함께 수행하는 행위들로 이루어진다. 빅데이터와 AI 알고리즘, 네트워크 등 미디어테크놀로지는 이러한 행위들을 통해 인간의 육체와 감각을 미디어와 결합시킨다. 우리가 미디어와 결합되는 접점 즉 미디어 인터페이스가 영화스크린에서 텔레비전 수상기로, 컴퓨터 단말기로, 노트북으로, 태블릿으로, 스마트폰으로, 손목시계로, 안경으로, 렌즈로, 나노 칩1으로 점점 거리를 좁히며 인간 몸으로 스며들수록 인간과 미디어의 결합은 더욱더 공고해진다. 지능화미디어는 인간의 육체와 감각과 경험이 더 이상 미디어와 분리될 수 없도록 만든다. 인간은 미디어와 공생체로서 세계를 경험하고 이해하며, 우리의 몸과 감각, 그리고 경험은 끊임없이 변주된다. 이런 점에서 우리는 인간과 미디어테크놀로지가 결합된 포스트휴먼이다.\n우리 모두가 포스트휴먼이라는 선언은, 인공지능이 인간과 경쟁하거나 인간을 위협하는 기술적 대상이 아니라 인간과 분리될 수 없는, 인간과 함께 하는 공생체임을 드러내는 것이다. 이런 선언은 인간과 비인간 사이의 경계를 가로질러 새로운 존재론과 윤리가 필요함을 역설한다. 경계짓기에 기인한 차별과 배제를 해결할 윤리적 실천의 가능성을 발견하고, 인공지능기술을 바라보는 시각, 즉 인간만이 우월한 존재이자 만물의 척도라는 인간중심주의 시각을 바꾸어야 함을 요청한다. 포스트휴머니즘은 바로 이러한 사유의 전환을 담고 있다. 포스트휴먼 시대의 윤리는 합리성과 이성에 근거한 근대적 사유에서 우선시한 존재들의 순위를 바꾸자는 것이며, 기존 질서와 다른 사회를 구성하자는 전복의 윤리이기도 하다."
  },
  {
    "objectID": "post.html#포스트휴먼과-트랜스휴머니즘",
    "href": "post.html#포스트휴먼과-트랜스휴머니즘",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.2 포스트휴먼과 트랜스휴머니즘",
    "text": "3.2 포스트휴먼과 트랜스휴머니즘\n\n3.2.1 포스트휴먼: 사이보그와 안드로이드\n포스트휴먼은 정보기술과 나노기술, 생명공학과 인지공학 등 테크놀로지 발달로 유기체인 인간과 인공적인 기계가 결합한 혼성적 존재이다. 포스트휴먼은 인간과 기계가 수렴하여 결합하는 과정에서 출발점이 무엇인가에 따라 사이보그와 안드로이드로 나뉠 수 있다. 인간과 기계가 결합한 혼성적 존재는 사이보그로 통칭되지만, 보다 명확하게 말하자면 사이보그는 유기체인 인간이 기술과 결합하여 점점 기계화되는 것을 의미하고, 안드로이드는 기계에 인간적 요소를 삽입하여 점점 인간화되는 것을 의미한다.\n먼저 사이보그는 살(flesh)과 전자회로의 물리적 병합이라는 이미지를 떠올리게 만드는 우리시대의 막강한 문화적 아이콘이다(Clark, 2003, p. 5). 사이보그 담론은 만화, 잡지, 텔레비전, 영화, 비디오 게임, 컴퓨터 게임 등 다양한 장르의 소재이고, 과학분야 뿐만 아니라 심리학, 정신분석학, 사회학, 철학, 문학, 문화학 등 다양한 분야에서 주요 관심사이다.\n사이보그는 두 가지 유형으로 대별할 수 있다(Tomas, 1991, p. 32; 김선희, 2005). 먼저 하드웨어 인터페이스 사이보그(hardware-interfaced cyborg)는 탈유기적인 형태로 단순히 인간의 신체에 부분적으로 기계적/인공적 부품을 대체하거나 기계적 장치를 이식한 고전적 형태의 사이보그이다. 인간의 신체일부를 인공안구나, 인공심장, 인공관절 등 인공장기로 교체하거나 인공보철물을 이식함으로써 인간능력의 한계를 극복한 사이보그가 이에 해당된다. 두 번째 유형은 소프트웨어 인터페이스 사이보그(software-interfaced cyborg)로 데이터에 기반하여 신체의 경계가 불분명한 인간구성물이다. 현실세계와 가상세계를 연결하는 네트워크화된 사이보그로서 어디에나 존재하고(편재성)과 눈에 잘 보이지 않는다(비가시성). 스마트폰을 사용하는 우리는 모두 네트워크화된 사이보그라고 할 수 있다. 클락(Clark, 2003)은, 인간은 도구나 컴퓨터 같은 테크놀로지를 통해 생각하고 커뮤니케이션하는 정신적 능력을 확장시킴으로써 궁극적으로 정신을 해방시키고 있으며 그런 점에서 우리는 ’인간-테크놀로지 공생체(symbionts)’이고 타고난 사이보그 (natural-born cyborg)이다.\n다음으로 안드로이드는 인공뇌, 인간의 감정 등을 담은 인조인간을 의미한다. 안드로이드는 반드시 인간의 형상을 한 인공물에 국한되지 않는다. 알파고를 개발한 딥마인드를 자회사로 둔 구글이 2007년 모바일 시장에 진출하면서 공개한 개방형 운영체계 이름을 안드로이드로 정한 것은, 궁극적으로 인간의 지능을 가진 디지털 존재를 개발하려는 전략을 상징적으로 보여준다. 실제 구글은 인공지능 기술을 주도하는 글로벌 기업으로서 위상을 구축하고 있고, 알파고의 바둑대국도 이러한 전략의 일환으로 만들어진 이벤트이다. 알파고나 ChatGPT 같은 생성형 인공지능, 인공지능 스피커, 인공지능 의사 왓슨, 사우디아라비아 시민권을 가진 인공지능 로봇 소피아, 인간과 교감하는 소셜로봇, 호텔이나 공공기관, 음식점에서 일하는 서비스 로봇 등은 인간의 지능과 감정을 탑재하여 인간과 가까워지려는 안드로이드의 형상들이다. 인간과 기계, 둘 중 어디에서 출발하는가와 상관없이 사이보그와 안드로이드 모두 유기체적인 것과 인공적인 것이 결합한 포스트휴먼이다. 그럼에도 불구하고 인간과 기계의 결합을 어떠한 관점에서, 어떠한 지향점을 가지고 바라보는가에 따라 포스트휴먼에 관한 논의는 트랜스휴머니즘과 포스트휴머니즘으로 크게 나누어볼 수 있다.\n\n\n3.2.2 트랜스휴머니즘\n트랜스휴머니즘(transhumanism)은 ’합리적 방법을 사용하여 인간은 육체적, 정신적, 사회적으로 보다 높은 단계로 발전할 수 있고 발전해야 한다는 철학’을 의미한다(Sandberg, 2000). 트랜스휴머니즘은 인간의 육체를 기술적 보철물로 대체하는 탈육화(disembodiment)를 지향한다. 탈육화는 고깃덩어리에 불과한 거추장스러운 육체를 벗어던지고 한계를 지닌 인간의 몸을 기계장치를 통해 교체하고 확장하고자 하는 육체이탈의 욕망을 드러낸 것이다. 트랜스휴머니즘에서 나타나는 육체이탈 담론은 인간의 능력을 확장하거나 강화함으로써 영생을 꿈꾸는 트랜스휴먼을 실현하고자 한다. 특히 인간의 두뇌에 AI를 직접 탑재하거나 컴퓨터 칩을 이식하고, 인간의 신경계에 컴퓨터 전자장치를 연결하여 신체 외부의 장치를 작동시키는 신경보철은 몸과 마음의 기능을 개선하는 장치이다. 왼쪽 팔 피부에 컴퓨터 칩을 이식하여 자신의 위치신호를 컴퓨터로 전송한 케빈 워릭(Kevin Warwick)2이나, 팔에 귀를 이식하거나 인공보철물로 제3의 팔을 만드는 등 자신의 몸을 기계인간으로 개조한 스텔락(Sterlac)은 기계가 말초신경의 심연에까지 침투할 수 있음을 보여준다(마정미, 2008, 205~206쪽). 워릭이나 스텔락은 몸에 컴퓨터 칩을 내장하였지만 신경보철이 궁극적으로 지향하는 것은 뇌-컴퓨터 인터페이스(brain-computer interface)이다. 미국 국방부 산하 연구기관인 DARPA(Defense Advanced Research Projects Agency)는 뇌를 컴퓨터와 직접 연결해 뇌 신호를 포착함으로써 인간의 생각만으로 기계를 움직이게 하는 인터페이스를 개발하고 있다. 뇌가 컴퓨터를 신체의 일부로 인식하고 불완전한 정보를 보완하기 위해 새로운 능력을 개발하여 인공장치에 스스로를 적응시킴으로써 인간과 컴퓨터가 결합한 사이보그가 될 수 있다. 물론 이러한 실험의 근저에는 인간 뇌의 가소성과 신경망의 유연성에 대한 신뢰가 깔려있다.\n사고능력과 커뮤니케이션 능력을 향상시키기 위해 인터넷, 모바일미디어와 웨어러블(wearable) 컴퓨터 등 컴퓨터의 도움을 받는 것에서부터 소프트웨어 에이전트를 사용하는 것, 나아가 정신을 재구조화하여 인간-AI로 탄생하는 것은 트랜스휴머니즘으로 가는 단계들이다. 이러한 과정을 거쳐 인간은 전례가 없는 육체적, 지적, 심리적 능력을 가진 불멸의 인간인 포스트휴먼이 되는 것이다.\n트랜스휴머니즘이 지향하는 탈육화는 심신이원론에 기반한다. 근대서구사상의 근간을 이루는 데카르트의 심신이원론은 정신과 물질(육체)을 분리한다. 심신이원론에 따르면 연장(extension) 즉 공간을 점유하는 특성을 본질적 속성으로 하는 물질과, 사유를 그 본질적 속성으로 하는 실체로서의 정신이 독립적으로 존재한다(심상규, 2008, 39~54쪽). 데카르트가 확립하고자 의도했던 철학적 결론은, 정신이 육체의 우위에 있다는 것이며, 정신 혹은 사유작용을 통한 인간의 존엄성을 확보하고 동물 같은 다른 생명체보다 인간이 더 우월한 지위에 있다는 것을 드러내는 것이다. 육체이탈을 사이보그의 열망으로 보는 트랜스휴머니즘은 육체를 정신의 산물인 육체 이미지로 대체함으로써 탈육화된 정신이나 육체의 부정이라는 데카르트적 이원성을 강화시킨다3(Penny, 1994, p. 243).\n이러한 트랜스휴머니즘은 생물학적이고 지능적인 인간의 한계, 물리적 환경의 한계, 그리고 개인과 집단의 발전을 제약하는 문화적, 역사적 한계를 초월하고자 한다. 기술을 통해 인간육체의 한계를 벗어나 인간의 능력을 향상시킬 수 있다는 트랜스휴머니즘은 기술숭배에 기반한 기술적 낙관론이자 기술 진화론에 기반한다. 기후위기 같은 인류세 문제를 인공지능 기술을 통해 해결할 수 있다고 주장하는 에코모더니즘 역시 트랜스휴머니즘의 기술숭배적인 주장과 맥을 같이 하며, 인공지능을 둘러싼 주류담론들이라 할 수 있다."
  },
  {
    "objectID": "post.html#포스트휴머니즘의-세-가지-사유-탈이원론-탈인간주의-탈인류중심주의",
    "href": "post.html#포스트휴머니즘의-세-가지-사유-탈이원론-탈인간주의-탈인류중심주의",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.3 포스트휴머니즘의 세 가지 사유: 탈이원론, 탈인간주의, 탈인류중심주의",
    "text": "3.3 포스트휴머니즘의 세 가지 사유: 탈이원론, 탈인간주의, 탈인류중심주의\n포스트휴머니즘은 서구유럽의 이상적 인간상인 단일하고 통일된 정체성을 지닌 비트루비우스적 인간을 제외하고 그 어떤 존재도 허용하지 않는 일자성을 거부한다. 그 대신 성과 인종, 자연의 차이를 인정하되 이러한 차이가 위계화되거나 차별화 기제로 작용하지 않는 비일자성 즉 복수성을 받아들인다. 이러한 포스트휴먼의 대표적 도상이 도나 해러웨이의 사이보그이다. 도나 해러웨이(Haraway, 1991)의 사이보그 선언은 1990년대 과학기술에 대한 숭배가 만연하던 신냉전하에서 사회주의자들이 주장하는 기술혐오를 반대하며 등장한다. 해러웨이는 지배자의 도구로 사용되던 과학 기술을 전유하여 여성 해방에 적극적으로 사용하자고 주장한다. 이러한 맥락에서 해러웨이가 사이보그 선언에서 설명하는 도상(그림 1) 속 여성은 기술을 두려워하지 않는 여성이다. 코요테 가죽을 뒤집어쓰고, 가슴에 반도체 칩을 부착한 채 컴퓨터 앞에 앉아 있는 유색인종 여성은 기술에 충실하면서도 기술을 배반하는 ’모순의 전략’을 통해 남성중심적 기존 질서를 전복하는 사이보그를 상징한다. 동시에 인간과 기계, 인간과 동물, 인공물과 자연의 합성물로서 이러한 이분법적 경계를 가로질러 생산적 결합을 만들어내는 유용한 연결장치로서 상상력의 원천으로 강조된다.\n다빈치의 비트루비우스적 인간\n해러웨이의 사이보그\n유기체적인 것과 인공적인 것의 경계가 사라진 포스트휴먼은 유목적 주체(Braidotti), 인간-기술 공생체(Clark), 인간-기계 앙상블(Simondon), 인간-행위자 네트워크(Latour)로 논자에 따라 상이하게 명명되지만, 인공지능을 대상화하거나 경쟁상대로 보는 인간중심주의의 실체적 관점에서 벗어나, 인간과 기계가 동등한 위치에서 공진화하는 관계적 실재임을 드러낸다.\n포스트휴먼은 탈인간중심주의, 탈인류중심주의, 탈이원론이라는 포스트휴머니즘의 세 가지 사유를 실천하는 존재이다(Ferrando, 2019/2021).\n우선, 탈인간중심주의는 오랫동안 서구근대의 사고체계를 지배해온 서구근대 인간중심주의를 비판한다. 인간중심주의는 인간이 자기의식과 자율성, 창조성을 지닌다는 이유로 서구 근대 계몽된 인간이 세상을 지배하는 주체가 될 수 있으며, 다른 존재들보다 우위에 있다고 주장한다. 인간중심주의는 레오나르도 다빈치의 비트루비우스적 인간을 서구근대인의 완벽한 표상으로 설정하는데, 비트루비우스적 인간은 서구/백인/남성/이성애자/비장애인/정착민이다. 이 외의 인간 존재 즉 비서구/유색인종/여성/성소수자/장애인/난민은 ’인간’이 아니며, 따라서 차별과 배제, 혐오의 대상이 된다.\n탈인류중심주의는 인간종족주의 및 인간예외주의를 통해 종간 위계를 만들어내는 인류중심주의를 비판한다. 팬데믹은 인간이 다른 생명과 공존하며 상호의존적이고 연결된 삶을 살고 있음을 적나라하게 보여주었다. 남아프리카 공화국 사파리 도로 위에서 낮잠 자는 사자의 무리는 인간의 것이라고 당연시되던 자리가 실은 인간에게 빼앗긴 다른 생명의 자리였음을 보여준다. 사스, 메르스, 코로나19 같은 바이러스의 창궐은 개척이라는 명목으로 자행된 산림파괴와 동물서식지의 변화, 이로 인한 생물다양성의 불균형이 초래한 파국을 절감하게 한다. 여기에는 오랜 기간 당연시해왔던 인간예외주의가 자리한다. 인류중심주의는 동물이나 식물, 바이러스와 균류 등 비인간 존재자들을 자본주의적 이윤을 추구하기 위한 착취와 개발의 대상으로만 여겨 함께 일궈온 생태계를 파괴하게 만든다. 이런 점에서 기후위기와 재난을 야기한 인류세는 무어가 말하는 자본세 또는 해러웨이가 말하는 츌루세이기도 하다. 인공지능이 형성한 디지털 생태계 역시 인류세라는 곤경을 만들어내는 인류중심주의의 산물이라고 할 수 있다.\n이와 같은 인간중심주의와 인류중심주의는 차별화의 기제로 작동하는 이분법적 사고에 기반을 둔다. 인간과 자연, 인간과 동물, 인간과 기계, 남성과 여성, 문화와 자연 등 이분법의 쌍들은 경계 안을 우선시하고 경계 밖을 배제하면서 전자와 후자 사이에 주인과 노예라는 사회적 위계질서를 구축한다. 포스트휴머니즘은 이러한 이분법적 사고에 근거한 대립과 차별의 인식체계와 조직원리 등에 저항하며, 인간 아닌 존재들과의 공생과 공존, 공진화를 지향한다.\n이와 같이 포스트휴머니즘은 탈인간중심주의, 탈인류중심주의, 탈이원론을 지향한다. 포스트휴먼은 유목적 주체라는 새로운 주체로서 기술을 통한 인류진보라는 이면에 감추어진 불평등과 차별, 소외받고 주변화된 존재들, 인간이외에 동물이나 지구, 기계 등 비인간적 존재들을 논의의 중심으로 끌어들이고 모든 종을 가로질러 연대할 가능성을 열어놓는다. 유목적 주체는 유전공학과 인공지능 살상무기 등 포스트휴먼의 곤경을 바꿀 수 있는 포스트휴먼 주체로서 ’되기’라는 윤리적이고 정치적인 실천한다. 브라이도티가 제안하듯이 포스트휴먼은 복수의 주체성을 실험하는 하나의 방식으로 기계-되기, 동물-되기, 소수자-되기를 제안한다.\n포스트휴먼의 복수종 되기"
  },
  {
    "objectID": "post.html#포스트휴머니즘의-윤리-공-산의-윤리post-4",
    "href": "post.html#포스트휴머니즘의-윤리-공-산의-윤리post-4",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.4 포스트휴머니즘의 윤리: 공-산의 윤리4",
    "text": "3.4 포스트휴머니즘의 윤리: 공-산의 윤리4\n현재 인류가 처한 곤경은 인류의 오랜 삶의 방식인 공존과 공생의 삶을 위태롭게 하고 있다. 이것에 대해 근본적인 성찰이 이루어지지 않는다면, 포스트휴먼의 사회는 배제와 차별, 불평등의 사회로 전락할 위험에 처할 것이다. 여기서는 포스트휴머니즘의 사유를 바탕으로 포스트휴먼이 만들어갈 사회성의 윤리 및 사회문화적 실천들을 탐색하고자 한다. 먼저 포스트휴먼이 상호의존하는 다른 종들과 공생하는 생태계를 만들기 위한 실천으로, 유목적 주체로서 연결짓기와 친척만들기를 제안한다. 이것은 경계짓기와 위계구조를 근간으로 하는 배타적・차별적 생태계를 구조적으로 해체하는 실천이다. 응답하기와 책임지기는 공존과 공생의 생태계를 유지하는 데 필요한 윤리로서 사회적 유대를 확장하기 위한 태도적 차원의 실천이다. 마지막으로 자동화에 저항하기는 데이터-네트워크-인공지능의 기술생태계를 전유하려는 자본과 권력에 대항해 정치사회적 연대를 형성하는 실천이다. 이들 각각은 포스트휴먼이 새로운 생태계를 형성하고 유지하고 지켜내기 위해 유기적으로 결합시켜야 하는 환경적, 태도적, 행위적 차원의 실천들이다.\n\n3.4.1 유목적 주체로서 연결 짓기\n공존과 공생의 윤리를 실천하는 포스트휴먼 주체로서 유목적 주체는, 기술을 통한 인류진보라는 이면에 감추어진 불평등과 차별로 소외받고 주변화된 존재들, 인간이외에 동물이나 지구, 기계 등 비인간종들과 연대할 수 있는 가능성을 열어놓는다(Braidotti, 2013/2015). 유목적 주체의 이러한 횡단적 주체성은 복잡한 상호관계의 생기적 네트워크에서 타자들과 유대할 수 있는 가능성을 제공한다. 이런 점에서 포스트휴먼은 노마디즘을 삶의 양식으로 삼는다. 노마디즘은 정주성이 가져온 구별과 분리, 차별에 저항하면서 끊임없이 탈주하려는 삶에 대한 사유 혹은 윤리를 내포한다.5 노마디즘은 서구근대가 만든 고정된 관념과 거대담론, 위계화와 정체성의 정치를 벗어나고자 한다는 점에서 포스트휴머니즘과 맥이 닿아있다.\n노마디즘의 실천행위는 낯선 것들과의 끊임없는 연결짓기이다. 연결짓기(connecting)는 클릭 하나로 맺어지는 단순한 ‘연결’(networking)이 아니라 접촉(contact)을 내포하는 보다 호혜적인 관계를 의미한다. 그것은 불안하고 유동적인 상태에서 무한히 변주될 수 있는, 위계화되지 않은 관계들을 만들어내는 실천이다. 연결짓기는 들뢰즈의 리좀(Rhizome)으로 형상화할 수 있는데, 리좀은 하나의 점, 하나의 질서를 고정시키는 나무나 뿌리와는 전혀 다른, 어떤 지점에서든 다른 지점과 연결 접속될 수 있는 중심없는 뿌리이다(Deleuze & Guattari, 1980/2001, 19~20쪽). 리좀은 시작도 끝도 없이 언제나 사이(milieu)에 있으며, 이 사이에서 “변이, 팽창, 정복, 포획, 꺾꽂이를 통해 나아간다”(47쪽). 리좀의 원리처럼 연결짓기는 모든 종들을 가로질러 다양한 이질적인 것들과 접속하며 끊임없이 탈주하는 지도그리기와 같다.\n연결짓기는 또한 이곳과 저곳, 안과 밖을 구분하고 단절시키는 경계(boundary)를 무너뜨리고, 낯선 사람들의 사회적 접촉이 활발하게 일어나는 접경(border)을 만들어내는 것을 의미한다(Sennett, 2012/2013, 138~140). 연결짓기는 같은 처지나 생각을 가진 동질적인 사람들이 아닌 이질적이고 낯선 사람들과 함께 하는 접경지대에서 이들과 공통감각을 키우는 실천이다. 접경지대에서는 고정된 정체성으로부터 벗어나 새롭게 마주한 사람들과 정보를 공유하거나 감정적 유대를 나누는 사회성을 경험할 수 있다.\n포스트휴먼의 연결짓기는 공생과 환대의 윤리를 사회성의 원리로 삼는다. 노마드는 자신이 신뢰하는 무리들과 함께 이동하며, 자신들이 기대어 사는 자연을 파괴하지 않고, 손님을 공손하게 맞이하고 융숭히 대접하며, 다른 가족에게 자신이 갖고 있던 것을 나눠주고 함께 음식을 만들어 먹는 나눔의 의무를 실천한다(Attali, 2003/2005, 75~97쪽). 이러한 노마드적 삶은 자연과 타자와 공동체에 대한 신뢰와 환대, 그리고 공생의 윤리가 어떻게 삶 속에 구현될 수 있는지를 보여준다.\n이와 같이 포스트휴먼은 무한한 접속을 통해 새로운 관계 맺음의 가능성을 항상 열어놓고 경험을 공유하고 소통하며 사회적 유대를 쌓아가는 연결짓기를 실천할 필요가 있다. 그러나 이러한 연결짓기는 누가, 어떤 이유로, 어떻게 연결하느냐에 따라 사회성 형성에 차이가 있을 수 있다. 이미 목격하고 있듯이 AI 알고리즘은 비슷한 성향을 가진 사람들만을 연결하여 필터버블에 가두고, 자신의 세계 밖에 있는 사람들을 혐오하게 만들어 사회적 연대와 유대형성을 방해한다. 플랫폼 기업은 이윤추구 목적으로 팬데믹 사회의 생존 인프라가 된 플랫폼 네트워크를 전유한다. 이처럼 포스트휴먼 공생체의 연결짓기는 언제든 권력과 자본에 포획될 위험에 처해 있는 것이다. 따라서 차별과 배제, 불평등을 거부하는 유목적 주체의 연결짓기에는 해러웨이가 말한 공-산의 사유가 깃들여야 한다. 가령 백신제조사의 지적재산권 면제를 주장하며 추가접종 대신 저소득국가의 백신접종률을 높이자고 전 세계 음악가들과 지도자들이 동참한 글로벌 시티즌 라이브(Global Citizen Live)6 같은 ‘백신 평등’ 운동이나, 팬데믹으로 공연을 할 수 없는 첼리스트들이 각자의 장소에서 연주한 동영상을 유튜브로 중계하는 코비드 첼로 프로젝트(Covid Cello Project)7, 자신이 살고 있는 집의 창문 밖 풍경을 영상으로 업로드하여 코로나19로 여행할 수 없는 서로에게 창밖 풍경을 선사하는 윈도우스왑(WindowSwap) 프로젝트8 등은 국경을 넘어 서로를 염려하고 위로하며 누구나 접속할 수 있는 느슨한 연결짓기의 사례들이라 할 수 있다.\n\n\n3.4.2 친척 만들기\n근대 인간중심주의는 인종, 젠더, 섹슈얼리티, 계급, 국적 등 정체성이라는 경계짓기의 틀을 동원하여 다른 존재들과의 관계를 사유한다. 이런 사유에서는 중심과 주변, 우월한 것과 열등한 것, 경계 내와 경계 밖이라는 대립적이고 위계적인 관계가 당연시 된다. 그런데 이러한 서구-백인-남성-이성애자-기업가 시각에 근본적으로 대항하는 방식은 그 대척점에 있는 비서구-흑인-여성-레즈비언-노동자를 주체로 세우는 것으로 충분하지 않다. 이러한 방식은 인간 아닌 존재들과의 관계에서도 경계를 구분하고 획정하는 틀을 지속적으로 필요로 하기 때문이다. 따라서 포스트휴머니즘은 위계질서를 무너뜨리기 위해 정체성의 정치를 전복하고, 사회적 관계를 바라보는 시각 자체를 재구성할 것을 요구한다.\n그 방법으로 해러웨이는 ’자식이 아닌 친척(kin)을 만들자’고 제안한다. 혈통이나 계보에 묶인 자식이 아니라, 지구상에서 번성하고 있는, 낯설고, 불가사의하고, 끊임없이 출몰하는, 활동적인 무엇, 인간과 인간 아닌 것들을 포함한 복수종(multispecies)들을 친척으로 만들자는 것이다(Haraway, 2016/2021, 177~178쪽). 서로를 구속하고 길들이며 배타적 친밀함을 유지하는 폐쇄적 가족이 아니라, 상호의존의 공생 관계에 있는 모든 인간, 동물, 기계를 친척으로 만들어 그들과 사회적 유대를 나누는 것이다. 이것은 서로를 보살피고 지지하고 묶어주는 거대한 그물망을 만드는 것이며, 서로의 삶이 깊이 연결되어 있음을 확인하는 과정이기도 하다. 그러나 친척만들기는 이 그물망에서 서로 협력하며 평화를 누리는 것이라기보다 서로를 돌보고 길들이면서도 끊임없이 ’트러블을 일으키고 트러블과 함께 하는 것’이기도 하다.\n친척을 만드는 것은 특정한 연결짓기 즉 절합(articulation)를 통해 이루어질 수 있다. 가령 아마존 카야포족 남자의 캠코더 촬영은 비디오캠코더, 땅, 식물들, 동물들, 비디오를 보게 될 청중들, 그리고 기타의 유권자들로 구성된 인간/비인간의 집합체를 만들어낸다(최유미, 2020, 259쪽). 이 집합체는 아마존 숲의 생태를 염려하는 각자의 정치적 의사를 표명하며, 연대를 통해 서로를 친척으로 삼는다.\n코로나19 팬데믹에서 위기를 맞고 있는 돌봄9에서도 친척만들기 실천을 찾아볼 수 있다. 돌봄은 가부장적 질서에서 여성만의 노동으로 폄훼되고, 신자본주의 수익창출의 수단으로 여겨진다. 돌봄의 다면적이고 심각한 위기상황을 이해하고 해결하기 위해 여러 분야의 전문가가 모인 더케어컬렉티브 (The Care Collective)는 돌보는 관계를 확장하고, 충분한 돌봄자원을 확보하기 위해 ‘난잡한’(promiscuous) 돌봄 윤리를 제안한다(The Care Collective, 2020). 난잡한 돌봄은 “인간과 비인간을 막론하고 모든 생명체 사이에서 이루어지는 모든 형태의 돌봄이 필요와 지속가능성에 따라 공평하게 그 가치를 인정받고 사용되어야” 함을 의미한다(79~80쪽). 이 윤리는 신자유주의의 진정성 없는 돌봄을 거부하고 인간-비인간 모두가 차별받지 않고 돌봄의 관계를 확장해나갈 것을 요구한다. 이것은 돌보는 친척의 범위를 가능한 한 넓히는 것이며, 난잡한 돌봄을 통해 사회성의 역량을 키우는 것이다.\n바이런(Byron, 2021)의 연구는 난잡한 돌봄의 사례를 보여준다. 바이런은 LGBTQ+ 젊은이들이 디지털 미디어를 이용해 어떻게 서로를 돌보는지를 구체적으로 분석한다. 폭력과 자살 위험에 노출되고, 공적 보건의료와 사회적 지지에서 배제된 LGBTQ+ 젊은이들은 소셜미디어 플랫폼에서 익명으로 정보와 조언과 감정적 지지를 나누며 서로를 돌보는 일상적 실천을 수행한다. 이러한 실천은 소셜미디어 플랫폼에 모인 사람들을 친척으로 만들며 ’디지털 돌봄 문화’를 만들어나간다.\n이러한 사례들은 비디오캠코더와 소셜미디어 같은 미디어테크놀로지가 특별한 연결짓기를 통해 친척 만들기에 기여할 수 있음을 보여준다. 그러나 AI 알고리즘이 보여주듯이 미디어테크놀로지는 공통의 관심사로 구획지어진 폐쇄적이고 파편화된 배타적 영역을 만들어낼 수도 있다. 그렇게 되면 위계화된 정체성 정치의 폭력성을 비판하며 차이에 기반을 둔 새로운 사회적 관계를 만들어내고자 하는 포스트휴머니즘의 친척 만들기는 실패할 수밖에 없다. 따라서 상이한 종들을 가로질러 친척을 만드는 것은, ’자식’으로 상징되는 사회적 관계를 전복하고 사회성의 구조를 새로 짜는 부단한 노력을 필요로 한다.\n이것은 인공지능과의 관계로도 확장될 수 있다. 대화형 인공지능 이루다는 사회적 약자 및 소수자에 대한 혐오와 차별 발언으로 사회적 물의를 일으켰는데, 이는 우리사회의 혐오와 차별을 그대로 반영한 것이다. 하지만, 보다 중요한 것은 우리가 어떤 인공지능을 원하는가이다. 인공지능 이루다는 친밀함의 욕망을 실현할 대상으로 기획되었기 때문에 연인끼리 주고받은 카톡 대화를 데이터로 사용한다. 따라서 젊은-여성-AI인 이루다는 욕망의 대상일 뿐 처음부터 친구가 될 수 없다. 반면에, 목적지향 대화형 인공지능인 사라(SARA, the Socially Aware Robot Assistant)10는 이용자가 편안하고 친밀한 관계에서 업무를 수행할 수 있도록 도와주는 비서로 기획되었다. 사라는 통제된 상황에서 특수한 목적으로 이루어진 대화데이터, 사회적 상호작용의 기본 원칙 및 비언어상호작용이 적용된 인공데이터를 사용한다. 사라는 사회적-감정적 유대를 강화하는 사회성을 학습하기 때문에 이용자와 인공지능은 서로에게 반응하면서 보다 친밀하고 편안한 관계를 만들 수 있다. 이런 점에서 친구의 자리를 대체하려는 이루다는 결코 친구가 될 수 없지만 이용자와 협력하는 사라는 사회적 유대를 형성하는 친척이 될 수 있다.\n\n\n3.4.3 응답하기와 책임지기\n유목적 주체의 연결짓기가 수평적이고 횡단적이며 상호의존적인 관계가 아닌 소수의 이익만을 극대화하는 불평등 구조로 작동하지 않기 위해서는 응답하기와 책임지기라는 윤리에 기반해야 한다.\n어지럽고 불안하고 뒤죽박죽인 시대에 트러블을 만들고 트러블과 함께 살아가기를 주장하는 해러웨이는 이분법적 질서와 정체성에 기반을 둔 정치를 거부하면서 인간종이 아닌 사이보그나 반려종 같은 다양한 종들간의 공-산(sym-poiesis)을 제안한다(Haraway, 2016/2021; 최유미, 2020). 공-산은 인류가 한번도 벗어난 적이 없는 오랜 삶의 방식으로서, 상호이익을 위해 협력만 하는 것이 아니라 실패를 통해 변화를 만들어가며 새로운 관계를 생성해내는 함께-만들기를 의미한다. 해러웨이의 ’실뜨기’나 키머러의 ’향모 땋기’는 공-산의 삶을 사유하기 위한 하나의 은유이다.\n\n“실뜨기는 주고받기의 리듬이 유지되는 한 모든 종류의 수족으로 다수가 놀 수 있는 것이다. 학문과 정치도 실뜨기를 닮았다. 열정과 행동, 가만히 있기와 움직이기, 정박과 출항이 필요한 꼬임과 뒤얽힘 속에서 건네주기”(Haraway, 2016/2021, 23쪽).\n\n실뜨기가 공-산인 이유는, 서로 주고 받는 방식을 통해 새로운 것을 만들어내면서 이 과정을 지속하기 위해서는 서로에게 성실하게 응대해야 함을, 우리 삶의 방식이 서로가 서로에게 기대는 상호의존성에 있음을 깨닫게 만들기 때문이다.11 여기서 중요한 것이 바로 응답하기로, 응답하기는 상호의존성의 윤리적 실천이라 할 수 있다. 이 윤리는 상대를 책임(responsibility)지는 것, 즉 서로 얼굴을 마주하고 귀를 기울이며 상대의 요구에 즉각적으로 응답(response)할 수 있는 능력(ability)을 키우는 것이다(최유미, 2020, 43~45쪽).\n실뜨기와 마찬가지로 향모12 땋기도 공-산의 사유를 드러낸다. 서로 마주보고 머리를 맞댄 채 서로의 손을 바라보며 이야기를 나누는 향모 땋기는, 원주민 토박이의 지혜와 과학지식, 그리고 이 둘을 한 데 모으는 과학자의 이야기가 함께 녹아 있다(Kimmerer, 2013/2020). 향모 땋기는 인간과 다른 종들이 상호의존 관계임을 깨닫는 과정이며, 인류세가 만든 인간과 자연의 대립, 옛것과 새것의 갈등, 이로 인한 관계의 상실을 다른 종들과의 호혜적 관계를 통해 치유하고 복원하는 것이다. 그리고 이것은 우리가 주고받은 것들에 대해 책임지는 과정을 상징한다. 이처럼 실뜨기나 향모 땋기는 포스트휴먼이 가져야 할 응답하기와 책임지기라는 윤리를 실천한다. 이 과정에서 중요한 것이 이야기하기이다. 이야기하기는 자신의 경험과 처지를 이야기하고 다른 사람의 이야기를 들음으로써 공감과 감응, 사회적 유대를 만드는 역능이다. 이야기하기는 새로운 연결을 계속해서 만들어가는 과정이며, 차별과 편견, 혐오를 없애고 세상을 바꿀 수 있는 힘을 지닌다.\n유튜브 채널인 닷페이스13는 코로나19 팬데믹 상황에서 확진자를 치료해야하는 감염병동 의료진, 집집마다 방문하여 사람들을 접촉할 수밖에 없는 가스검침원, 거동이 불편한 노인들을 돌봐야 하는 요양보호사 등 비가시적 노동자들이 자신의 이야기를 할 수 있도록 채널을 내어준다. 이들의 이야기는 사회적 편견과 불평등을 드러내는 것임과 동시에 자신들을 이해하게 만드는 것이며, 이야기를 듣는 사람들을 자신의 친척으로 만드는 것이다. 이야기를 들은 사람들은 댓글로 응답하며 함께 변화를 만들어나가는 ’친척-되기’를 실천한다. 서로의 이야기에 응답하기는 이웃의 삶을 찬찬히 들여다보되, 그 삶에 적극적으로 개입하지 않으면서도 약한 유대를 회복하고 사회적 연대로 나아갈 수 있는 기초적 실천이다. 따라서 포스트휴먼인 우리는 자기 자신과 자신이 서있는 자리에 관해 이야기할 수 있어야 하며, 서로의 이야기를 가치있는 것으로 인정하고 지지해주고, 그 이야기의 가치를 실현할 수 있는 조건들을 만들어주어야 한다.\n이처럼 이야기하고 응답하고 책임지기는 경계짓기에 의한 배제가 아니라 이질적인 것들을 받아들이며 새로운 연결짓기를 생성하는 것이기도 하다. 그것은 ’정중함의 미덕을 가지고 방문하기’일 수도 있고(최유미, 2020, 95쪽), 자리를 내어주며 사회적 성원으로 환대하는 것일 수도 있다(김현경, 2015). 중요한 것은 일상의 안부를 묻는 소소한 의례이든 사회적 지지를 확인하는 공동체 의례이든 모든 관계에는 응답하기와 책임지기가 수행되어야 한다는 것이다. 이런 점에서 응답하기와 책임지기는 짐멜이나 마페졸리가 강조하는 ’관계를 만들어내는 근본 역능’을 일상적으로 실천하는 것이며, 사회적 유대의 결핍을 채우고 사회성의 탈구현상을 해결하는 윤리이기도 하다.\n\n\n3.4.4 자동화에 저항하기\n포스트휴먼은 삶 자체가 데이터를 생산하고 이용하는 데이터 기반의 삶이지만, 데이터를 수집하고 분석하는 빅데이터 기술 및 AI 알고리즘은 객관적이지도 중립적이지도 평등하지도 않다. 즉 누가 어떤 이유로 어떤 관점에서 어떻게 데이터를 수집하는가, 누구의 데이터는 수집하고 누구의 데이터는 배제하는가, 데이터 축적과 가공에는 어떤 알고리즘이 작동하는가 등 데이터 수집과 분석, 활용에는 정치적, 사회적 논리들이 작동한다. 특정 집단의 데이터만을 위해 데이터 알고리즘이 설계될 경우 데이터 수집에서 제외된 사람들은 그 시스템의 혜택으로부터 소외될 수밖에 없다.14\n우선, 빅데이터 기술은 사회적 약자를 데이터 수집에서 배제시킴으로써 차별을 고착화한다. ’인간은 곧 남성’이라는 위계화된 이분법적 사고는, 여성의 데이터를 배제하는 데이터 편향성의 논리로 작동한다. 일상과 직장, 설계, 의료, 공공생활, 재난 등 많은 분야에서 여성에 관한 정보와 지식이 무의식적으로 배제됨을 실증적으로 제시한 페레즈(Perez, 2019/2020)는 데이터 수집의 편향성과 여성의 비가시성이 얼마나 만연해있는가를 보여준다. 이러한 ’젠더 데이터 공백’은 여성의 건강과 안전을 위협하고, 여성을 차별하며, 존재 자체를 투명하게 만든다. 데이터가 곧 존재의 증명인 포스트휴먼 사회에서 여성은 존재하지 않으며, 여성을 위한 제도나 사회설계는 아예 고려조차 되지 않는다. 빅데이터 기술에서 배제되는 사람들은 대부분 여성, 빈민, 노숙자, 아동 같은 사회적 약자들이다.\n둘째, AI 알고리즘은 불평등을 자동화한다는 점이다. 기술숭배론은 빅데이터 기술과 알고리즘이 적절한 공공자원을 제공함으로써 사회적 불평등을 해소할 것이라고 기대하지만, 실제 연구는 이러한 기대가 틀렸음을 보여준다. 공공서비스에 도입된 자동화된 의사결정 시스템을 분석한 유뱅크스(Eubanks, 2018)는, 예측 알고리즘, 위험모형, 자동화된 복지수급자격 판정시스템 같은 정교한 데이터 기반 기술이 공공서비스에 도입되면서 불평등이 자동화되었다고 비판한다. 디지털 빈곤관리 도구인 자동화된 시스템이 가난을 지속시키고 공공자원의 요청을 단념시킴으로써 자동화된 불평등을 고착한다는 것이다. 이런 식으로 알고리즘과 빅데이터, 위험모형은 가난한 사람들을 분석하고 감시하고 처벌하는 ’디지털 구빈원’으로 작동한다. 더구나 데이터 수집과 분석 알고리즘이 투명하지 않으면 자동화 알고리즘에 저항하거나 수정하는 것이 불가능하다. 결국 자동화된 알고리즘은 사회적 불평등을 구조화한다.\n셋째, 데이터-네트워크-인공지능 기술은 사회성을 변형시킨다는 점이다. 코로나19로 생계에 타격을 입은 소상공인들은 자신의 피해를 입증하는 데이터를 인정받아야만 공적 지원을 받을 수 있다. 문제는 이런 식의 재난지원시스템이 사회적 유대 형성에는 아무런 도움이 되지 않는다는 점이다. 공적 지원의 자동화는 자신의 생존에만 집중하게 함으로써 타인의 고통과 사회적 불평등 문제를 외면하게 만든다. 본래 공적 부조를 돕는 조력자였던 공무원 역시 사회적 지지 네트워크로서보다는 소상공인이나 해고노동자의 데이터만을 평가하는 심판관이 된다. 정서적 유대는 업무의 효율성을 떨어뜨리는 비합리적 요소일 뿐이다. 이처럼 자동화된 행정시스템에서는 사회적 약자의 고통과 절망에 공감하며 유대를 형성할 여지가 거의 없다. 결국 자동화시스템은 사회적 유대를 약화시키고 사회성을 변형시키는 기제로 작동한다.\n이와 같이 편향된 데이터 테크놀로지와 자동화된 의사결정시스템은 차별과 불평등을 양산하고, 사회적 유대를 약화시킨다. 이러한 지능화테크놀로지에 대항하는 것은 데이터-네트워크-인공지능 기술생태계를 재배치할 수 있는 공-산의 윤리를 실천하는 것이다.\n우선, 데이터 액티비즘(Data Activism)은 데이터를 활용하여 사회문제를 해결하고 민주주의를 진작시키려는 일련의 사회적 실천이다.15 하나의 예로, 마스크 재고 알림 앱은 다양한 행위주체들이 연결짓기를 통해 코로나19 확산 초기 마스크 대란을 해결하는 데 기여하였다. 약국이 마스크 재고를 실시간으로 입력하면 질병청이 이 데이터를 API화하고, 이를 기반으로 협동조합, 시민단체, 개발자, 서비스 운영자로 구성된 ’코로나19 공공데이터 공동대응팀’이 앱을 개발하면 통신사가 서버를 제공하고 포털이 서비스를 제공하는 방식으로 프로젝트를 진행한 것이다(권오현, 2021). 이 프로젝트는, 상이한 배경을 가진 주체들이 공동의 프로젝트라는 접경지대에서 서로의 요구에 응답하고 책임지며, 공공의 이익을 위해 연대한 사회적 실천이라고 볼 수 있다.\n보다 근본적인 실천방식은 자동화를 변화시키는 비자동화 능력 즉 일을 발명하는 것이다. 디지털 테크놀로지를 파르마콘으로 보는 스티글레르(Stiegler & Kyrou, 2015/2018)는 자동화가 소비주의와 신자본주의의 무관심 경제를 만들어내는 것을 경계하면서, 자동화가 공공에 기여하는 약이 되도록 전환해야 한다고 주장한다. 스티글레르는 그 가능성을 모든 사람에게 개방되어 있는 프리웨어나 비자동화에 기초한 해석 웹 시스템에서 발견한다. 노트공유, 해석적 소셜네트워크, 온라인 토론지원 시스템 등의 플랫폼에서 아이디어나 실행방식, 모든 종류의 주장들을 대조하는 알고리즘을 체계적으로 실행하는 프로젝트이다(73~76쪽). 이 프로젝트의 목적은 무관심 경제로 인한 정신의 자동화를 변화시킬 수 있는 일의 발명, 즉 앎-살 줄-앎, 함께 할 줄-앎, 생각할 줄-앎을 키우는 것이다. 데이터 액티비즘이나 스티글레르의 일의 발명은 기술혐오와 기술숭배라는 이분법을 뛰어넘는다. 데이터-네트워크-인공지능 생태계가 이미 포스트휴먼의 삶의 조건이 된 현실에서 이러한 이분법은 대안이 되기 어렵다. 지금 우리에게 필요한 포스트휴먼의 윤리적 실천은 지능화테크놀로지를 사회적으로 재구성하는 것이다. 그것은 어떠한 목표와 가치를 설정하느냐, 얼마나 다양한 사회문화적 배경을 가진 사람들이 관여하느냐, 누구의 관점에서 데이터에 접근하느냐, 어떻게 알고리즘을 설계하느냐에 따라 달라질 수 있으며, 여기에 불평등과 차별을 해소하기 위한 공-산의 윤리가 개입되어야 한다."
  },
  {
    "objectID": "post.html#우리-모두는-트러블과-함께-하는-반려종",
    "href": "post.html#우리-모두는-트러블과-함께-하는-반려종",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.5 우리 모두는 트러블과 함께 하는 반려종",
    "text": "3.5 우리 모두는 트러블과 함께 하는 반려종\n영화 &lt;노마드랜드&gt;16는 2008년 세계금융위기로 가족과 직장을 잃고 밴을 집으로 삼아 떠도는 노마드의 삶을 다룬다. 이 영화는 지금까지 논의한 포스트휴먼의 윤리적 실천을 구체적으로 형상화한다. 주인공 펀17은 이동 지역의 식당이나 농장, 아마존 물류센터 등에서 일하며 노마드로서 살아간다. 펀이 정주하며 일했던 석고보드회사는 전형적인 제조업 기업인 반면, 임시로 일하는 아마존은 디지털 자본주의의 최전선에 서있는 플랫폼 기업이다. 이것은 산업구조의 변화를 보여주는 것이기도 하지만, 플랫폼 자본주의의 불안정하고 일시적인 노동형태를 드러내며 플랫폼 기업과 노동자 사이의 갈등을 드러낸다. 실제 미국에서 노마드는 금융위기로 집을 압류당하고 해고로 인한 경제위기와 가족해체 등으로 정주의 삶을 포기할 수밖에 없는 사람들이 대부분이다.\n그럼에도 노마드로서의 삶은 고립된 삶이 아니라 계속해서 연결되는 삶이다. 캠핑장18이라는 물리적 공간은 노마드로서의 삶을 지탱하는 베이스캠프가 되고, 사람들은 이곳을 거점으로 모이고 흩어지며 새로운 사람들을 계속해서 만난다. 이것이 가능한 것은 노마드들간의 연결지점 혹은 통로 역할을 하는 밥 웰스의 웹사이트와 블로그, 그리고 각자의 스마트폰이 있기 때문이다. 미디어테크놀로지와 결합한 노마드들은 유목적 주체로서 낯선 사람들과 위계화되지 않은 횡단적 연결짓기를 수행한다. 때때로 트러블을 일으키지만 자기돌봄뿐만 아니라 함께-돌봄을 실천하며 서로의 이야기를 듣고 서로에게 반응하며 사회적 유대를 형성한다. 이곳에서의 교류는 길을 떠나는 사람들에게 노마드로서의 존엄성을 지키게 해주는 정서적 자원을 제공한다. 그래서 포스트휴먼인 노마드들은 고독하지만 외롭지 않은 삶, ’따로 또 같이’라는 유목적 삶을 살면서 자연과 사람을 친척으로 만드는 것이다.\n우리사회에도 공존과 공생의 삶을 만들어가는 다양한 실험들이 존재한다. 청년주거 안정과 자치공동체를 활성화하기 위한 사회주택협동조합19은, 집을 매개로 한 연결짓기와 일상적 유대를 통해 청년들의 사회성을 강화하려고 시도한다. 이 시도는 낯선 사람들과 함께 사는 경험을 공유하는 공간으로 집을 재구성한다. 가족으로 구성된 공간에서, 가족 아닌 사람들과 함께 사는 공간으로 집에 대한 인식을 확장시키고, 이를 통해 혈연에 기반을 둔 가족을 벗어나 상호의존하며 공생하는 친척을 만들자는 것이다. 지자체가 빈집 정보를 제공하는 아키야뱅크(빈집은행)를 운영하고, 빈집을 사들여 전 세대가 어울려 살 수 있는 공공주택을 마련하거나 지역커뮤니티로 재생하는 일본의 사례 역시 함께 살아가기를 실천하는 방안이라 할 수 있다(남지현, 2018).\n소셜 다이닝(social dinning)도 식사라고 하는 일상적 의례를 통해 사회적 유대를 형성하려는 시도이다. ’식구’라는 말처럼 일반적으로 식사는 친밀한 관계 속에서 이야기하기와 듣기를 수반하여 이루어지는데, 소셜 다이닝은 식구의 의미를 낯선 사람들에게로 확대한다. 다른 처지에 놓인 사람들이 식탁에 앉아 각자의 이야기를 나누는 행위는 친척을 만드는 실천이기도 하다. 그러나 온라인 플랫폼 기반 소셜 다이닝은 대부분 성공하지 못했는데, 식사조차 ’경험의 공유’를 판매하는 수익 창출 도구로 활용하기 때문이다.20 소셜 다이닝의 사례는 자본의 개입이 사회성을 훼손시킬 수 있음을 방증한다.\n인간의 가장 기본적 생존조건인 주거와 식사를 통해 사회적 유대를 강화하려는 이러한 시도들은 플랫폼이라는 온라인 접속과 대면 접촉을 통한 함께-하기가 유기적으로 결합될 때 실현될 수 있다. 비대면 삶의 방식이 강화될수록 오히려 대면 접촉을 확장하기 위한 노력을 더 많이 기울여야 한다. 사회적 유대를 돈독히 하기 위해 ’쓸모없는 함께-하기’라는 순수한 사회성의 형식을 만들어내는 것도 하나의 방법이다(Maffesoli, 2000/2017, 154~159쪽). 쓸모없는 함께-하기는 일상적 삶에서 아주 소소한 경험들을 만들어내면서 수평적이고 감성적인 연대를 형성하는 것이다. 코로나19의 사회적 거리두기로 대면 접촉이 줄어드는 상황에서 서로의 안부를 묻고 곁을 내어주는 이러한 사회적 실천이 어느 때보다 절실히 필요하다. 팬데믹으로 팽배해진 인종주의와 동성애 혐오 등 사람들의 편견도 사회적 접촉을 통해 해소될 수 있다. 다른 생각을 하는 사람들과 더 많이 접촉하고 더 가까이 있을수록 편견이 줄어들기 때문이다(Berbner, 2019/2021).\n무엇보다 중요한 것은 포스트휴먼이 사회성의 역능을 키울 수 있는 공-산의 생태계를 만드는 것이다. 데이터-네트워크-인공지능의 기술적 생태계가 자본과 차별의 논리로 작동하지 않도록 이분법적 경계들을 지우고 이질적인 것들과 접촉하면서 이들과 친척이 되는 새로운 생태계를 만들어야 한다. 새로운 생태계가 공생과 환대의 생태계가 되기 위해서는 서로에게 응답하고 서로의 이야기에 귀 기울이며 서로를 책임지는 무수한 실천들을 통해 상호의존적 연결관계를 회복하는 것이 필요하다. 더불어 기술적 생태계의 자동화된 불평등으로부터 새로운 생태계를 지켜내기 위한 저항도 요구된다. 이러한 포스트휴먼의 윤리적 실천들은 사회적 연대와 유대의 탈구현상을 절합하고 사회성의 역능을 키우며, 데이터-네트워크-인공지능 기술생태계를 공-산의 생태계로 재배치하는 데 기여할 것이다.\n이 과정에서 포스트휴먼인 우리는 대문자 인간(Humanity)이 아닌 인류(humankind)로서 기술, 동물, 바이러스 등 무수한 비인간 존재들과 상호의존적인 공생적 실재임을 깨닫고 이들과 함께 자본주의 논리에 저항하는 생태학적 인식을 가져야 한다(Morton, 2017/2021). 이것이 진정 포스트휴머니즘의 윤리인 공-산을 실천하는 포스트휴먼의 자세일 것이다."
  },
  {
    "objectID": "post.html#참고문헌",
    "href": "post.html#참고문헌",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.6 참고문헌",
    "text": "3.6 참고문헌\n권오현 (2021). 디지털시민주권을 확립하는 시민개발자(시빅해커)들. 《한국언론정보학회 봄철 정기학술대회 발표문》. 2021. 5. 29.\n김상호 (2009). 확장된 몸, 스며든 기술: 맥루한 명제에 관한 현상학적 해석. 「언론과학연구」, 제9권 2호, 167~206.\n김선희 (2005). 사이보그와 개인동일성의 문제: 컴퓨터와의 융합을 통하여 우리는 영생할 수 있는가? 「철학」, 제85집, 171~191.\n김재희 (2014). 우리는 어떻게 포스트휴먼 주체가 될 수 있는가? &lt;철학연구&gt;, 제106집, 215-242.\n김재희 (2017). &lt;시몽동의 기술철학: 포스트휴먼 사회를 위한 청사진&gt;. 파주: 아카넷.\n남지현 (2018). 일본에서는 빈집을 어떻게 활용하고 있나? 《세계와 도시》, 22호, 12~25.\n마정미 (2008). 포스트휴먼과 탈근대적 주체에 관한 연구. 「인문콘텐츠」 제13호, 193~212.\n박선희 (2022). 인간-미디어 공생체의 사회: 포스트휴먼의 사회성과 윤리적 실천. &lt;언론과 사회&gt;, 29권 4호. 5~49.\n브루노 라투르 외 (2010). &lt;인간·사물·동맹: 행위자네트워크 이론과 테크노사이언스&gt;. 서울: 이음\n신상규 (2008). 「푸른 요정을 찾아서: 인공지능과 미래인간의 조건」. 서울: 프로네시스.\n이수안 (2017). 테크노페미니즘으로 본 몸의 물질성과 감각의 확장: 오를랑(Orlan)의 테크노바디를 중심으로. &lt;한국여성학&gt;, 제33권 1호, 77~105.\n임석원 (2013). 비판적 포스트휴머니즘의 기획: 배타적인 인간중심주의 극복. 이화인문과학원 편 &lt;인간과 포스트휴머니즘&gt; (61~82쪽). 서울: 이화여자대학교출판부.\n전혜은 (2010). &lt;섹스화된 몸: 엘리자베스 그로츠와 주디스 버틀러의 육체적 페미니즘&gt;. 서울: 새물결.\n최유미 (2020). 《해러웨이, 공-산의 사유》. 서울: 도서출판b.\nAttali, J. (2003). L’Homme Nomade. 이효숙 (역) (2005년). 《호모 노마드, 유목하는 인간》. 서울: 웅진지식하우스.\nBerbner, B. (2019). Geschichten gegen den Hass. 이승희 (역) (2021). 《혐오없는 삶: 나와 다른 사람과 친구가 될 수 있을까?》. 서울: 판미동.\nBraidotti, R. (2013). The posthuman. 이경란 (역) (2015). 《포스트휴먼》. 파주: 아카넷.\nByron, P. (2021). Friendship and digital cultures of care. London: Routledge.\nClark, A. (2003). Natural-born cyborgs: Minds, technologies, and the future of human intelligence. 신상규 옮김 (2015). &lt;내추럴-본 사이보그: 마음, 기술, 그리고 인간지능의 미래&gt;. 파주: 아카넷.\nCoeckelbergh, M. (2020). AI ethics. 신상규・석기용 옮김 (2023). . 파주: 아카넷.\nDeleuze, G., & Guattari, F. (1980). Mille plateaux: Capitalisme et schizophrenie. 김재인 (역) (2001). 《천 개의 고원: 자본주의와 분열증 2》. 서울: 새물결.\nEubanks, V. (2018). Automating inequality: How high-tech tools profile, police, and punish the poor. 김영선 (역) (2018) 《자동화된 불평등: 첨단 기술은 어떻게 가난한 사람들을 분석하고, 감시하고, 처벌하는가》. 서울: 북트리거.\nFerrando, F. (2020). Philosophical posthumanism. 이지선 옮김 (2021). &lt;철학적 포스트휴머니즘: 포스트휴먼 시대를 이해하는 237개의 질문들&gt;. 아카넷.\nHaraway, D. (1991). Simians, cyborgs and women: The reinvention of nature. 황희선·임옥희 옮김 (2023). &lt;영장류, 사이보그 그리고 여자: 자연의 재발명&gt;. 파주: 아르테.\nHaraway, D. (2016). Manifestly Haraway. 황희선 옮김 (2019). &lt;해러웨이 선언문&gt;. 책세상\nHaraway, D. (2016). Staying with the trouble: Making kin in the Chthulucene. 최유미 (역) (2021). 《트러블과 함께하기: 자식이 아니라 친척을 만들자》. 서울: 마농지.\nHayles, N. K. (1993). The seductions of cyberspace. In D. Trend. (Ed.), Reading Digital Culture (pp. 305~321). MN: Univ. of Minnesota Press.\nHayles, N. K. (1996). Embodied virtuality: Or how to put bodies back into the picture. In M. A. Moser (Ed.), Immersed in Technology: Art and Virtual Environments (pp. 1~28). Cambridge, MA: MIT Press.\nHayles, N. K. (1997). Voices out of bodies, bodies out of voices: Audiotape and the production of subjectivity. A. Morris. (Ed.). Sound state: Innovative poetics and acoustical technologies(pp. 74-96). Chapel Hill, NC: The University of North Carolina Press.\nHayles, N. K. (1999). How we became posthuman: Virtual bodies in cybernetics, literature, and informatics. 허진 (역) (2013). 《우리는 어떻게 포스트휴먼이 되었는가》. 파주: 플래닛.\nHerbrechter, S. (2009). Posthumanismus: Eine kritische Einführung. 김연순·김응준 옮김 (2012). &lt;포스트휴머니즘: 인간 이후의 인간에 관한 문화철학적 담론&gt;. 서울: 성균관대학교 출판부.\nKimmerer, R. W. (2013). Braiding sweetgrass. 노승영 (역) (2020). 《향모를 땋으며: 토박이 지혜와 과학 그리고 식물이 가르쳐준 것들》. 서울: 에이도스.\nLum, C. (Ed.) (2006). Perspectives on culture, technology and communication: The media ecology tradition. 이동후 (역) (2008). 《미디어생태학사상: 문화, 기술 그리고 커뮤니케이션》. 서울: 한나래.\nLynch, M. P. (2016). The Internet of us: Knowing more and understanding less in the age of big data. New York, NY: Liveright Publishing. 이충호 옮김. &lt;인간 인터넷: 사물 인터넷을 넘어 인간 인터넷의 시대로&gt;. 서울: 사회평론.\nMaffesoli, M. (2000). Le Temps des tribus. 박정호·신지은 (역) (2017). 《부족의 시대: 포스트모던 사회에서 개인주의의 쇠퇴》. 서울: 문학동네.\nMcLuhan, M. (1964). Understanding media: The extensions of man (Critical ed.) (2003). 김상호 (역) (2011). 《미디어의 이해: 인간의 확장》 (비평판). 서울: 커뮤니케이션북스.\nMiah, A. (2008). A critical history of posthumanism. In B. Gordijn & R. Chadwick. (Eds.), Medical enhancements and Posthumanity (pp. 71-94). New York, NY: Routledge.\nMorse, M. (1998). Virtualities: Television, Media Art, and Cyberculture. Bloomington: Indiana University Press.\nMorton, T. (2017). Humankind: Solidarity with non-human people. 김용규 (역) (2021). 《인류: 비인간적 존재들과의 연대》. 부산: 부산대학교출판문화원.\nO’Neil, C. (2016). Weapons of math destruction: How big data increases inequality and threatens democracy. 김정혜 (역) (2017). 《대량살상수학무기: 어떻게 빅데이터는 불평등을 확산하고 민주주의를 위협하는가》. 서울: 흐름출판.\nPenny, S. (1994). Virtual reality as the completion of the enlightenment project. In G. Bender & T. Druckrey (Eds.). Cultures on the Brink: Ideologies of Technology (pp. 231~248). Seattle: Bay Press.\nPerez, C. (2019). Invisible women: Data bias in a world designed for men. 황가한 (역) (2020). 《보이지 않는 여자들: 편향된 데이터는 어떻게 세계의 절반을 지우는가》. 서울: 웅진지식하우스.\nPeters, J. D. (2015). The marvelous clouds: Toward a philosophy of elemental media. 이희은 (역) (2018). 《자연과 미디어: 고래에서 클라우드까지, 원소 미디어의 철학을 향해》. 서울: 컬처룩.\nSennett, R. (2012). Together: The rituals, pleasures, and politics of cooperation. 김병화 (역) (2013). 《투게더: 다른 사람들과 함께 살아가기》. 서울: 현암사.\nSimondon, G. (1958). Du mode d’existence des objets techniques. Éditions Aubier. 김재희 (역) (2011). 《기술적 대상들의 존재 양식에 대하여》. 서울: 그린비.\nSrnicek, N. (2017). Platform capitalism. Cambridge. 심성보 (역) (2020). 《플랫폼 자본주의》. 서울: 킹콩북.\nStiegler, B., & Kyrou, A. (2015). L’emploi est mort, vive le travail!. 권오룡 (역) (2018). 《고용은 끝났다, 일이여 오라! : 베르나르 스티글레르와의 대담》. 서울: 문학과지성사.\nThe Care Collective (2020). The Care manifesto. 정소영 (역) (2021). 《돌봄 선언: 상호의존의 정치학》. 서울: 니케북스.\nTsing, A. L. (2015). The mushroom at the end of the world: On the possibility of life in capitalist ruins. 노고운 옮김 (2023). 《세계 끝의 버섯: 자본주의의 폐허에서 삶의 가능성에 대하여》. 서울: 현실문화.\nWarwick, K. (2002). I, Cyborg. 정은영 (역) (2004). 《나는 왜 사이보그가 되었는가》, 서울: 김영사."
  },
  {
    "objectID": "post.html#더-생각해볼-문제",
    "href": "post.html#더-생각해볼-문제",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.7 더 생각해볼 문제",
    "text": "3.7 더 생각해볼 문제\n\n인공지능을 공포나 숭배의 대상이 아니라 다른 존재로 볼 수 있을까? 그 존재와 어떤 관계를 형성할 수 있을까? 인공지능과 더불어 산다는 것은 어떤 의미이고 어떤 경험일까?\n우리의 일상을 지배하는 배달플랫폼 노동 같은 플랫폼 자본주의 문제를 포스트휴머니즘의 관점에서 접근한다면 어떤 이야기들을 만들어내고 어떤 해결책을 찾아낼 수 있을까?\n기후재앙으로 일컬어지는 인류세 시대에 포스트휴먼으로서 어떤 태도와 윤리적 실천이 가능할까?"
  },
  {
    "objectID": "post.html#더-읽을거리",
    "href": "post.html#더-읽을거리",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.8 더 읽을거리",
    "text": "3.8 더 읽을거리\n신상규 외 (2020). &lt;포스트휴먼이 몰려온다: AI 시대, 다시 인간의 길을 여는 키워드 8&gt;. 파주: 아카넷.\n김초엽·김원영 (2021). &lt;사이보그가 되다&gt;. 파주: 사계절."
  },
  {
    "objectID": "post.html#footnotes",
    "href": "post.html#footnotes",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "",
    "text": "트랜스휴머니스트인 케빈 워릭(Warwick, 2002/2004)은 자신과 부인의 왼쪽 팔 피부에 전자 칩을 이식하여 자신의 위치신호를 컴퓨터로 전송하고, 부인과 전자신호로 교감하는 실험을 진행하였다.↩︎\n워릭이 사이보그가 되는 자세한 과정에 대해서는 Warwick(2002/2004) 참조.↩︎\n김선희(2005)는 신체동일론에 입각하여 사이보그를 분석하고 있는데, 고전적 사이보그의 경우 사이보그 신체가 시공적으로 물리적 공간에서 지속된다는 점에서 신체동일성이 보장되는 반면, 탈고전적 사이보그의 경우 개인이 유기적 네트의 일부가 되거나 신체의 물리적 경계가 사라짐으로써 신체동일성의 문제도 해소되어버린다고 주장한다.↩︎\n박선희(2022)의 일부내용을 재인용함.↩︎\n노마디즘이 국경을 넘어 이윤을 추구하는 글로벌기업을 옹호하고, 노동유연화에 내몰려 떠돌 수밖에 없는 노동조건을 은폐하며, 경계를 넘나들 수 없는 난민이나 이주노동자에 대해 무관심한 것을 정당화한다고 바판받기도 하지만, 이 논문에서 노마디즘은 근대의 고정된 위계구조와 그로 인한 사회적 불평등에 대해 문제제기하는 하나의 은유 혹은 사유를 의미한다.↩︎\nhttps://www.globalcitizen.org/en/live/↩︎\nhttps://www.youtube.com/watch?v=0ly61HpQ3mU&t=4s↩︎\nhttps://www.window-swap.com/Window↩︎\n돌봄은 직접 누군가에게 육체적・심리적 도움을 주는 것만을 의미하는 것이 아니라 사회적 역량이자 삶에 필요한 모든 것을 보살피는 사회적 활동이며, 우리의 상호의존성을 인지하고 포용하는 것을 의미한다(The Care Collective, 2020, 17쪽).↩︎\nhttp://articulab.hcii.cs.cmu.edu/projects/sara/↩︎\n실뜨기 놀이를 경험한 사람이라면, 놀이를 할 때마다 매번 새로운 패턴이 만들어지고, 실뜨기를 계속하기 위해 서로 노력해야 하며, 서로의 노력 여하에 따라 놀이가 지속될 수도 중단될 수도 있음을 알 것이다. 상대와 놀고 싶은 마음이 없을 때, 또는 성실한 응대를 하고 싶지 않을 때 두세 번만에 놀이를 중단시킬 수도 있다.↩︎\n향모는 향기롭고 성스러운 풀로, 부족사람들에게는 ’어머니 대지님의 머리카락’이다. 세 갈래로 땋아서 선물을 주거나, 태워서 제의적 검댕을 만든 후 몸과 영혼을 치유하는 데 사용한다.↩︎\nhttps://www.youtube.com/c/facespeakawake↩︎\n데이터 수집・분석・활용은 프라이버시 침해와 감시를 동반한다. 그러나 아이러니하게도 빅데이터 기술의 혜택을 받기 위해서는 프라이버시 침해와 감시를 감수하고 자신의 정보를 모두 내놓아야 한다. 근대국민국가의 통치성인 조직화와 감시가 샴쌍둥이라는 기든스(Giddens, 1985)의 성찰이 자동화사회에서도 적용될 수 있음을 보여준다.↩︎\n데이터 액티비즘은, 오픈소스운동, 시빅해킹(civic hacking) 등 여러 형태로 이루어지며, 그 활동범위도 감시활동과 환경보호, 건강, 구호활동 등 매우 다양하다.↩︎\n이 영화는 언론인 제시카 브루더(Jessica Bruder)가 밴과 트레일러를 타고 다니며 생활하는 사람들을 3년간 취재한 논픽션 《노마드랜드: 21세기 미국에서 살아남기》(Nomadland: Surviving America in the Twenty-First Century, 2017)를 원작으로 만들어졌다.↩︎\n펀(Fern)은 땅에 심는 씨앗이 아니라 흩뿌려지는 포자로 생존하는 양치식물을 상징하는데, 특정 정착지 없이 떠도는 삶을 사는 노마드를 의미한다. 특정한 목적없이 어디로든 뻗어나갈 수 있다는 점에서 펀이라는 양치식물의 이미지는 들뢰즈와 가타리의 리좀과 닮아있다.↩︎\n아이러니하게도 이 캠핑장은 아마존과 협력관계인데, 이는 플랫폼이 삶의 터전이 될 수도 자본주의의 전유공간이 될 수도 있음을 암시한다.↩︎\n민달팽이주택협동조합 https://minsnailcoop.com/↩︎\n소셜다이닝은 플랫폼 스타트업으로 시작되었으나 이벤트 중심의 운영과 수익성 문제로 오래 지속되지 못했다. 금천구청이 지원하는 대대식당과 소소식당 같은 지역기반 소셜다이닝은 유지되었으나, 현재는 코로나19로 중단된 상태이다.↩︎"
  },
  {
    "objectID": "labor.html#서론",
    "href": "labor.html#서론",
    "title": "4  인공지능과 노동",
    "section": "4.1 서론",
    "text": "4.1 서론\n1800년대 초공장 자동화가 노동을 대체할거라고 생각하여 기계를 파괴한 러다이트(Luddites) 운동을 시작으로 기술 혁신은 역사적으로 근로자들 사이에 일자리 대체에 대한 불안을 불러일으켜 왔다. 로봇 등 일자리 자동화가 인간 노동을 대체할 것이라는 두려움은 일부 현실이 되기도 하고 기술 혁신으로 인해 다른 부문의 노동 수요를 자극하여 새로운 일자리를 만들어내기도 했다. 그렇다면 인공지능(최근 생성형 인공지능으로 인해 촉발된) 기술 혁신은 노동 시장에 어떤 영향을 미칠까?\n노동 시장에 대한 혁신의 효과를 정확하게 측정하는 것은 매우 복잡한 문제이다. 제2차 세계대전 이후 미국은 40년 동안 혁신으로 인해 노동 수요가 강화되었고, 이후 40년 동안 성장이 둔화되었다. 전기, 내연 기관과 같은 2차 산업 혁명의 주요 발명품은 복제할 수 없을 만큼 엄청난 경제적 영향을 미쳤다. 연구에 따르면 제품 혁신(새롭거나 향상된 제품/서비스)은 프로세스 혁신(제품/서비스 생성 방식의 개선)보다 생산성과 경제적 성과를 더 높이는 경향이 있다. 노동 시장을 연구하는 학자들에 따르면 노동력을 강화(labor-augmenting)거나 인간이 수행하지 않거나 수행할 수 없는 작업을 수행하는 데 초점을 맞춘 기술 혁신은 결국 총 노동 수요를 자극하여 더 많은 일자리의 창출로 이어진다고 주장한다. 이러한 혁신은 프로세스 혁신보다 제품 혁신과 더욱 밀접하게 연계된다고 보았다. 반대로, 인공지능과 같은 기존 인간의 작업 수행에 초점을 맞춘 노동 자동화 기술은 프로세스 혁신과 더 유사하며 총 노동 수요를 감소시킬 가능성이 더 높다.\n하지만, 혁신을 순전히 제품이나 프로세스, 노동 증대 또는 노동 자동화로 분류하는 것은 복잡한 문제를 너무 단순하게 푸는 것과 같다. 예를 들어, 자동차 자체는 제품 혁신일 뿐만 아니라 운송이라는 과정에 대한 프로세스 혁신이기도 하기 때문이다. 혁신이 노동에 미치는 영향은 대체(자동화를 통해 노동 수요 감소), 회복(새로운 작업 창출 및 그에 따른 노동 수요 창출), 생산성 효과로 분류된다. 노동 수요에 대한 혁신의 전반적인 효과는 생산성 효과의 규모에 따라 결정되는데, 이는 높은 임금의 노동을 혁신이 대체할 때 가장 강력하다고 할 수 있다.\n인류는 현재 인공지능(AI)이라는 거대한 혁명의 중심에 서 있다. 이 혁명은 단순히 기술 발전을 넘어 우리의 일상생활, 직업의 본질, 심지어 우리가 자신과 세계를 인식하는 방식까지 근본적으로 변화시키고 있다. 따라서 인공지능이 노동 시장에 미치는 효과는 개인의 생계, 기업의 운영 방식, 국가 경제의 구조에 이르기까지 매우 광범위할 수 밖에 없다. 이 변화를 이해하는 것은 단순한 호기심, 즉 ’인공지능이 어떤 직업을 대체할 수 있을까?’에 대한 답을 찾는 것을 넘어, 우리 모두에게 미래 사회를 준비하는 실질적인 필요성이 되다. 본 챕터에서는 ‘인공지능과 노동’이라는 주제를 통해 인공지능의 발전이 사회에 미치는 영향을 깊이 있게 탐구하고자 한다.\n먼저 인공지능에 대한 기본적인 이해부터 시작해보려고 한다. 인공지능은 컴퓨터나 기계가 인간의 지능적인 행동을 모방하고, 복잡한 문제를 해결하며, 의사결정을 내릴 수 있는 고도의 기술이다. ****이 기술은 이미 여러분의 일상 속 깊숙이 자리 잡고 있다. Siri와 Bixby와 같은 스마트폰의 개인 비서 기능부터 의료 분야의 진단 시스템, OTT의 맞춤형 콘텐츠 추천, 금융 서비스의 자동화된 고객 상담까지 다양한 형태로 우리 삶에 영향을 미치고 있다.\n하지만 인공지능이 인류에 가장 큰 영향을 주는 영역은 노동 시장일 것이다. 일부는 인공지능을 일자리를 파괴하는 위협으로 보지만, 다른 이들은 기존의 일을 혁신하고 새로운 기회를 창출하는 동력으로 여기곤 한다. 실제로 인공지능은 특정 작업을 자동화함으로써 일부 직업을 대체할 수 있으나, 동시에 새로운 직업을 창출하고, 기존 직업의 효율성을 높이며, 전례 없는 혁신을 가져올 수 있다.\n\n예를 들어, 의료 분야에서는 인공지능이 다양한 방식으로 활용되고 있다.인공지능은 환자의 위치를 정확하게 파악하고 CT 이미지를 재구성하는 데 도움을 주어 방사선과학과의 효율성을 향상시키거나, 심장 기능을 평가하는 데 사용되는 초음파 측정의 복잡성을 줄이는 데 도움을 주고 있다.1\n\n\n제조업에서는 인공지능이 생산 과정을 최적화하고 불량률을 줄이는 데 기여하고 있다.예를 들어, 지멘스(Siemens)는 인공지능과 기계 학습을 활용하여 산업 자동화와 데이터 분석을 개선하고 있고, IBM은 클라우드, 인공지능, 기계 학습 도구를 제공하여 생산 시간과 비용을 줄이는 데 도움을 주고 있다.2\n\n그러나 인공지능의 발전이 가져오는 변화는 단순히 기술적인 측면에 그치지 않는다. 우리는 어떤 일자리가 사라지고, 어떤 일자리가 변화할지, 그리고 새로운 일자리는 어떻게 생겨날지를 예측하고 대비해야 한다. 이 과정에서 정부, 기업, 교육 기관, 개인 등 모든 이해관계자의 역할이 매우 중요하다. 정부는 적절한 정책과 규제를 통해 인공지능의 긍정적인 측면을 촉진하고 부정적인 영향을 최소화해야 하며, 기업은 직원들의 재교육과 재취업 지원 프로그램을 제공하여 변화하는 시장 요구에 부응해야 한다. 또한, 교육 기관은 학생들에게 미래의 노동 시장에 필요한 기술과 지식을 제공하여 이러한 변화에 대비할 수 있도록 해야 한다. 마지막으로 우리 모두 개인 차원에서 이러한 변화를 단순히 관찰자의 입장에서 보는 것이 아니라, 적극적으로 이해하고, 준비하며, 올바른 방향으로 이끌어 갈 필요가 있다."
  },
  {
    "objectID": "labor.html#인공지능의-진보와-일자리-자동화",
    "href": "labor.html#인공지능의-진보와-일자리-자동화",
    "title": "4  인공지능과 노동",
    "section": "4.2 인공지능의 진보와 일자리 자동화",
    "text": "4.2 인공지능의 진보와 일자리 자동화\n\n4.2.1 인공지능 기술의 급격한 발전과 현재의 다양한 적용\n많은 사람들이 처음으로 인공지능이 대단하다고 느끼는 순간은 대체로 인공지능이 체스나 바둑과 같은 게임에서 인간 챔피언을 이기는 장면이었을 것이다. 예를 들어, 1997년 IBM의 ’딥 블루’가 세계 체스 챔피언 가리 카스파로프를 이겼고, 2016년에는 Google의 ’알파고’가 바둑의 세계 챔피언 이세돌을 이겼다. 물론 이세돌 9단은 알파고와의 4번째 대국에서 ’신의 한수’를 만들어 승리했지만, 이러한 순간들은 인공지능이 단순한 계산을 넘어 인간의 전략적 사고를 모방하고, 때로는 뛰어넘을 수 있는 능력을 가지고 있음을 전 세계에 알리는 신호탄이 되었다.3\n그러나 실제로 인공지능은 체스나 바둑과 같은 게임에서의 승리에 그치지 않고, 이미 우리 일상생활에 깊숙이 침투해 있다. 예를 들어, 음성 인식 기술을 통해 우리는 스마트폰이나 가정용 스마트 스피커에게 명령을 내릴 수 있다. Siri, Alexa, Google Assistant와 같은 AI 비서들은 단순한 명령 수행에서부터 일정 관리, 뉴스 업데이트 제공, 심지어 감성적인 대화에 이르기까지 다양한 작업을 수행하고 있다. 이러한 기술들은 우리의 생활 방식을 변화시키며, 더 편리하고 연결된 삶을 가능하게 한다.\n의료 분야에서 AI의 역할은 더욱 중요해지고 있다. AI는 환자 데이터를 분석하여 질병을 진단하고, 적절한 치료 방법을 제안하며, 심지어 새로운 약물을 개발하는 데 기여하고 있다. 예를 들어, 구글의 DeepMind는 ‘알파폴드’ 프로그램을 통해 단백질의 3D 구조를 예측하는 데 혁명적인 진전을 이루었다. 이러한 발전은 맞춤형 의학과 치료법의 개발을 촉진시켜, 더 효과적이고 개인화된 의료 서비스를 제공할 수 있게 한다.4\n자동차 산업에서는 자율 주행 차량의 등장이 가까운 미래의 현실로 다가오고 있다. Tesla, Waymo, Uber와 같은 회사들은 이미 도로에서 자율 주행 테스트를 진행하고 있다. 이 기술이 완전히 성숙되고 널리 보급되면, 교통 사고의 감소, 교통 체증의 해소, 운전자의 편의 증진 등 많은 긍정적인 변화를 가져올 것으로 기대된다. 자율 주행 차량은 또한 이동성에 제한이 있는 사람들에게 새로운 기회를 제공하고, 물류 및 배송 산업에도 혁신을 가져올 것이다.\n\n\n4.2.2 인공지능이 일자리와 직업 구조에 미치는 광범위한 영향\n일부 전문가들은 인공지능을 ’일자리 파괴자’로 보는데, 특히 반복적이고 예측 가능한 작업을 수행하는 직업들이 위험에 처해 있다고 지적한다. 예를 들어, 제조업에서는 로봇과 AI 시스템이 인간 노동자를 대체하여 더 빠르고 정확하게 작업을 수행할 수 있는데, 이러한 자동화는 생산성을 크게 향상시킬 수 있지만, 동시에 기존의 일자리를 줄일 수 있는 이중적인 특성을 가지고 있다. 고객 서비스 분야에서는 AI 챗봇이 기본적인 문의 사항을 처리하고, 효율적인 서비스를 제공하면서, 전통적인 콜센터 직원의 역할을 변화시키고 있다.5\n반면, 다른 전문가들은 인공지능을 ’기회의 창출자’로 보고, AI가 담당할 수 있는 반복적인 작업들을 자동화함으로써, 인간 노동자들이 더 창의적이고 전략적인 작업에 집중할 수 있게 될 것이라고 주장한다. 예를 들어, 프롬프트 엔지니어, 인공지능을 활용한 콘텐츠 기획자와 같은 새로운 직업들이 등장하고 있다. 이러한 직업들은 기술 발전의 이점을 활용하며, 경제와 사회에 새로운 가치를 창출한다. 또한, 의료 분야에서 AI가 의사와 간호사의 진단과 치료 결정을 보조함으로써, 의료 서비스의 질을 향상시키고, 환자에게 더 나은 치료를 제공할 수 있게 되었다. 이러한 변화는 기존의 직업을 더 효율적이고 효과적으로 만들며, 새로운 전문 분야와 기회를 창출할 것이다.6\n\n\n4.2.3 자동화가 가능한 일자리는 인공지능으로 인해 대체 가능하다\n일자리 자동화는 인공지능(AI)과 로봇공학의 발전으로 인해 특정 작업이 기계에 의해 수행되는 현상을 말한다. 이 현상은 제조업을 시작으로 의료, 금융, 서비스업에 이르기까지 다양한 분야에서 점점 더 두드러지게 나타나고 있다. 자동화의 가장 대표적인 예로는 자동차 제조업에서의 로봇 팔 사용을 들 수 있다. 이 로봇들은 사람보다 훨씬 빠르고 정확하게 차량을 조립하고, 무거운 부품을 들어 옮기며, 위험한 작업을 수행하여 인간 노동자의 부상 위험을 줄인다. 그러나 이러한 기계의 도입은 전통적인 조립 라인에서의 수많은 일자리를 줄이는 결과를 가져왔다. 사람들은 이제 더 복잡하고 창의적인 업무에 집중할 수 있게 되었지만, 동시에 많은 사람들이 기존의 일자리를 잃게 된 것이다.\n서비스업에서의 자동화는 주로 ’소프트웨어 로봇’이나 AI를 통해 이루어지고 있습니다. 예를 들어, 은행업계에서는 전통적인 창구 직원의 역할이 ATM과 온라인 뱅킹 시스템에 의해 대체되고 있다. 이러한 기계는 24시간 동안 고객의 요구에 응답할 수 있으며, 오류의 가능성을 줄이면서 많은 루틴 업무를 처리할 수 있다. 더 나아가, AI 기반의 챗봇은 고객 문의에 대응하고, 간단한 금융 상담까지 제공할 수 있는데, 이는 고객 서비스의 효율성을 높이고, 인간 직원이 더 복잡하고 전략적인 작업에 집중할 수 있게 한다. 그러나 이러한 변화는 전통적인 고객 서비스 직종의 일자리 수를 줄이는데 큰 기여를 하고 있다.\n\n\n4.2.4 자동화로 인해 기존 인간의 역할이 바뀔 것이다\n한편, 기존의 자동화가 가능한 일부 직업들은 인공지능에 의해 대체되기보다는 역할이 변화하는 형태로 진화할 것이다. 전통적으로 회계사는 기본적인 계산과 데이터 입력과 같은 반복적인 작업을 수행해왔다. 하지만 이제 이러한 작업은 인공지능에 의해 자동가 가능하다. 그렇다고해서 회계사라는 직업이 사라지지는 않는다. 오히려 회계사로 하여금 더 복잡한 분석, 전략적 의사결정 지원, 고객 상담과 같은 더 높은 수준의 작업에 집중할 수 있는 기회를 제공할 것이다. 인공지능은 데이터를 빠르게 처리하고 데이터 베이스에 저장할 수 있는 능력을 가지고 있지만, 그 데이터 안에서 의미 있는 정보를 분석하고 이를 사용자(회계사)에게 제공하는 능력은 제한적이다. 회계 전문가들은 이러한 인공지능의 능력을 활용, 기업의 방대한 데이터를 분석하여 중요한 통찰력을 얻을 수 있으며, 이 정보를 기반으로 보다 전략적인 결정을 내릴 수 있다. 더 나아가, 회계 전문가들은 이 과정에서 얻어진 자료를 바탕으로 조직의 자문 역할을 강화할 수 있게 된다. 특히, 블록체인 기술과 일부 AI 도구를 활용함으로써, 회계 감사의 안전성과 보안성이 크게 향상될 것으로 기대된다. 이러한 기술의 도입은 회계 업무의 효율성과 정확성을 높이는 동시에 조직 전반의 신뢰성을 증진시킬 것이다.이처럼 자동화는 기존 직업의 역할을 변화시키고, 직업을 보다 전문화되고 창의적인 방향으로 발전시킬 수 있는 잠재력을 가지고 있다.7\n\n\n4.2.5 취약한 직업군과 견고한 직업군\n인공지능의 위협을 가장 많이 받는 직업들은 일반적으로 높은 수준의 반복적 작업을 포함하고 예측 가능한 환경에서 일하는 직업들이다. 이러한 직업군에는 제조업 노동자, 콜센터 직원, 일부 사무직, 그리고 운전 기반의 직업들이 포함된다. 이들 직업에서는 AI나 로봇이 인간보다 더 빠르고 정확하게 작업을 수행할 수 있으며, 피로나 오류의 위험이 없다는 장점이 있다. 따라서 이러한 직업들은 인공지능이 대중화될 수록 사라질 위험이 높은 취약한 직업군이라고 할 수 있다.\n반면, 창의성, 복잡한 사회적 상호작용, 고도의 비판적 사고를 요구하는 직업들은 AI가 쉽게 대체하기 어려운 영역이다. 예술가, 과학자, (직관을 통해) 전략적 결정을 내리는 관리자 등은 인간만의 독특한 능력을 필요로 하는 영역에서 활동하며, 이러한 직업들은 AI의 발전에도 불구하고 여전히 중요하고 가치 있는 역할을 수행할 것이다."
  },
  {
    "objectID": "labor.html#생성형-인공지능과-일자리",
    "href": "labor.html#생성형-인공지능과-일자리",
    "title": "4  인공지능과 노동",
    "section": "4.3 생성형 인공지능과 일자리",
    "text": "4.3 생성형 인공지능과 일자리\n파운데이션 모델(Foundation model)을 사용하여 텍스트, 이미지, 코드와 같은 새로운 콘텐츠를 생성하는 기술인 생성형 인공지능은아직 초기 단계이지만 노동 시장에 미치는 잠재적 영향은 상당할 것이다. 1980년 이후 혁신은 저임금 및 중임금 일자리 자동화에 중점을 두어 상대적으로 낮은 비용 절감으로 인해 생산성 효과가 제한되었지만 생성형 인공지능으로 촉박된 노동 시장의 반응은 고임금 근로자에게 더 높은 위험을 초래하고 잠재적으로 상당한 비용 절감으로 이어져 결과적으로 더 강력한 생산성 효과와 총 노동 수요를 증가시킬 수 있다는 점에서 기존의 기술 혁신발 노동 시장의 변화와는 결이 다를 것으로 예상된다.\n생성형 인공지능은 일반적으로 노동 수요 증가 가능성이 낮은 프로세스 혁신으로 분류된다. 이는 상당한 생산성 효과를 가져옴과 동시에 많은 직업이 사라질 수 있음을 시사한다. 범용 기술인 생성형 인공지능은 코드 작성부터 사기 탐지(Fraud detection)까지 다양한 산업과 작업에 걸쳐 광범위한 영향을 미치며 생산성 향상을 이루어낼 것으로 기대되고 있다.\n이때, 인구 통계학적 변수와 국가마다 다르게 진화하는 인공지능에 대한 제도들은 생성형 인공지능이 노동 수요에 미치는 영향을 다르게 만들 수 있는 요소들이다. 노동력 부족은 자동화를 장려하며, 이는 노동력이 노령화되거나 감소하는 국가에서 생성형 인공지능의 채택과 응용이 더 많아지고 결과적으로 생산성 효과가 더 커질 수 있음을 시사한다. 그러나 근로자 보호와 노조의 힘이 강한 제도를 가진 국가(또는 지역)에서는 인공지능의 노동 대체 효과가 약해 직업적 업무가 보다 순차적으로 전환되고 단순히 업무를 자동화하는 대신 기존 고용된 일자리 노동자들의 생산성 향상을 위해 생성형 인공지능을 사용하는 데 중점을 둘 것이다. 노동자 보호가 강화되면 비용 절감 효과가 낮아 국가 전반적인 생산성 효과가 감소할 수 있지만, 노동 시장에서 보다 균형감 있고 지속 가능한 전환으로 이어질 수도 있다.\n생성형 인공지능은 노동 시장에서 불평등(Inequality)에 영향을 미칠 수 있다. 고소득 직종과 산업이 가장 위험에 처해 있기 때문에 이들 근로자를 대체하면 처음에는 불평등이 줄어드는 것 처럼 보일 수도 있을 것이다. 연구에 따르면 생성형 인공지능을 도입하면 상위 1%에 속하는 사람들은 큰 영향을 받지 않을 수도 있지만 상위 1%와 10% 사이의 소득 격차는 줄어들 수 있다고 한다. 고소득 기술을 AI로 대체하면 저임금 기술에 대한 수요와 임금이 증가하여 임금 격차가 더욱 줄어들기 때문이다. 그러나 이것이 보편적인 불평등 완화로 이어질지는 미지수이다. 생성형 인공지능으로 대체되는 고소득 직종이 많이 분포한 지역은 경제적, 사회적으로 어려움을 겪을 수 있기 때문이다.8"
  },
  {
    "objectID": "labor.html#인공지능이-불러온-사회-변화-트렌드",
    "href": "labor.html#인공지능이-불러온-사회-변화-트렌드",
    "title": "4  인공지능과 노동",
    "section": "4.4 인공지능이 불러온 사회 변화 트렌드",
    "text": "4.4 인공지능이 불러온 사회 변화 트렌드\n\n4.4.1 고용 없는 노동의 현실화\n현대 산업에서 자동화와 인공지능의 급속한 발전은 노동 시장에 본질적인 변화를 가져오고 있다. 기계와 소프트웨어의 작업 복잡성 증가로 인해 다양한 산업에서 여러 수준의 자동화가 도입되고 있으며, 이러한 인공지능으로 촉발된 디지털 전환이 고숙련 일자리 증가 및 저숙련 일자리 감소로 이어지는 숙련 편향적 기술 변화를 초래하고 있다(Balsmeier & Woerter, 2019). 국내에서도 약 55~57%의 일자리가 향후 컴퓨터로 대체될 가능성이 높은 것으로 나타나며, 일부 연구에서는 이 비율이 70.6%에 달한다고 보고되었다(김세움 2015; 박가열 외 2016)\n자동화, 로봇화, 컴퓨터화의 발전은 반복업무기반(routine-based) 직업의 인력 수요 감소를 예측하며, 특히 교통, 유통, 사무직 등이 디지털 전환에 따라 높은 위협을 받는 분야로 분석됩니다. 미국에서는 약 47%의 일자리가 고위험군에 속한다고 보고되었다(Frey & Osborne, 2017). 그러나 동시에 프로그래밍, 데이터 분석과 같은 비반복적 인지 기반 직무의 일자리는 증가할 것으로 예측된다(Acemoglu & Restrepo, 2018).\n인공지능은 또한 노동의 질적 측면에서도 변화를 가져온다. 특히 플랫폼 노동의 증가는 새로운 형태의 일자리와 노동거래 방식을 등장시켰으며, 이러한 경제는 네트워크를 통한 직접적인 일자리 중개 및 배정에 관여한다. 플랫폼 노동은 서비스 또는 가상재화 거래, 디지털 플랫폼을 통한 일거리 구함, 대가나 보수 중개, 그리고 특정인이 아닌 다수에게 열려 있는 일감 중개 등을 특징으로 한다. 이러한 경제는 코로나19의 영향으로 더욱 활성화되었으며, 특히 광의의 플랫폼 종사자 수가 2021년 220만 명에서 2022년 292만 명으로 증가하기도 하였다(고용노동부, 2022).\n플랫폼 노동의 확대는 고용 없는 노동의 장기적인 경제성장과 효율성 증가 잠재력을 가지고 있지만, 이는 또한 사회 안전망의 변화와 교육 체계에 대한 새로운 요구를 제기한다. 플랫폼 노동자에 대한 보호 제도 관련 논의가 발전했지만, 전통적 일자리에서 플랫폼 일자리로의 이동은 노동시장 내 인력 수급 및 운용 문제를 야기할 수 있다. 이러한 노동 시장의 편차와 불균형에 대응하기 위해서는 인간의 삶의 질을 향상시키는 지속 가능한 발전을 중심으로 한 정책적 방안이 필요하다(민순홍, 2023).\n또한, 알고리즘에 의한 노동자 통제와 관련한 논의도 중요하다. 최근 인공지능 플랫폼 노동에 일감을 배분하고 통제하는 데 인공지능 알고리즘을 적용하는 형태가 등장했기 때문이다. 이는 다이내믹 프라이싱(Dynamic pricing)을 통한 이윤 극대화 목적으로 활용되지만, 노동과 관련한 공정성 및 투명성 측면에서 문제가 제기되고 있다(장진희·노성철·현종화, 2022).\n결론적으로, 디지털 전환에 따른 다양한 노동 변화에 대응하기 위해서는 정부와 기업 간의 적절한 정책적 협력이 필수적이다. 기술 자동화가 대체하는 것은 ’직업’이 아니라 ’직무’임을 인식하고, 새롭게 등장하는 일자리에 대한 다방면의 고려가 필요하다. 또한, 플랫폼 노동과 관련하여 윤리적인 기술 활용과 적절한 규제의 중요성을 인식하고, 이에 대한 논의를 적극적으로 이어나가야 한다.\n\n\n4.4.2 인간노동 대체의 소비시대\n현대 소비자는 편의와 개인화된 서비스를 요구하며, 특히 코로나19와 인공지능의 발달로 인한 비대면화와 온라인화는 디지털 전환을 가속화하고 있다. 이는 전통적인 소비 패턴의 변화뿐만 아니라, 새로운 서비스와 비즈니스 모델의 등장을 촉발시켰다. 인공지능과 로봇은 판매부터 로지스틱까지 다양한 분야에서 활용되며, IoT·AI의 발달로 즉각적이고 개인화된 소비자 참여가 가능해졌다. 이에 따라 구매 채널 방식도 옴니채널이나 온디맨드 서비스 등으로 전환되는 추세다(한국마케팅연구원, 2022).\n유통업계는 AI를 활용한 ‘초개인화’ 서비스를 선보이며 소비자의 취향과 생각을 분석해 상품과 서비스를 제공하고 있다. 이는 ‘나만을’ 위한 맞춤 상품 소비를 가능하게 하며, 기업들은 구매 전환율을 높이고 차별화된 쇼핑 환경을 제공함으로써 시장에서 경쟁력을 확보하고자 한다. 이러한 디지털 전환은 노동과 소비시장의 구조 변화를 가져온다. 알고리즘 소비의 상대적 유익성, 소비자가 알고리즘 시장에서 가지는 지배력, 비용 절감이나 효용 비율이 소비자의 후생에 미치는 영향 등이 중요한 요소다. 알고리즘 도입이 증가할수록 과거의 노동력 투입이나 인지적 노력보다 더욱 적은 비용으로 빠르고 효과적인 삶의 수행이 가능해진다.\n인공지능 알고리즘은 경제활동에서의 비효율을 축소해 소비효용을 개선할 것으로 전망된다. 이는 정보를 낮은 비용으로 분석해 선택 다양성을 확보하고, 소비자 의사결정을 보완하며, 개인 특성에 따른 편향성을 감소시키는 등의 방식으로 이루어진다(Picht and Freund ,2018). 디지털 전환에 따른 소비 패턴의 재편은 개인적 측면뿐만 아니라 지역 및 사회 전반에 영향을 미칠 수 있다. 개인별 맞춤 가격 설정은 사업자에게는 새로운 기회를 창출할 수 있으나, 소비자 잉여의 축소와 같은 부정적 측면도 있다(허민영, 임병권, 2021). 기술 발전과 정보량 증가에 따라 알고리즘이 개인의 지불의사가격에 근접해 가격을 설정하는 사업모델이 확대될 것으로 예상된다.\n이러한 변화는 사회적 책임과 윤리를 중시하는 새로운 요구사항을 제기한다. 개인화된 가격으로 인한 소비자 이익 감소, 가격의 공정성과 시장의 신뢰, 기업의 개인정보 활용 범위와 윤리성 등 새로운 소비자 문제를 검토할 필요가 있다. 또한, 인공지능 알고리즘이 수집하는 개인정보와 그것의 활용에 대한 윤리적, 규범적, 철학적 고민이 필요하다.\n인공지능 시대의 소비 패턴 변화에 대응하기 위해서는 기업과 정부 간 협력이 필수적이며, 소비자 보호를 위한 적절한 규제가 중요하다. 이러한 미래의 지능정보사회에서 나타날 법적 이슈는 소비자의 안전을 보장할 개인정보 보호 강화와 새로운 산업에 대한 규제 완화 요구라는 상호 모순된 법제적 측면이 병존할 수 있다. 인공지능 알고리즘을 활용한 시장의 발전은 정보규율에 의해 결정되며, 개인정보의 수집 및 활용과 보호 사이에 존재하는 상충관계를 균형잡는 것이 중요하다.\n\n\n4.4.3 유목적 관계의 시대\n글로벌화와 디지털 기술 발전은 국가 간 경제 통합을 촉진하고 글로벌 네트워크를 가능하게 했다. 이는 다문화와 다양성을 중요한 가치로 부각시키며 사람들 사이의 연결을 강화했다. 최근 글로벌 대기업들은 다양성을 중요한 가치로 여기며 ’다양성 보고서’를 작성하는 등 인력 구성에 있어 연령, 성별, 민족, 성적 취향 등의 다양성을 존중하고 있다. 매킨지리포트에 따르면 다양성이 높은 기업은 그렇지 않은 기업보다 재무 수익이 높다고 한다(McKinsey, 2015). 이는 다양성이 기업 경쟁력으로 이어지기 때문이다.\n원격 노동과 프리랜서 경제는 노동 시장의 유연성을 증대시켰다. 교육과 업무 스킬의 전세계적 적용은 인력 이동을 촉진하고, 이민과 인력 이동은 새로운 경제 기회를 제공한다. 디지털 시대에는 컴퓨터의 등장으로 일하는 방식이 근본적으로 변화했으며, 직원들은 서로 다른 연관된 업무를 비동시적으로 일할 수 있게 되었다. 이는 근무의 유연성을 높이고 공간에 대한 종속성을 완화시켜 ‘디지털 노마드’ 현상을 가져왔다(정민재·이정교, 2018).\n가상 커뮤니티와 사회적 네트워킹은 협력과 공유 문화의 확산을 촉진하며 사람들의 삶의 질을 향상시켰다. 이는 모바일 기반의 전자상거래의 편재성, 편리성, 즉흥성, 경제성 등에서 확인할 수 있듯이 시간과 공간의 제약에서 벗어나 다양한 가치를 제공하는 구매채널로 자리 잡았다(김경희, 2018).\n유목적 관계의 시대는 국제 협력과 다자간 협상의 필요성을 강조하며, 정보 보안과 개인 정보 보호가 중요한 이슈가 되었다. 클라우드, AI 등의 기술 적용과 비대면 업무 등이 일상화되어가고 있지만, 사이버 위협도 빠르게 증가하고 있다. 예를 들어 줌바밍 해킹 사건처럼 화상 회의 중에 해커가 침투하여 내부 정보를 빼돌리거나 불법 게시물을 올리는 사건이 문제가 되었다. 이러한 취약성은 회사 내부의 중요 자료 유출에 대한 우려와 통제되지 않는 위험성을 초래한다. 따라서 디지털대전환시대의 소비 패턴 변화에 대응하기 위해서는 기업과 정부 간 협력이 필수적이며, 소비자 보호를 위해 적절한 수준의 규제가 중요하다. 이와 더불어 인공지능 알고리즘을 활용한 시장의 발전은 정보규율에 의해 결정되며, 개인정보의 수집 및 활용과 보호 사이에 존재하는 상충관계를 균형잡는 것이 중요하다."
  },
  {
    "objectID": "labor.html#논의와-대응책",
    "href": "labor.html#논의와-대응책",
    "title": "4  인공지능과 노동",
    "section": "4.5 논의와 대응책",
    "text": "4.5 논의와 대응책\n\n4.5.1 다양한 관점에서의 논의\n인공지능(AI)과 노동에 대한 논의는 사회의 다양한 분야와 각자의 전문 분야에 따라 다른 관점을 제공한다. 경제학자들은 인공지능이 경제 성장과 생산성, 그리고 노동 시장에 미치는 영향을 분석한다. 이들은 AI가 일자리를 대체할 가능성, 새로운 일자리 창출, 임금 격차와 같은 경제적 파급 효과를 연구한다. 사회학자들은 사회 구조와 노동 시장에 대한 변화를 탐구하며, 인공지능이 사회적 계층, 교육, 불평등과 어떻게 상호작용하는지를 분석한다. 철학자와 윤리학자들은 인공지능의 발전이 인간의 존엄성, 자율성, 윤리적 선택에 미치는 영향을 논하며, AI의 책임과 도덕적 한계에 대해 논의한다. 기술 전문가들은 AI 기술의 발전 가능성과 한계를 탐색하고, 새로운 혁신과 기술적 도전을 이해하기 위해 노력한다. 이러한 다양한 관점의 논의는 인공지능이 노동에 미치는 복합적인 영향을 이해하고, 보다 폭넓고 균형 잡힌 시각을 갖는 데 중요하며, 모든 관점에서의 포괄적 논의가 필요하다.\n예를 들어, 경제학적 관점에서는 일자리 자동화가 생산성을 향상시킬 수 있지만, 동시에 불평등을 심화시킬 수 있다는 점을 지적한다. 이는 특히 저숙련 노동자나 반복적인 작업을 수행하는 사람들이 AI에 의해 가장 먼저 영향을 받을 수 있음을 의미한다. 사회학적 관점에서는 인공지능이 노동 시장에서 일부 집단에게 더 큰 영향을 미칠 수 있으며, 이는 사회적 긴장과 분열을 야기할 수 있음을 강조한다. 예를 들어, 특정 지역이나 산업에서의 일자리 감소는 지역 경제에 심각한 영향을 미칠 수 있으며, 이는 사회적 불안정성을 증가시킬 수 있다는 것이다.\n\n\n4.5.2 정부, 기업, 교육기관의 역할\n인공지능과 노동의 상호작용에 대응하기 위해서는 정부, 기업, 교육기관 등 모든 이해관계자의 역할과 책임이 매우 중요하다. 정부는 적절한 정책과 규제를 통해 인공지능의 긍정적인 측면을 촉진하고 부정적인 영향을 최소화해야 한다. 이는 국가 차원에서의 전략적 계획을 포함하며, 인공지능 관련 정책, 법률, 윤리 지침을 개발하는 것을 의미한다. 예를 들어, 정부는 재교육 및 재취업 프로그램을 지원하여 노동 시장의 변화에 대비할 수 있도록 해야 하며, 인공지능의 윤리적 사용을 위한 법적 틀을 마련해야 할 것이다. 또한, 정부는 사회 안전망을 강화하여 기술 변화로 인해 일자리를 잃은 사람들이 생계를 유지할 수 있도록 지원해야 한다.\n기업은 인공지능을 도입하면서 발생할 수 있는 윤리적, 사회적 문제를 고려해야 한다. 이는 기업의 사회적 책임(CSR) 활동의 일환으로, 직원들의 재교육과 재취업 지원 프로그램을 제공하는 것을 포함한다. 기업은 기술 발전에 따른 직원의 직업 안전과 복지를 보장하기 위한 노력이 필요하며, 이를 위해 직원들과의 소통과 협력을 강화해야 할 것이다. 또한, 기업은 인공지능의 윤리적 사용을 위한 내부 지침과 프로토콜을 개발하고, 이해관계자들과의 협력을 통해 지속 가능한 방식으로 기술을 채택해야 한다.\n교육기관은 학생들에게 미래의 노동 시장에 필요한 기술과 지식을 제공하는 중요한 역할을 한다. 이는 단순히 기술적 기술을 가르치는 것뿐만 아니라, 비판적 사고, 창의성, 사회적 기술과 같은 보편적 기술을 강조하는 것을 포함한다. (이 책이 그러한 역할을 하기 바란다). 교육기관은 평생 학습의 중요성을 강조하고, 학생들에게 유연한 학습 기회를 제공함으로써 빠르게 변화하는 노동 시장에 대응할 수 있게 해야 한다. 이는 전통적인 교육 시스템을 넘어서 온라인 학습, 직업 훈련 프로그램, 계속 교육 과정 등 다양한 형태의 교육을 포함할 수 있다.\n\n\n4.5.3 인공지능과 노동의 미래에 대한 심층적 전망\n인공지능(AI)과 노동의 상호작용은 앞으로 수십 년 동안 우리 사회와 경제에 광범위하고 깊이 있는 영향을 미칠 것이다. 이 변화의 파도는 일부 직업을 사라지게 하거나 근본적으로 변화시킬 것이며, 동시에 새로운 직업과 기회를 창출할 것으로 보인다. 이러한 변화는 우리에게 도전을 제시할 뿐만 아니라, 적절하게 대응한다면 무한한 가능성을 열어줄 기회로도 작용할 수 있다. 즉, 인공지능의 발전은 생산성 향상, 혁신 촉진, 삶의 질 개선과 같은 긍정적인 잠재력을 가지고 있는 동시에 불평등의 심화, 직업 안정성의 감소, 윤리적 및 사회적 문제와 같은 도전도 함께 제기하고 있다."
  },
  {
    "objectID": "labor.html#더-읽을-거리와-생각해-볼-문제",
    "href": "labor.html#더-읽을-거리와-생각해-볼-문제",
    "title": "4  인공지능과 노동",
    "section": "4.6 더 읽을 거리와 생각해 볼 문제",
    "text": "4.6 더 읽을 거리와 생각해 볼 문제\n\n4.6.1 더 읽을 거리: 2023년 미국 작가 조합 파업\n\n2023년 5월 2일부터 9월 27일까지 148일 동안 진행된 미국 작가 조합 파업은 인공지능과 노동의 관계에 대한 중요한 시사점을 제공한다. 파업의 주요 쟁점은 인공지능의 발전으로 인해 작가들의 일자리가 위협받을 수 있다는 우려였다. 작가 조합은 인공지능을 활용한 콘텐츠 제작을 규제하고, 작가들의 최저임금을 인상하는 등의 요구를 했다.\n파업은 148일 만에 잠정 합의로 종결되었다. 합의안에는 인공지능을 활용한 콘텐츠 제작에 대한 규제 강화, 작가들의 최저임금 인상, 작가들의 단체교섭권 강화 등이 포함되었다.\n이 파업은 인공지능이 노동시장에 미치는 영향에 대한 노동자들의 우려를 반영한 것이다. 인공지능은 노동의 생산성을 높이고 새로운 일자리를 창출하기도 하지만, 기존 일자리를 대체하기도 한다. 따라서 인공지능과 노동의 관계를 조화롭게 발전시키기 위해서는 인공지능의 부정적 영향에 대한 대책을 마련하는 것이 필요하다.\n2023년 미국 작가 조합(WGA) 파업은 디지털 시대의 복잡한 노동 문제를 상징하는 중요한 사건이다. 이 파업은 OTT 시장의 확장과 작가들의 업무량 증가에도 불구하고 임금이 줄어들었다는 불만에서 시작되었다. 이와 관련하여, 작가들은 재방료 기준 확립, 원고료와 출연료 인상, AI를 통한 인력 감축 철회 등을 요구했다. 특히, AI 기술의 발전이 각본 작업에 미치는 영향과 OTT 드라마의 재방료 문제는 논쟁의 핵심이었다.\n\n\n\n4.6.2 인공지능과 노동의 쟁점\n인공지능과 노동의 관계에 대한 쟁점은 크게 두 가지로 나눌 수 있다.\n\n첫 번째 쟁점은 인공지능이 노동시장에 미치는 영향에 대한 것이다. 인공지능이 노동의 생산성을 높이고 새로운 일자리를 창출하기도 하지만, 기존 일자리를 대체하기도 한다. 따라서 인공지능이 노동시장에 미치는 영향에 대한 분석은 매우 중요하다.\n두 번째 쟁점은 인공지능과 노동의 관계를 조화롭게 발전시키기 위한 방안에 대한 것이다. 인공지능과 노동의 관계를 조화롭게 발전시키기 위해서는 인공지능의 부정적 영향에 대한 대책을 마련하는 것이 필요하다. 또한, 인공지능을 활용한 새로운 일자리를 창출하기 위한 노력도 필요하다.\n\n\n\n4.6.3 생각해 볼 문제: 2023년 미국 작가 조합 파업과 관련하여\n기술 발전과 창작 환경: 2023년 미국 작가 조합 파업에서 작가들은 AI의 사용을 제한하고, OTT 드라마의 재방료 문제를 제기했다. 이러한 요구는 기술 발전이 창작 환경에 어떤 영향을 미치는지에 대한 중요한 질문을 제기한다.\n\n기술의 발전이 창작자의 권리와 수익을 어떻게 변화시키고 있는지, 그리고 이에 대한 공정한 해결책은 무엇이라고 생각하는가?\n\n\n4.6.3.1 디지털 시대의 재방료와 로열티\n\nOTT 시대의 등장으로 전통적인 재방송 개념이 사라졌고, 작가들의 수익 구조에 변화가 생겼다. 이러한 변화 속에서 작가와 창작자들의 재방료와 로열티를 어떻게 보호하고 보장해야 할까?\n또한, OTT 플랫폼이 작품별 조회수와 같은 데이터를 공개하지 않는 것에 대해 어떻게 생각하는가? 노동 운동과 기술 변화에 대한 대응: 미국 작가 조합의 파업은 기술 변화에 대응하는 노동 운동의 중요한 사례이다. 미래에 기술의 변화가 더욱 가속화될 것으로 예상될 때,\n노동 운동이나 기타 사회적 운동이 이러한 변화에 어떻게 효과적으로 대응해야 한다고 생각하는가?\n또한, 노동자의 권리 보호를 위해 어떤 전략이 필요할까?\n\n\n\n4.6.3.2 Food for thoughts\n\n2023년 미국 작가 조합 파업의 결과는 인공지능과 노동의 관계에 어떤 영향을 미쳤을까?\n인공지능이 노동시장에 미치는 영향에 대해 긍정적, 부정적 측면을 각각 생각해 보자.\n인공지능과 노동의 관계를 조화롭게 발전시키기 위한 방안을 제시해 보자."
  },
  {
    "objectID": "labor.html#부록",
    "href": "labor.html#부록",
    "title": "4  인공지능과 노동",
    "section": "4.7 7. 부록",
    "text": "4.7 7. 부록\n본 챕터는 아래 노션 페이지에서 확인 가능하며, 계속해서 내용이 수정, 보완될 예정입니다. 내용과 문법에 오류가 있으면 코멘트 달아주시고, 아래 메일 주소로 보내주시면 감사하겠습니다.\nhttps://www.notion.so/cjleeskku/32a45c406c9d4390b90df16fe2a14641?pvs=4\n메일 주소: changjunlee@skku.edu"
  },
  {
    "objectID": "labor.html#참고문헌",
    "href": "labor.html#참고문헌",
    "title": "4  인공지능과 노동",
    "section": "4.8 8. 참고문헌",
    "text": "4.8 8. 참고문헌\n길은선·송영진·신위뢰 (2019). &lt;고용 없는 성장의 특성 및 산업별 분석&gt; (연구보고서 2019-913). 산업연구원.\n김세움 (2015). &lt;기술진보에 따른 노동시장 변화와 대응&gt;. 한국노동연구원.\n민순홍 (2023). &lt;플랫폼 노동 선택의 결정 요인과 플랫폼 종사자의 직업 이동 경로 분석&gt; (연구자료 2023-01). 산업연구원.\n박가열‧천영민‧홍성민‧손양수 (2016). &lt;기술변화에 따른 일자리 영향 연구&gt;. 한국고용정보원.\n이금노 (2018). &lt;인공지능 알고리즘 기반 경제에서의 소비자문제 연구&gt; (정책연구 18-17). 한국소비자원.\n이문호 (2020). 4차 산업혁명을 둘러싼 쟁점들 -’노동사회학적 관점’에서-. 노동연구, 40, 47-86.\n장진희·노성철·현종화 (2022). &lt;플랫폼노동의 알고리즘 현황과 대응방안 - 알고리즘의 공정성과 투명성, 노동자 통제를 중심으로&gt; (연구총서 2022-08). 한국노총중앙연구원.\n최병록 (2017). &lt;4차 산업혁명시대의 소비자이슈와 소비자정책&gt;. 한국기술혁신학회 추계학술대회.\n한국마케팅연구원 (2022). AI 기술과 초개인화 서비스. &lt;마케팅 2022&gt;, 56권 8호, 26-36.\n허민영·임병권 (2021). &lt;코로나 이후 디지털 전환 가속화에 따른 소비자정책 방향 연구&gt; (정책연구 21-01). 한국소비자원.\nAcemoglu, D., & P. Restrepo. (2018). The race between man and machine: Implications of technology for growth, factor shares, and employment. American Economic Review, 108(6), 1488-1542.\nBalsmeier, B., & M. Woerter. (2019). Is this time different? How digitalization influences job creation and destruction. Research Policy, 48(8), 103765. https://doi.org/10.1016/j.respol.2019.03.010.\nCharles, K.K., E. Hurst, & M.J. Notowidigdo (2013). Manufacturing decline, housing booms, and non-employment, Technical Report, NBER Working Paper, 18949, National Bureau of Economic Research.\nFrey, C.B. and M.A. Osborne (2017), “The future of employment: How susceptible are jobs to computerisation?,” Technological Forecasting & Social Change, 114, 254-280.\nGal, M., & Elkin-Koren, N. (2017). Algorithmic Contracts.Harvard Journal of Law and Technology,30, 309.\nPicht, P. G., & Freund, B. (2018). Competition (law) in the era of algorithms.Max Planck Institute for Innovation & Competition Research Paper, (18-10)."
  },
  {
    "objectID": "labor.html#footnotes",
    "href": "labor.html#footnotes",
    "title": "4  인공지능과 노동",
    "section": "",
    "text": "10 real-world examples of AI in healthcare↩︎\n11 AI in Manufacturing Examples to Know | Built In↩︎\nHow IBM’s Deep Blue Beat World Champion Chess Player Garry Kasparov Google’s AlphaGo wins final Go game against Lee Sedol↩︎\n딥마인드, 단백질 생성 AI ‘알파폴드’ 최신 버전 공개↩︎\nAI in Manufacturing: How It’s Used and Why It’s Important for Future Factories The Top 5 Benefits of Using Chatbots in Customer Service Teams↩︎\n10 new jobs created with AI in the workplace↩︎\n[Biz Focus] AI가 회계사 대체한다고?…오히려 귀한몸 된다 - 매일경제↩︎\nGenerative AI and the labor market: A case for techno-optimism↩︎"
  },
  {
    "objectID": "tai.html#인공지능-신뢰성-문제",
    "href": "tai.html#인공지능-신뢰성-문제",
    "title": "5  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "5.1 인공지능 신뢰성 문제",
    "text": "5.1 인공지능 신뢰성 문제\n인공지능(artificial intelligence, AI) 기술이 빠르게 발전하면서 사회 전반에 AI 전환(AI transformation, AIX)에 대한 기대가 커지고 있다. 그러나 다른 한편으로는 AI가 초래할지도 모를 부정적 영향에 대한 우려도 적지 않다.\nAI는 알고리듬에 의한 모델링 편향(modeling bias) 문제, 학습데이터에 의한 교육 편향(bias in training) 문제, AI를 악용하는 사람이나 조직의 문제 등으로 공정성 문제가 발생할 수 있다(손영화, 2023). 그 결과 AI가 편견을 학습해 범죄 예측이나 채용, 복지 제공 등에서 성이나 인종, 국적 차별을 하는 사례가 보고됐다. 학습데이터에 포함된 개인정보를 유출한다거나, 자살이나 위험한 놀이를 권유하거나, 체스 게임 중에 인간을 위협하고 실제 상해를 입히는 사례도 있었다 (한국정보통신기술협회, 2023; 손영화, 2023). 챗GPT의 환각(hallucination), 생성 AI를 활용한 가짜 뉴스, AI 생성물을 논문이나 시험, 과제물, 창작물 등에 비공개로 사용하는 것 등도 AI의 신뢰도를 훼손한다. 전쟁에 사용되는 킬러 로봇(killer robots)에 대한 우려도 커지고 있다(Beutel, Geerits, & Kielstein, 2023; Krishnan, 2016).\n게다가 이러한 문제를 개선하고자 하더라도 AI의 설명가능성 문제가 제기된다. 많게는 수조 개의 매개변수를 학습하는 딥러닝 방식이 일반화되면서 AI가 왜 그런 결과를 내놓았는지를 이해하기 어렵기 때문이다(고학수 등, 2021). 때문에 2024년 1월 현재, 딥러닝의 대부인 요슈아 벤지오(Yoshua Bengio) 캐나다 몬트리올대학교 교수와 미래학자인 유발 하라리, 일론 머스크 테슬라 사장, 에마드 모스타크 스테빌리티AI 사장 등 3만3000명이 넘는 인물들이 GPT-4를 넘어서는 AI 개발을 6개월 간 중단하자는 서한에 서명하기도 했다(Futrue of life, 2023.3). AI의 성능 개선 속도가 너무 빠르기 때문에 예기치 못할 AI의 해악에 대응할 시간을 벌자는 취지다.\nAI가 야기할지도 모르는 정치적, 경제적, 사회적, 문화적, 국제적 측면의 전방위적 문제에 대한 논의는 AI 윤리, AI 공정성(fairness), 설명가능한 인공지능(eXplainable AI, XAI), 책임 있는 AI(responsible AI) 등의 논의를 거쳐 일단 신뢰할 수 있는 인공지능(trustworthy AI, TAI)이라는 개념 아래 종합되는 추세다.\n이 장에서는 TAI의 개념과 전개, 그리고 저널리즘 분야에 적용 가능성을 살펴보도록 한다. 내용을 간략하게 살펴보면 다음과 같다. 우선 2절에서는 학제적인 신뢰 개념을 검토하고 이를 바탕으로 TAI를 정의한다. 우선 신뢰의 하위 개념으로는 신뢰성과 신뢰도를 살펴본다. 다음으로 신뢰의 두 유형으로 인간 신뢰와 기계 신뢰를 논의한다. 이어 TAI의 개념을 신뢰성과 신뢰도, 인간 신뢰와 기계 신뢰 측면에서 개념화한다. 3절에서는 TAI의 논의가 어떻게 전개됐는지 유럽연합 집행위원회(European Commission, EC), 경제협력개발기구 (Organisation for Economic Co-operation and Development, OECD), 그리고 국내에서 진행된 TAI 논의를 중심으로 살펴본다. TAI의 논의는 윤리적인 측면에서 기술적인 측면으로 구체화됐다. 4절에서는 TAI의 구체적인 영역(domain)으로서 저널리즘 분야를 놓고, TAI가 저널리즘 분야의 AI, 즉 저널리즘 AI에 어떻게 적용될 수 있는지 살펴본다. 저널리즘 AI의 신뢰성은 AI로서의 신뢰성 외에도 언론인과 언론사의 신뢰성을 제고함으로써 달성된다. 이를 위해서는 언론인이 저널리즘 AI의 기술적 루프(loop) 속에 참여해야 한다."
  },
  {
    "objectID": "tai.html#신뢰할-수-있는-인공지능의-개념",
    "href": "tai.html#신뢰할-수-있는-인공지능의-개념",
    "title": "5  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "5.2 신뢰할 수 있는 인공지능의 개념",
    "text": "5.2 신뢰할 수 있는 인공지능의 개념\n\n5.2.1 신뢰의 정의\nTAI를 논의하기에 앞서 신뢰의 개념을 살펴보자. 우선 신뢰는 신뢰자(trustor)가 신뢰 대상(trustee)을 믿는 것이다. 그러나 신뢰는 단순히 믿는 것 이상을 의미한다. 신뢰에 대한 심리학, 사회학, 경제학 분야의 연구들을 검토한 바에 따르면, 신뢰는 타인의 의도(intention)나 행동(behavior)에 대한 긍정적인 기대(positive expectation)에 근거하여 취약성(vulnerability)을 수용하려는 의지에 따른 심리적 상태로 정의할 수 있다(Rousseau et al, 1998).\n무엇보다 신뢰는 위험(risk)이 존재한다. 이러한 면에서 신뢰는 확신(confidence)과 다르다. 확신에는 위험 감수가 없다. 그러나 신뢰에는 위험 감수가 존재한다(Luhmann, 2000). 위험은 신뢰 대상의 의도와 미래의 행동이 불확실성(uncertainty)을 갖기 때문에 발생한다. 즉 신뢰 대상은 반사회적인 의도와 행동을 할 수 있다. 이러한 위험이 있는 상태에서 신뢰자는 신뢰 대상에 대한 긍정적 기대를 바탕으로 자신의 취약성을 받아들일 때 신뢰자는 신뢰 대상을 신뢰한다고 할 수 있다. 위험은 신뢰의 기회를 마련한다. 신뢰는 위험 감수에 도움을 준다. 위험 감수는 신뢰를 강화한다(Coleman, 1990).\n신뢰는 상호적이다. 신뢰를 위해서는 신뢰자가 신뢰 대상을 신뢰할 뿐만 아니라, 신뢰 대상이 신뢰자를 신뢰해야 한다. 즉 신뢰 관계에서 신뢰자는 신뢰 대상으로, 신뢰 대상은 신뢰자로 바뀔 수 있다.\n신뢰의 기능으로는 거래 비용(transaction cost)을 감소시키고 조직 통합을 강화한다는 점을 들 수 있다(Gambetta, 1988; Meyerson et al., 1996; Shapiro et al., 1992). 사실 완전한 확신을 얻기란 쉽지 않다. 때문에 신뢰는 불확실성 상황에서 협력의 지름길 역할을 한다.\n신뢰는 제도적으로 공급되어야 한다. 신뢰는 오랜 시간 동안 형성되고, 안정과, 해체, 그리고 재부상하는 과정을 거친다(Fukuyama, 1995; Miles et al., 1995). 신뢰는 훼손되기 쉬우며 때문에 과소 공급되는 공공재와 같다. 신뢰는 법적 통제에 의존하지는 않는다. 신뢰를 지키지 않았을 때 처벌하는 식의 법적 통제는 오히려 신뢰를 훼손시킬 수도 있다(Sitkin & Bies, 1993). 그러나 신뢰를 뒷받침할만한 제도적 장치는 신뢰 증진에 도움이 된다(Nooteboom, Berger, & Noorderhaven, 1997).\n\n\n5.2.2 신뢰성과 신뢰도\n신뢰 아래의 긍정적 기대는 다양하다. 우선 상대가 자신에게 이로운 행동을 할 것이라는 호의(benevolence)에 대한 기대가 있다. 신뢰 대상이 사회적, 도덕적 원칙을 지킬 것으로 믿는 진실성(integrity)에 대한 기대도 있다. 신뢰 대상이 기대하는 바를 실행할 수 있으리라는 능력(competence)에 대한 기대도 포함된다(김길수, 2020; Mayer et al., 1995). 호의를 갖고 있지만 진실성이 없다면 그것은 정파성을 띄게 된다. 상대방이 호의와 진실성을 갖고 있다고 하더라도 이를 실현할 능력이 없다면 신뢰할만한 대상이 되지 않는다.\n신뢰와 관련된 개념으로 신뢰도(credibility)와 신뢰성(trustworthiness)이라는 용어가 사용된다. 이러한 용어들이 혼용되는 경향이 있지만, 대체로 신뢰도는 신뢰자의 속성, 신뢰성은 신뢰 대상의 속성을 일컫는 말로 구분할 필요가 있다. 신뢰도는 신뢰자가 신뢰 대상을 얼마나 신뢰하는지를 나타내는 심리적 수준을 의미한다. 이는 신뢰의 개인심리적 측면으로 볼 수 있다. 신뢰성은 신뢰 대상의 의도와 행동, 그리고 능력과 관련된다. 즉 신뢰성은 신뢰의 사회제도적 측면에 더 초점을 두고 있다.\n예컨대 언론 신뢰는 “언론사가 만족스러운 방식으로 기능을 수행할 것이라는 기대를 바탕으로 뉴스 콘텐츠를 기꺼이 받아들이려는 수용자의 의향”이라고 정의할 수 있다(Hanitzsch et al., 2018). 여기서 수용자의 의향은 언론 신뢰의 개인심리적 측면, 즉 신뢰자의 개인심리적 신뢰도를 의미한다. 언론사의 기능 수행은 언론 신뢰의 사회제도적 측면, 즉 신뢰 대상으로서 언론사라는 제도의 신뢰성을 뜻한다. 신뢰도는 수용자의 인지적 반응과 정서적 반응을 포괄한다. 각각에 대응하여 신뢰성은 사실성과 공정성을 모두 충족해야 한다. 즉 신뢰성 문제는 언뜻 보면 기능에 치우친 것처럼 보일 수 있으나 실은 가치 문제를 포함한다.\n\n\n5.2.3 인간 신뢰와 기계 신뢰\n전통적으로 신뢰는 개인 수준이든 집단 수준이든 인간에 대한 신뢰(trust in people)의 측면에서 다뤄졌다. 그러나 현대 사회에서는 기술을 신뢰 대상으로 하는 기술 신뢰(trust in technology)의 중요성이 커지고 있다. 특히 디지털 시대에 기술이 인간의 역할을 점점 더 많이 대체하면서 기술 신뢰에 대한 연구가 늘고 있다(김길수, 2020).\n기술 신뢰는 인간 신뢰와 다른 점이 있다. 우선 기술 자체는 의도를 갖고 있지 않다. 때문에 기술은 신뢰 대상이 아니라든가, 기술 신뢰를 다룰 때 일종의 능력인 성능을 중시하는 경향이 있었다. 그러나 기술도 인간과 마찬가지로 위험 요소를 갖고 있다. 즉 기술 역시 신뢰 대상으로 간주할 수 있다(김길수, 2020).\n전통적으로 사회과학에서 기술 신뢰는 기술을 활용하는 개인이나 조직을 대상으로 했으나 최근에는 서비스나 기술 자체에 대한 신뢰 문제로 확대되는 추세다(Jarvenpaa & Leidner, 1999; Mcknight et al., 2011; McKnight et al., 2002). 다른 한편 공학적으로 기술 신뢰는 시스템의 성능 문제를 중심으로 다루어졌으나 최근에는 기술의 사회적 영향력을 고려하고 기술의 사회적 구성을 고민하는 방향으로 발전하고 있다. 종합하면, 기계 신뢰는 기계 자체에 대한 신뢰성을 바탕으로 인간 신뢰를 부분적으로 통합하는 형태를 띄고 있다.\n\n\n5.2.4 신뢰할 수 있는 인공지능의 개념\n신뢰의 개념에 비추어 볼 때, TAI가란 신뢰성을 갖춘 AI, 그리고 이를 통해 인간이 신뢰도를 갖는 AI으로 볼 수 있다. 신뢰성을 갖춘 AI는 기본적으로 목표 과업(task)을 만족스러울만한 성능으로 수행해야 한다. 예컨대 객체 탐지를 위한 모델은 개와 고양이를, 행동 인식을 위한 모델은 걷기와 달리기를, 상황 이해를 위한 모델은 화재나 교통 사고를 빠르고 정확하게 파악해야 한다. 프롬프트를 입력하면 이미지를 만드는 멀티모달 AI(multimodal AI) 모델은 사용자가 “불 속에서 달리는 고양이”를 그리라고 했을 때 사용자 의도에 부합하는 영상을 생성해야 한다. 더 나아가 신뢰성을 가진 AI는 AI 기술과 서비스 자체의 신뢰성과 함께, AI를 기획, 개발, 운영하는 인간이나 집단의 신뢰성을 포괄한다. 이러한 신뢰성에는 신뢰자인 사용자에 대한 신뢰성 역시 포함된다. 즉 AI 신뢰성에는 사용자가 AI를 악의적으로 사용하지 않을 것이라는 기대도 포함된다.\nTAI는 기계 신뢰에 속한다. 우선 AI를 기획, 개발, 운영하는 인간이나 집단의 신뢰성을 생각해볼 수 있다. 이들은 의도를 가질 수 있다. 신뢰자는 신뢰 대상인 인간 기획자, 개발자, 운영자의 선의를 기대하는 방식으로 신뢰도를 갖는다. 다음으로 AI의 신뢰성을 살펴보자. AI는 실제로는 의도를 갖고 있지 않다. 즉 불확실성이 없다. 때문에 AI 자체는 엄밀한 의미에서 신뢰 대상이 될 수 없다고 간주할지 모른다. 그러나 AI는 두 가지 측면에서 의도를 고려해야 한다. 우선 AI는 종종 의도를 가진 것처럼 느껴진다. AI는 기계 중에서도 가장 자율적으로 실행된다. 인간이 개입하도록 따로 설계하지 않는 한, 즉 인간이 루프 속에 들어가고(human in the loop), 인간이 최종 결정하는 식의 인간 중심적으로(human-centered) 설계되지 않는 한 AI는 매우 높은 수준 자동으로 의사 결정을 할 수 있도록 만들어진 자율적이고 지능적인 자동장치(automata)이자 에이전트(agent)이다. 특히 딥러닝을 포함하는 기계학습 방식의 AI는 학습을 통해 입력 값과 출력 값의 쌍으로 이루어진 데이터세트를 바탕으로 의사 결정에 필요한 함수를 스스로 찾는다. 그리고 스스로 찾은 함수를 이용해 입력 값에 대한 출력 값을 예측, 분류, 생성한다.\n또 하나는 AI는 설사 그것을 기획, 개발, 운영, 사용하는 인간이 선한 의도를 갖고 있다고 하더라도 실제로는 해악이 되는 결과를 만들어 낼 수 있다. 이는 우선 AI의 성능 문제일 수 있다. 즉 인간의 선의를 AI가 제대로 구현하지 못할 수 있다. 예컨대 기계 번역기가 영어를 한국어로 제대로 번역하지 못할 수 있다. 이는 성능 개선을 통해 해결할 수 있다. 더 중요한 문제는 AI가 창발적 해악(emergent harm)을 초래할 수도 있다는 점이다(박도현, 2021). AI는 기본적으로 학습데이터 자체가 아니라 학습데이터의 극히 유한한 질서, 즉 특징(feature)을 무한한 가능성을 가진 벡터 공간(vector space)에 임베딩(embedding) 내지 인코딩(encoding)하고, 이를 바탕으로 새롭지만 극히 유한한 상황에 대해 예측하는 식으로 판별(discriminative model) 또는 생성(generative model)한다. 이 과정에서 AI는 학습데이터 자체와는 다른 새로운 상황을 판별하고 생성할 수 있다. 이를 창발로 부를 수 있다. 이러한 창발은 의도에 부합할 수도 있고 그렇지 않을 수도 있다. 즉 인간이 선의를 갖고 AI를 기획, 개발, 운영하고, 사용자도 선의를 갖고 AI를 사용한다고 하더라도, AI가 해악을 산출할 수도 있다.\n정리하면 AI는 AI과 관련된 인간의 불확실성, AI 성능의 불확실성, AI의 창발적 해악에 대한 불확실성을 갖는다. 따라서 TAI는 AI과 관련된 인간과 조직에 대한 신뢰성, AI의 성능 측면의 신뢰성, 그리고 AI가 인과적 해악(causal harm)은 물론 창발적 해악도 산출하지 않을 것이라는 신뢰성을 갖춰야 한다. 이러한 신뢰성을 바탕으로 AI에 대한 신뢰도가 높아지면 AI 신뢰 역시 높아지게 된다."
  },
  {
    "objectID": "tai.html#신뢰할-수-있는-인공지능의-전개",
    "href": "tai.html#신뢰할-수-있는-인공지능의-전개",
    "title": "5  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "5.3 신뢰할 수 있는 인공지능의 전개",
    "text": "5.3 신뢰할 수 있는 인공지능의 전개\nAI의 사회적 영향력에 대한 논의는 AI 윤리, 설명가능한 인공지능 측면에서 진행되다가, TAI의 논의로 종합되는 추세이다. 이 절에서는 TAI 논의의 전개를 TAI 관련 주요 가이드라인인 EC의 신뢰할 수 있는 인공지능 윤리 가이드라인(Ethics guidelines for trustworthy AI), OECD의 AI 권고안(Recommendation of the Council on Artificial Intelligence), 그리고 한국정보통신기술협회(Telecommunications Technology Association, TTA)의 &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;를 중심으로 살펴보도록 한다.\n\n5.3.1 EC의 신뢰할 수 있는 인공지능 논의\n2019년 4월 EC의 &lt;신뢰할 수 있는 인공지능 윤리 가이드라인&gt;은 TAI의 논의가 본격화된 시발점이라 할 수 있다. 이 가이드라인에서는 TAI를 1) 합법적이고(lawful), 2) 윤리적이며(ethical), 3) 강건한(robust) AI로 규정한다. 합법성은 모든 법률과 규정을 따르는 것이다. 윤리성은 윤리 원칙과 가치를 존중하는 것이다. 강건함은 기술적인 측면과 사회적 측면을 모두 고려한다. 흔히 강건함은 AI가 학습데이터가 아닌 다양한 실제 상황(in the wild)에서도 객체 탐지나 행동 인식과 같은 목표 과업을 높은 성능으로 수행할 수 있음을 의미한다. 그러나 TAI에서 강건함은 창발적 해악까지 예방한다는 의미로 해석할 수 있다.\nTAI를 실현하기 위한 핵심 요구사항(key requirements)으로는 1) 인간 기관의 관리 감독(human agency and oversight), 2) 기술적 견고성 및 안전성(Technical Robustness and safety), 3) 개인 정보 보호 및 데이터 거버넌스(privacy and data governance), 4) 투명성(transparency), 5) 다양성, 비차별성 및 공정성(diversity, non-discrimination and fairness), 6) 사회적, 환경적 복지(societal and environmental well-being), 7) 책무성(accountability) 등 일곱 가지를 제시했다.\n1)은 AI 시스템이 루프 속 인간을 통해 인간의 감독을 받아 인간 기본권 증진과 의사 결정에 도움을 주도록 작동해야 한다는 것을 의미한다. 2)는 AI가 문제 발생시 항상 대응할 수 있도록 유연하게 설계되어 의도치 않은 해악까지도 방지할 수 있어야 한다는 것을 뜻한다. 3)은 개인정보 보호는 물론 데이터에 대한 합법적 접근과 관리를 위한 데이터 거버넌스를 구축해야 한다는 것을 말한다. 4)는 설명가능한 AI에 대한 요구사항이다. AI와 관련된 데이터, 시스템, 비즈니스 모델이 투명하고 추적 가능해야 한다. 또한 이해 당사자에게 AI의 기능과 한계를 적절하게 설명해야 한다. 5)는 취약 계층에 대한 부정적 영향을 제거하는 것을 의미한다. 또한 장애와 무관하게 AI에 접근가능해야(accessible) 한다. 6)은 지속가능성(sustainability) 측면의 복지 증진과 관련된다. 즉 AI가 현 인류는 물론 미래 세대에게 혜택을 줄 수 있어야 한다. 또한 다른 생명체를 포함한 환경의 영향을 고려해야 한다. 7)은 AI 시스템의 알고리듬, 데이터, 설계, 그리고 그 결과에 대한 감사가능성(auditability), 책임 요구에 대한 응답가능성(responsibility), 그리고 적절한 보정 가능성이 보장되어야 한다는 것을 의미한다.\nEC는 가이드라인을 바탕으로 2020년 7월 &lt;신뢰할 수 있는 인공지능 자체 평가 목록&gt;(Assessment List for Trustworthy Artificial Intelligence, ALTAI)를 내놓았다(EC, 2020). 이어 이러한 노력은 세계 최초의 AI 규제 법안 인공지능법(Artificial intelligence Act)으로 결실을 맺는다. 인공지능법의 초안은 2021년 4월 발의됐다(손영화, 2021). 이후 EU는 수정을 거쳐 2023년 12월 법안에 합의했다(김진희, 2023.12.10). 해당 법안은 테러나 범죄 예방, 법 집행, 국가 안보 등 일부 분야를 제외한 안면인식과 대규모 언어 모델(large language model, LLM) 사용을 엄격히 규제하고 있다. 또한 자율 주행이나 의료 등 고위험 기술에 대해서는 데이터 공개와 엄격한 테스트를 강제했다. 이를 위반할 경우 최대 3500만 유로, 또는 전 세계 매출의 7%에 해당하는 벌금을 부과하는 강제 수단도 포함됐다.\n\n\n5.3.2 OECD의 신뢰할 수 있는 인공지능 논의\nEU의 적극적인 움직임은 국제적으로 TAI 논의 확산을 가속화했다. EC 가이드라인이 나온지 한 달 뒤인 2019년 5월 경제협력개발기구(Organisation for Economic Co-operation and Development, OECD)는 AI 권고안(Recommendation of the Council on Artificial Intelligence)을 발표했다(OECD, 2019). 권고안에 따르면 TAI란 AI 시스템의 기획, 개발, 구축, 운영 등 전 단계에서 신뢰 가능한 AI의 원칙들이 실현된 AI 시스템을 의미한다. 이는 AI 생태계 전반에 걸쳐 작동해야 한다. 이를 위해 OECD는 TAI를 위한 5개 원칙을 제시했다.\n첫째, 포용 성장, 지속가능 발전과 복지 증진(inclusive growth, sustainable development and well-being)이다. AI는 인간의 능력, 창의력, 소수집단에 대한 포용력을 증진시키고, 사회적 불평등을 해소하며, 환경을 보호하는데 사용되도록 노력해야 한다.\n둘째, 인간 중심적 가치와 공정성(human-centered values and fairness)이다. AI는 인권, 자유, 민주적 가치, 인간의 존엄성, 자율성, 노동권, 평등, 다양성, 공정성, 사회 정의, 개인정보 보호 등을 개선하는데 사용되어야 한다는 것을 의미한다.\n셋째, 투명성과 설명가능성(transparency and explainability)이다. EC 가이드라인과 마찬가지로, AI 행위자(actors)는 사용자나 고객 등 AI 이해관계자에게 AI 시스템의 개발, 운영 등에 대해 정보를 투명하게 제공해야 하며, 그 의사결정과 핵심 내용을 이해하기 쉽게 설명해야 한다는 것을 뜻한다.\n넷째, 강건함, 보안, 안전(robustness, security and safety)이다. AI 시스템이 이러한 요건을 갖추기 위해 AI의 전체적인 사용 주기 전반에서 위험이 지속적으로 모니터링되고 추적 가능해야 한다. 위험이 발견됐을 때는 이를 분석하고 대응할 수 있도록 만들어져야 한다.\n다섯째, 책임성(accountability)이다. AI 행위자는 AI 시스템의 신뢰성을 구현할 수 있도록 행동 강령이나 안내서를 명시하고, 관련 문서를 공개하며, 필요한 감사를 받음으로써 책임성을 증명할 수 있어야 한다.\n이 밖에도 권고안에는 TAI에 대한 국가 정책 및 국제 협력에 대한 제안도 포함됐다. 여기에는 TAI 연구에 대한 공공 투자와 민간 투자 장려, 규제 프레임워크와 평가 메커니즘의 개발, 노동 시장 변화에 대응하기 위한 사회적 협의와 교육 지원, 개발도상국에 대한 지원과 국제 표준 마련을 포함한 국제 공조 등을 제안한다. 이에 따라 미국, 일본, 한국, 싱가포르, 호주 등 각국은 TAI를 위한 정책을 발표했다(The White House Office of Science and Technology Policy, 2020). 국제표준화기구(International Organization for Standardization)와 국제전기기술위위원회(International Electrotechnical Commission, IEC), 즉 ISO/IEC의 AI 위원회 JTC1/SC42 산하 그룹 중 신뢰성 작업 그룹(Working Group 3, WG3) 등은 기술적 관점에서 TAI 표준화 작업을 진행하고 있다(곽준호, 2022).\n\n\n5.3.3 한국의 신뢰할 수 있는 인공지능 논의\n한국은 2020년 11월 과학기술정보통신부가 &lt;국가 인공지능 윤리 기준&gt;을 발표했다(과학기술정보통신부, 2020). 우선 3대 기본 원칙으로 1) 인간 존엄성 원칙, 2) 사회의 공공선 원칙, 3) 기술의 합목적성 원칙을 꼽았다. 1)은 인간이 AI와 교환 불가능한 가치를 가지므로 인간에 해가 되지 않도록 개발되어야 한다는 것을 의미한다. 2)는 AI가 사회적 약자와 취약 계층의 접근성을 보장하고, 인류 보편적 복지를 향상하도록 개발되어야 한다는 것을 뜻한다. 3)은 AI가 인류의 삶과 번영에 필요한 도구라는 목적에 맞게 윤리적으로 개발되어야 한다는 것을 말한다.\n3대 원칙을 실행하는 10대 요건으로는 1) 인권보장, 2) 프라이버시 보호, 3) 다양성 존중, 4) 침해 금지, 5) 공공성, 6) 연대성, 7) 데이터 관리, 8) 책임성, 9) 안전성, 10) 투명성이 포함됐다. 각각의 내용은 앞서 설명한 EC의 가이드라인이나 OECD 권고안과 일맥상통한다.\n2021년에는 4차산업혁명위원회가 &lt;사람이 중심이 되는 AI를 위한 신뢰할 수 있는 인공지능 실현 전략(안)&gt;을 내놓았다(4차산업혁명위원회, 2021). 그 내용으로는 1) 신뢰 구현을 위한 법‧제도, 윤리적, 기술적 요구사항을 종합한 AI 개발 가이드북 제작과 보급, 2) AI 제품이나 서비스에 대한 민간 자율의 인증제 도입 및 지원, AI 일괄 지원 플랫폼 운영, 3) 설명 가능성‧공정성‧견고성 등을 향상시키기 위한 신뢰성 원천기술 개발 추진, 4) 학습용 데이터의 신뢰성 확보를 위한 표준 기준 제시 및 데이터 개방, 5) 고위험 AI에 대한 국민 안전‧신뢰성 향상 방안 연구, 6) AI 윤리 기준 실천을 위한 윤리 교육 강화 및 개발자‧이용자용 체크리스트 보급, 윤리‧신뢰성 향상을 위한 공론의 윤리 정책 플랫폼 운영 등이 있다.\n이어 2023년 7월 TTA는 EC의 ALTAI와 유사한 &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;를 내놓았다(한국정보통신기술협회, 2023). 기존 가이드라인에 비해, TTA의 안내서는 기술적으로 구체화된 TAI 개발 및 검증 방법론을 제시한다. 안내서는 전 분야에 해당하는 일반 분야와 함께 공공·사회 분야, 의료 분야, 자율주행 분야 등 세부 분야별 안내서를 제시했다.\n안내서에는 TAI의 설계 요소를 AI 신뢰성 프레임워크로 제시한다. 해당 프레임워크는 크게 세 가지로 구성된다. 첫째, 신뢰성 확보 대상이다. AI 데이터, AI 모델과 알고리즘, AI 시스템, 사람-AI 인터페이스가 해당된다. 둘째, 생애주기별 요구사항 분류이다. AI 생애주기란 AI 시스템을 구현하고 AI 서비스를 운영하는 과정을 뜻한다. AI 생애주기는 계획 및 설계, 데이터 수집 및 처리, AI 모델 개발, 시스템 구현, 운영 및 모니터링 등 5단계로 나뉜다. 셋째, AI 윤리 기준 준용이다.\n신뢰성 요건으로 AI 윤리 기준의 10대 요건 중 기술적 적용 가능한 4개 요건을 선별했다. 4개 요건은 다양성 존중, 책임성, 안전성, 투명성이다. AI 신뢰성 프레임워크를 정리하면 &lt;그림 1&gt;과 같다.\n\n\n\n그림 1. 인공지능 신뢰성 프레임워크(출처: 한국정보통신기술협회, 2023)\n\n\n또한 신뢰성 요건별 세부 속성 및 키워드는 &lt;표 1&gt;과 같다(한국정보통신기술협회, 2023). 그리고 신뢰성 요건을 충족시키기 위한 요구사항을 생애주기별로 15개 제시했다. AI 생애주기별 요구사항과 적용되는 신뢰성 요건 및 각 요구사항은 &lt;표 2&gt;와 같이 정리할 수 있다(한국정보통신기술협회, 2023).\n\n표 1. 신뢰성 요건별 정의와 관련 속성 및 관련 키워드(출처: 한국정보통신기술협회, 2023 일부 수정).\n\n\n\n\n\n\n\n\n신뢰성 요건\n정의\n관련 속성\n관련 키워드\n\n\n\n\n다양성 존중\nAI가 특정 개인이나 그룹에 대한 차별적이고 편향된 관행을 학습하거나 결과를 출력하지 않으며, 인종･성별･연령 등과 같은 특성과 관계없이 모든 사람이 평등하게 AI 기술의 혜택을 받을 수 있는 것\n공정성(fairness), 정당성(justice)\n편향(bias), 차별(discrimination), 편견(prejudice), 다양성(diversity), 평등(equality)\n\n\n책임성\nAI가 생명주기 전반에 걸쳐 추론 결과에 대한 책임을 보장하기 위한 메커니즘이 마련되어 있는 것\n책임성(responsibility), 감사가능성(auditability), 답변가능성(answerability)\n책임(liability)\n\n\n안전성\nAI가 인간의 생명･건강･재산 또는 환경을 해치지 않으며, 공격 및 보안 위협 등 다양한 위험에 대한 관리 대책이 마련되어 있는 것\n보안성(security), 견고성(robustness), 성능보장성(reliability), 통제가능성(controllability)\n적대적 공격 (adversarial attack), 복원력(resilience), 프라이버시(privacy)\n\n\n투명성\nAI가 추론한 결과를 인간이 이해하고 추적할 수 있으며, AI가 추론한 결과임을 알 수 있는 것\n설명가능성(explainability), 이해가능성(understandability), 추적가능성(traceability), 해석가능성(interpretability)\nXAI(eXplainable AI), 이해도(comprehensibility)\n\n\n\n표 2. 신뢰성 요건 충족을 위한 AI 생애주기별 요구사항 생애주기 요구사항 신뢰성 요건 다양성 책임성 안전성 투명성 1. 계획 및 설계 1. AI 시스템에 대한 위험관리 계획 및 수행 　 O 　 O 2. AI 거버넌스 체계 구성 O O O O 3. AI 시스템의 신뢰성 테스트 계획 수립 　 　 O O 2. 데이터 수집 및 처리 3. 데이터의 활용을 위한 상세 정보 제공 　 O 　 O 4. 데이터 강건성 확보를 위한 이상(Abnormal) 데이터 점검 　 　 O 　 5. 수집 및 가공된 학습 데이터의 편향 제거 O O 　 O 3. AI 모델 개발 6. 오픈소스 라이브러리의 보안성 및 호환성 확보 　 O O 　 7. AI 모델의 편향 제거 O 　 　 　 8. AI 모델 공격에 대한 방어 대책 수립 　 　 O 　 9. AI 모델 명세 및 출력 결과에 대한 설명 제공 　 O 　 O 4. 시스템 구현 11. AI 시스템 구현 시 발생 가능한 편향 제거 O 　 　 　 12. AI 시스템의 안전 모드 구현 　 O O O 13. AI 시스템의 설명에 대한 사용자의 이해도 제고 　 　 　 O 5. 운영 및 모니터링 14. AI 시스템의 추적가능성 확보 　 　 O O 15. 서비스 제공 범위 및 상호작용 대상에 대한 설명 제공 　 O 　 O 출처: 한국정보통신기술협회(2023), 일부 수정.\n요구사항은 다시 대분류 34개, 소분류 67개의 2단계의 체크리스트로 세분화했다. &lt;표 3&gt;은 생애주기 중 AI 모델 개발 단계의 요구사항 및 대분류 수준의 체크리스트의 예이다.\n소분류 수준의 체크리스트는 매우 구체적인 기술적 방안을 담고 있다. 예컨대 요구사항 9의 ’AI 모델 공격에 대한 방어 대책 수립’과 같은 내용은 일종의 AI 시스템의 모델을 훔쳐가는 모델 추출 공격(model extraction attack)에 대한 질의(query) 횟수 제한과 같은 기술적 방어 기법이나, AI 시스템에 적용된 모델을 속이는 모델 회피 공격(model evasion attack)에 대한 적대적 훈련(adversarial training)과 같은 방어 기법의 적용 여부를 따진다.\n표 3 신뢰할 수 있는 인공지능 모델 개발 단계의 요구사항과 체크리스트\n출처: 한국정보통신기술협회(2023)"
  },
  {
    "objectID": "tai.html#저널리즘-분야에서-신뢰할-수-있는-인공지능의-함의",
    "href": "tai.html#저널리즘-분야에서-신뢰할-수-있는-인공지능의-함의",
    "title": "5  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "5.4 저널리즘 분야에서 신뢰할 수 있는 인공지능의 함의",
    "text": "5.4 저널리즘 분야에서 신뢰할 수 있는 인공지능의 함의\n사회 전반의 AI 전환(AIX) 추세에 따라 저널리즘 분야에서도 AI 활용 가능성이 높아지는 추세다(한국언론진흥재단, 2020; 이현우ˑ이성민ˑ이상규, 2023; Beckett, 2019; Diakopoulos, 2019; Jones et al., 2022). 저널리즘에 활용되는 AI를 저널리즘 AI라고 부를 수 있다(박대민, 2023; Beckett, 2019). 이 절에서는 저널리즘 AI에서 TAI의 함의를 살펴본다.\n사실 TAI 관련 논의에서 저널리즘은 물론 미디어에 대한 관심은 높지 않다. EU의 AI 정책 문서에서 미디어에 대한 언급은 많지 않고 저널리즘에 대한 언급은 더욱 적다. 언론사 역시 AI 활용에 대한 높은 관심에도 불구하고 자사의 윤리 강령에 AI 활용 가이드라인을 포함시키는 사례는 많지 않다(Porlezza, 2023). 그러나 탈진실 사회(post truth society)로도 불리는 현재에, 딥페이크, 가짜 뉴스, 편향, 반향실 효과 등 AI가 촉발할 것으로 예상되는 미디어와 저널리즘 분야의 신뢰성 위기는 심각한 수준이다(박대민, 2023; 박대민, 2022; Edelman, 2023; Newman et al., 2023).\n저널리즘 AI의 신뢰성은 크게 세 가지 측면에서 논의될 필요가 있다. 첫째, 저널리즘 AI는 AI 기술이므로 TAI로 설계되어야 한다. 둘째, 저널리즘 AI의 신뢰성은 AI를 사용하는 인간과 조직의 신뢰성과도 연계된다. 즉 언론사를 비롯한 미디어의 신뢰성이 저널리즘 AI의 신뢰성에 영향을 준다. 셋째, 저널리즘 AI의 신뢰성은 결국 사용자의 신뢰도를 높이는 방향으로 사용되어야 한다.\n사실 TAI의 논의에서는 첫번째 사항만 고려되는 경향이 강하다. TTA의 안내서를 저널리즘과 같이 특정 영역에 구체적으로 적용할 때 크게 두 가지 측면에서 한계가 보인다(박대민, 2023). 첫째, 해당 안내서가 AI 생애주기 중심으로 작성되어, 저널리즘 분야의 기사 생애주기와 맞지 않다. 즉 저널리즘 AI에서 TAI를 구현하려면, AI 생애주기를 기사 생애주기 관점에서 통합해야 한다. 둘째, TTA의 신뢰성을 구현에서 언론인과 같은 도메인 전문가의 역할이 과소평가되어 있다. 이는 비록 TTA가 AI 생애주기 전체를 고려한 요구사항을 만들었지만, 실제로는 기획과 개발 중심이고 운영을 과소평가했기 때문이다. 즉 기획과 개발에 생애주기 5단계 중 4단계를, 15개 요구사항 중 13개를 할당하고 서비스 운영에는 생애주기 1단계와 요구사항 2개만 할당한 것에서도 나타난다. 그러나 AI를 활용하는 각 영역 입장, 저널리즘 AI를 활용하는 미디어의 입장에서는 운영이 거의 전부나 다름없다. 물론 저널리즘 AI에 TAI를 적용할 때 운영만 고려하라는 것은 아니다. 핵심은 저널리즘 AI를 TAI로 구현할 때 언론인과 같은 영역 전문가의 역할이 훨씬 더 강화되어야 한다는 것이다. 기획, 개발, 운영 전반에 루프 속 인간으로서 언론인이 개입할 수 있도록 해야 한다(박대민, 2023; Broussard et al., 2019; Gutierrez-Lopez et al. 2019; Wu et al., 2022).\n한편 저널리즘 AI의 신뢰성이 언론사의 신뢰성의 영향을 받는다면, 저널리즘 AI의 신뢰성은 저널리즘의 신뢰성을 제고하고 사용자의 신뢰도를 높이는 방식으로 활용되어야 한다. 다행히 저널리즘의 가치 지향과 TAI의 가치 지향은 어느 정도 통약 가능성(commensurability)을 갖고 있다. TAI 투명성과 안전성은 저널리즘 사실성의, TAI의 다양성과 책임성은 저널리즘 공정성의 전제 조건이다. 즉 탈진실 사회에서 TAI 기반 저널리즘 AI를 활용함으로써 저널리즘 AI의 투명성과 안전성을 높여서 저널리즘의 사실성을 개선할 수 있다. 또한 TAI 기반 저널리즘 AI를 통해 저널리즘 AI의 다양성과 책임성을 높임으로써 저널리즘의 공정성을 향상시킬 수 있다(박대민, 2023).\n이것은 AI를 쓰면 저널리즘의 사실성과 공정성이 제고된다는 기술결정론적인 입장이 아니다. 반대로 AI라는 기술을 사실성과 공정성 제고를 위해 사용할 수 있도록 구성해야 한다는 구성주의적 입장에 가깝다. 그리고 그 구성 전략으로서 TAI의 논의를 참고할 수 있다는 것이다. 뿐만 아니라 JAI를 TAI로 구성하는 과정에서 TAI의 방법론을 더욱 정교화하고 합목적적으로 사용할 수 있다는 것을 의미한다.\n정리하면 TAI 기반 JAI는 다음과 같은 세 조건을 충족해야 한다. 첫째, 저널리즘 분야에서 AIX가 진행되어야 한다. 즉 저널리즘과 AI의 실천적 결합이 전면적으로 이뤄져야 한다. 둘째, JAI에 TAI를 적용되어야 한다. 저널리즘의 AI가 TAI 관점에서 신뢰성을 확보해야 한다. 셋째, JAI가 언론 신뢰 개선에 기여해야 한다(박대민, 2023).\n이를 TAI를 다른 사회 영역에 적용할 때로 일반화할 수도 있다. 첫 번째는 AIX 조건이다. AI를 도메인에 적용하는 것을 의미한다. 둘째, TAI 조건이다. AIX에 TAI를 적용하는 것이다. 셋째, 도메인 조건이다. AI가 해당 도메인의 신뢰를 제고할 수 있어야 한다(박대민, 2023)."
  },
  {
    "objectID": "tai.html#참고문헌",
    "href": "tai.html#참고문헌",
    "title": "5  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "5.5 참고문헌",
    "text": "5.5 참고문헌\n고학수·김용대·윤성로·김정훈·이선구·박도현·김시원 (2021). &lt;인공지능 원론&gt;. 서울: 박영사.\n과학기술정보통신부 (2021). &lt;신뢰할 수 있는 인공지능 실현 전략&gt;. 과학기술정보통신부.\n곽준호 (2022). 데이터의 품질과 인공 지능 시스템의 신뢰성. . 201권. 59-63.\n김길수 (2020). 인공지능의 신뢰에 관한 연구. &lt;한국자치행정학보&gt;, 34권 3호, 21-41.\n김진희(2023.12.10). EU, 37시간 진통 끝에 세계 최초 ‘AI 규제법’ 합의…주요 내용은? &lt;헬로T&gt;. Retrieved from https://www.hellot.net/news/article.html?no=84839\n박대민 (2023). 통과하면 사실로 인정되는 AI를 만들 수 있는가: 설명가능한 AI를 통한 사실성 제도로서 언론의 재구성. &lt;언론과사회&gt;. 31권 2호. 139-181.\n박대민 (2022). 미디어 인공지능: 컴퓨터 비전 분야 딥러닝 모델의 미디어 동영상 적용 가능성에 관한 연구. &lt;커뮤니케이션이론&gt;. 18권 1호, 111-154.\n박도현. (2021). &lt;인공지능과 해악&gt;. 서울대학교 법학대학원 박사학위논문. 4차산업혁명위원회 (2021). &lt;사람이 중심이 되는 AI를 위한 신뢰할 수 있는 인공지능 실현 전략(안)&gt;. 4차산업혁명위원회.\n손영화 (2023). AI 공정성에 관한 연구: 차별 없는 AI 사회의 실현. &lt;한양법학&gt;, 34권 3호. 275-304.\n손영화 (2021). EU AI 규칙안에 대한 일고찰. &lt;IP & Data 法&gt;, 1권 2호. 27-52.\n신예진 (2022). 신뢰할 수 있는 인공지능 개발 안내서. . 201권. 21-28.\n오로라(2024.1.4.). 美 최예진 교수 “안중근을 ’테러리스트’라는 AI, 韓 피해 상상 힘들어”. &lt;조선일보&gt;. Retrieved from https://www.chosun.com/economy/tech_it/2024/01/02/734OD6WEIZERFI3OARG55LQZHI/\n이현우·이성민·이상규 (2023). &lt;언론산업 인공지능(AI) 활용방안 연구&gt;. 서울: 한국언론진흥재단.\n한국언론진흥재단 (2020). &lt;2020 뉴스미디어의 신뢰·혁신·소통&gt;. 서울: 한국언론진흥재단.\n한국정보통신기술협회 (2023). &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;. 성남: 한국정보통신기술협회.\n한상기 (2021). &lt;신뢰할 수 있는 인공지능&gt;. 서울: 클라우드나인.\nBeckett, C. (2019). New powers, new responsibilities: A global survey of journalism and artificial intelligence. London School of Economics & Political Science.\nBeutel, G., Geerits, E., & Kielstein, J. T. (2023). Artificial hallucination: GPT on LSD?. Critical Care, 27(1), 148.\nBroussard, M., Diakopoulos, N., Guzman, A. L., Abebe, R., Dupagne, M., & Chuan, C. H. (2019). Artificial intelligence and journalism. Journalism & Mass Communication Quarterly, 96(3), 673–695.\nColeman, J. S. (1990). Foundations of social theory. Cambridge, MA: Belknap Press.\nDiakopoulos, N. (2019). Automating the news: How algorithms are rewriting the media. Harvard University Press.\nDhiman, D. B. (2023). Does Artificial Intelligence help Journalists: A Boon or Bane?. Available at SSRN 4401194.\nEdelman (2023). 2023 Edelman trust barometer (Global Report). Retrieved from https://www.edelman.com/trust/2023/trust-barometer\nEuropean Commission (May, 2019). Ethics guidelines for trustworthy AI. Retrieved from https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nEuropean Commission (July, 2020). Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment. Retrieved from https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment\nFukuyama, F. (1995). Trust: The social virtues and the creation of prosperity. New York: Free Press. Future of Life (2023.3) Pause Giant AI Experiments: An Open Letter. Retrieved from https://futureoflife.org/open-letter/pause-giant-ai-experiments/\nGambetta, D. (1988). Trust: Making and breaking cooperative relations. New York: Basil Blackwell.\nGutierrez-Lopez, M., Missaoui, S., Makri, S., Porlezza, C., Cooper, G., & MacFarlane, A. (2019, February). Journalists as design partners for AI. In Workshop for accurate, impartial and transparent journalism: challenges and solutions. CHI 2019.\nGuzman, A. L. (2018). What is human-machine communication, anyway? In Guzman A. L. (Eds.), Human-machine communication: Rethinking communication, technology, and ourselves (pp. 1-28). New York: Peter Lang.\nHanitzsch, T., Van Dalen, A., & Steindl, N. (2018). Caught in the nexus: A comparative and longitudinal analysis of public trust in the press. The International Journal of Press/politics, 23(1), 3-23.\nJarvenpaa, S. L., & Leidner, D. E. (1999). Communication and trust in global virtual teams. Organization Science, 10(6), 791-815.\nJones, B., Jones, R., & Luger, E. (2022). AI ‘Everywhere and Nowhere’: Addressing the AI Intelligibility Problem in Public Service Journalism. Digital Journalism, 10(10), 1731-1755.\nKrishnan, A. (2016). Killer robots: legality and ethicality of autonomous weapons. Routledge.\nLuhmann, N. (1988). Familiarity, confidence, trust: Problems and alternatives. In D. Gambetta (Ed.), Trust: Making and breaking cooperative relations (pp. 94-107). New York, NY: Blackwell Publishing.\nMayer, R. C., James H., Davis and F. David Schoorman. (1995). An Integrative Model of Organizational Trust. Academy of Management Review, 20(3). 709-734.\nMcknight, D. H., Carter, M., Thatcher, J. B., & Clay, P. F. (2011). Trust in a specific technology: An investigation of its components and measures. ACM Transactions on management information systems (TMIS), 2(2), 1-25.\nMcKnight, D. H., Choudhury, V., & Kacmar, C. (2002). Developing and validating trust measures for e-commerce: An integrative typology. Information systems research, 13(3), 334-359.\nMeyerson, D., Weick, K. E., & Kramer, R. M. (1996). Swift trust and temporary groups. In R. M. Kramer & T. R. Tyler (Eds.), Trust in organizations: Frontiers of theory and research: 166-195. Thousand Oaks, CA: Sage.\nMiles, R. E., & Creed, W. E. D. (1995). Organizational forms and managerial philosophies: A descriptive and analytical review. In B. M. Staw & L. L. Cummings (Eds.), Research in organizational behavior, vol. 17: 333-372. Greenwich, CT: JAI Press.\nNewman, N., Fletcher, R., Kirsten, E., Robertson, C. T., & Nielsen, R. K. (2023). Reuters Institute digital news report 2023. Reuters Institute for the study of Journalism. Retrieved from https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2023\nOECD (May 2019). Recommendation of the Council on OECD Legal Instruments Artificial Intelligence. Retrieved from https://legalinstruments.oecd.org/en/instruments/oecd-legal-0449\nOpdahl, A. L., Tessem, B., Dang-Nguyen, D. T., Motta, E., Setty, V., Throndsen, E., Tverberg, A., & Trattner, C. (2023). Trustworthy journalism through AI. Data & Knowledge Engineering, 146, 102182.\nPorlezza, C. (2023). Promoting responsible AI: A European perspective on the governance of artificial intelligence in media and journalism. Communications, 48(3), 370-394.\nRousseau, D. M., Sitkin S. B., Burt R. S, Camerer C., Not so Different after a Cross-discipline View of Trust. Academy of Management Review, 23(1) 393-404.\nShapiro, D., Sheppard, B. H., & Cheraskin, L. (1992). Business on a handshake. Negotiation Journal, 8: 365-377.\nThe White House Office of Science and Technology Policy (February 2020). American Artificial Intelligence Initiative: Year One Annual Report. Retrieved from https://www.nitrd.gov/nitrdgroups/images/c/c1/American-AI-Initiative-One-Year-Annual-Report.pdf\nWu, X., Xiao, L., Sun, Y., Zhang, J., Ma, T., & He, L. (2022). A survey of human-in-the-loop for machine learning. Future Generation Computer Systems, 135, 364-381.   ## 더 읽을 거리\n\nAI 신뢰성 문제는 다양하게 제기되고 있다. 특히 AI 윤리와 관련된 최근 이슈를 알고 싶다면 다음 뉴스레터를 참조할 수 있다. AI 윤리 레터: https://ai-ethics.stibee.com/\n2023년 12월, 유럽 연합이 세계 최초로 TAI을 핵심으는 하는 인공지능법에 합의했다. 2024년 1월 5일 현재, 구체적인 내용은 아직 공개되지 않았다. 관련 내용은 추후 아래 링크에서 업데이트될 것으로 보인다. 유럽 연합의 인공지능법(AI act): https://artificialintelligenceact.eu/\nTAI 논의는 거대 담론에서 기술 구현을 논의하는 단계로 구체화되는 추세다. 그 예로는 아래의 보고서를 참고할 수 있다. 한국정보통신기술협회 (2023). &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;. 성남: 한국정보통신기술협회.\n저널리즘 분야의 인공지능에서 신뢰할 수 있는 인공지능의 적용 방향성을 모색한 연구로는 다음 논문을 참고할 수 있다. 박대민 (2023). 신뢰할 수 있는 인공지능 기반의 저널리즘 인공지능: 언론 신뢰와 인공지능 신뢰성 간 통약가능성을 바탕으로. &lt;언론과 사회&gt;, 31권 4호, 5-47."
  },
  {
    "objectID": "tai.html#생각해볼-문제",
    "href": "tai.html#생각해볼-문제",
    "title": "5  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "5.6 생각해볼 문제",
    "text": "5.6 생각해볼 문제\n\nAI의 신뢰성 문제를 야기하는 구체적인 사례는 어떤 것이 있는가?\n유럽연합의 인공지능법에서 TAI 관련 주요 쟁점 및 해법은 무엇이었는가? 해당 법안 이후 TAI를 각국 정부와 국내외 기업에서는 어떻게 수용하고 있는가? 그 한계와 대안은 무엇인가?\nTAI를 기술적으로 구현하려는 시도로는 어떤 것이 있는가? 이를 미디어 영역과 같이 구체적인 사회 영역에 적용할 수 있는가? 어떤 사회적, 기술적 기획이 필요한가?"
  },
  {
    "objectID": "fml.html#compas-알고리즘",
    "href": "fml.html#compas-알고리즘",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "6.1 COMPAS 알고리즘",
    "text": "6.1 COMPAS 알고리즘\n수학적으로 정의된 공정성 개념에 쉽게 접근하기 위해서 COMPAS라는 범죄자 프로파일링 소프트웨어의 유명한 사례에 대해 먼저 논의해 보자. COMPAS (Correctional Offender Management Profiling for Alternative Sanction)는 Northpointe(현 Equivant)라는 기업에 의해 개발된 프로그램으로 범죄자의 신상 정보를 이용하여 재범 가능성을 예측하는 기능을 가지고 있다. 이는 미국 뉴욕, 위스콘신, 캘리포니아 등의 지방 법원에서 보석 결정 등을 내리는데 보조 도구로 사용되었다. 미국의 탐사보도언론 프로퍼블리카(Propublica)는 COMPAS에 의해 내려진 결정을 분석한 결과, COMPAS에 의한 자동화된 결정이 인종에 따라 편향되었다고 폭로하였다.\n아래 테이블은 &lt;알고리즘이 지배한다는 착각Outnumbered&gt;이라는 책에서 저자 데이티드 섬프터가 원 데이터를 재구성한 알기 쉬운 예이다.1\n\n\n\n흑인\n고위험\n저위험\n합계\n\n\n\n\n재범○\n1,369\n532\n1,901\n\n\n재범✕\n805\n990\n1,714\n\n\n합계\n2,174\n1,522\n3,615\n\n\n\n\n\n\n백인\n고위험\n저위험\n합계\n\n\n\n\n재범○\n505\n461\n966\n\n\n재범✕\n349\n1,139\n1,488\n\n\n합계\n854\n1,600\n2,454\n\n\n\n위의 표에서 ‘고위험’이 의미하는 바는 COMPAS가 해당 범죄자에 대해 재범 가능성이 높다고 판정한 경우를, ’저위험’은 그 반대의 경우이다. 즉, 각 열(column)은 자동 분류 알고리즘의 예측 결과에 대응한다. 반면, 각 행(row)에 표현된 ’재범○’, ’재범✕’는 실제로 이후에 데이터에 포함된 범죄자들이 재범을 저질렀는지 여부를 의미한다. 즉, 이 테이블은 COMPAS 알고리즘의 재범 가능성에 대한 예측과 사후적으로 밝혀진 실제 재범 여부를 모두 제공하기에, 이를 이용해 알고리즘 예측의 정확성을 평가해볼 수 있는 지도학습 모형에 해당한다.\n프로퍼블리카는 COMPAS의 편향된 예측에 대한 우려를 표명하면서, 몇 가지 통계치를 제시하였다. 위의 데이터에서 가장 먼저 비판할 수 있는 바는, 흑인이 고위험군으로 판정될 확률은 60%(=2174/3615)인 반면, 백인이 고위험군으로 판정될 확률은 34.8%(=854/2454)에 불과하다는 것이다. 물론, 이는 실제 두 인종 간의 재범률 차이에서 기인한 것일 수도 있다. 하지만, 프로퍼블리카는 알고리즘의 정확도가 두 인종 간에 다르게 나타난다는 점에서 또 다른 우려를 표명한다. 예컨대, 흑인의 경우 데이터에 포함된 1,714명이 실제 재범을 저지르지 않았는데도 불구하고, 그 중 805명은 COMPAS가 고위험군으로 분류하여 46.9%%(=805/1714)의 위양성률(false positive rate)을 보인 반면, 백인은 23.5%(=349/1488)의 위양성률을 가지고 있다는 것이다. 즉, 위의 데이터에 따르면, COMPAS 알고리즘은 백인 범죄자에 대해서는 그들에게 불리한 오류를 발생시킬 확률이 더 적다.\n이에 대해 COMPAS를 개발한 Northpointe는 이와 다른 정확도 개념을 이용해 프로퍼블리카의 비판을 반박하였다. 예컨대, 고위험군으로 예측된 2,174명의 흑인 중 1,369명이 실제 다시 범죄를 저질러, 63%(=1,369/2,174)의 정확도를 보였으며, 고위험군으로 예측된 854명의 백인 중 505명이 재범을 저질러, 59%(=505/854)의 정확도를 보였기에, COMPAS알고리즘은 두 인종 집단 사이에서 정확도의 큰 차이를 보이지 않았다는 것이다.\n이와같이 혼란스러운 논란의 바탕에는 사실 공정성이라고 부를만한 다양한 기준이 있으며, 이들이 서로 모순될 가능성이 크다는 매우 근본적인 문제가 존재한다. 이를 좀 더 분명하게 이해하기 위해 이제 약간의 수학적 기호와 정의를 도입해보도록 하자."
  },
  {
    "objectID": "fml.html#공정성의-수학적-정의",
    "href": "fml.html#공정성의-수학적-정의",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "6.2 공정성의 수학적 정의",
    "text": "6.2 공정성의 수학적 정의\nCOMPAS 사례를 통해 살펴본 공정성 개념들을 수학적으로 일반화하기 위해 우리는 몇 가지 확률변수(random variable)2들을 정의할 것이다. 먼저, 우리가 예측하고 싶은 ’진실’을 \\(Y\\)라고 부르자. COMPAS의 예에서 \\(Y\\)는 실제 재범 여부에 해당한다. \\(Y=1\\)라는 확률변수 \\(Y\\)의 실현값은 재범을 실제 저질렀음을, \\(Y=0\\)은 반대의 경우를 의미한다고 하자. COMPAS가 예측을 할 때는 물론 재범 여부를 알 수 없지만, 위의 경우와 같이 사후적으로는 해당 데이터가 수집되어 \\(Y\\)를 알게되는 경우가 있고, 이를 통해 인공지능의 예측 결과의 정확성을 사후적으로 평가해볼 수 있다. \\(Y\\)와 달리, \\(R\\)은 알고리즘의 예측을 표현하는 확률변수라고 하자. 즉, 위의 예에서 COMPAS가 해당 범죄자를 재범 ’고위험군’이라고 평가한 경우에는 \\(R=1\\), ’저위험군’이라고 평가한 경우에는 \\(R=0\\)이라는 확률변수의 실현값으로 표현할 할 수 있을 것이다.\n이제 개개인이 가지고 있는 특성 역시 확률변수로 표현하자. 수학적 공정성의 정의는 먼저 ‘차별’이 발생할 수 있는 ’민감한 개인의 특성’, \\(A\\)와 그렇지 않은 일반적인 개인의 특성 \\(X\\)가 구분된다고 가정한다. COMPAS의 예에서 \\(A\\)는 인종에 해당할 것이다. 기호의 간결함을 위해 \\(A=a\\)는 개인이 흑인인 경우를, \\(A=b\\)는 개인이 백인인 경우를 의미한다고 가정하자. \\(X\\)는 민감속성 이외의 모든 다른 특성이 될 것이다. 지금까지의 정의를 모두 종합하면 아래와 같다.\n\n\\(Y \\in \\{0,1\\}\\): 예측하고자 하는 진실 (e.g. 실제 재범 여부)\n\\(R \\in \\{0,1\\}\\): 모형의 예측 (e.g. 고위험군/저위험군)\n\\(A \\in \\{a,b\\}\\): 민감한 개인의 특성 (e.g. 인종)\n\\(X\\): 민감하지 않은 개인의 특성 (e.g. 거주지역, 직업)\n\n물론 \\(Y\\), \\(R\\), \\(A\\)는 위와 같이 0/1 또는 a/b처럼 이분법적으로 구분될 필요는 없으며, 더 많은 카테고리가 존재하는 경우, 또는 해당 변수들이 연속적으로 변화하는 숫자일 경우로 확장할 수도 있다. 여기서는 설명의 간결함을 위해 위와 같이 단순하게 표현할 수 있는 경우에만 논의를 한정할 것이다.\n위에서 프로퍼블리카 제기했던 비판 중 첫 번째 비판-즉, 흑인을 고위험군으로 예측하는 경우가 백인을 고위험군으로 예측하는 경우보다 더 많다는 지적을 확률로 표현하면 다음과 같다.\n\\[P(R=1|A=a) &gt; P(R=1|A=b)\\]\n여기서 \\(P()\\)는 (괄호 안에 표시할) 어떤 사건의 확률을 타나내는 수학 기호이다. 예컨대 \\(P(R=1)\\)은 (인종을 막론하고) COMPAS가 고위험군이라고 예측할 확률을 의미한다. \\(P( | )\\)라는 조금 더 복잡한 기호는 \\(|\\) 기호 뒤에 표시된 어떤 조건 하에서의 확률, 즉, ’조건부 확률(Conditional Probability)’을 의미한다. 즉, \\(P(R=1|A=a)\\)은 ’인종이 흑인인 조건 하에서(\\(A=a\\)) 고위험군으로 예측(\\(R=1\\))될 확률’을, \\(P(R=1|A=b)\\)는 ’인종이 백인인 조건 하에서(\\(A=b\\)) 고위험군으로 예측(\\(R=1\\))될 확률’을 의미한다. 프로퍼블리카의 비판은 이 두 조건부 확률이 동등해야 ’공정한 알고리즘’이라는 생각을 전재한다. 이를 ’통계적 동등성(statistical parity)’로서의 공정성이라고 한다.\n프로퍼블리카의 COMPAS 알고리즘에 대한 두 번째 비판은 알고리즘의 ‘정확도’가 두 인종 사이에 달랐다는 점이었다. 구체적으로, ’위양성률(False Positive Rate)’, 즉, 실제 재범을 저지르지 않은 흑인들을 고위험군으로 잘못 분류하는 경우가 백인에게 동일한 오류가 발생하는 경우에 비해 지나치게 잦았다는 것이다. 이러한 비판을 조건부 확률의 기호로 표현하자면 다음과 같다.\n\\[P(R=1|Y=0, A=a) &gt; P(R=1|Y=0, A=b)\\] 여기서 \\(P(R=1|Y=0,A=a)\\)는 ‘실재 재범을 일으키지 않았고(\\(Y=0\\)), 흑인이라는 조건(\\(A=a\\)) 하에서’ 고위험군으로 예측(\\(R=1\\))될 확률이, ‘실재 재범을 일으키지 않았고(\\(Y=0\\)), 백인이라는 조건(\\(A=b\\))’ 하에서 고위험군으로 예측(\\(R=1\\))될 확률보다 높다는 것이다.\n반면, Northpointe의 재반박, 즉, 고위험군으로 분류된 사람 중에 실재 재범을 일으킬 확률은 흑은과 백인 사이에 거의 같다는 주장은 어떻게 표현할 수 있을까? 다음의 수식을 살펴보자.\n\\[P(Y=1|R=1, A=a) = P(Y=1|R=1, A=b)\\]\n여기서 \\(P(Y=1|R=1,A=a)\\) 라는 표현과 앞서 프로퍼블리카의 비판에서 보았던 \\(P(R=1|Y=0,A=a)\\)라는 표현을 비교해보자. \\(Y\\)에 해당하는 값이 0이냐, 1이냐, 즉, 재범을 일으키지 않은 경우에 주목하느냐, 재점을 일으킨 경우에 주목하느냐 라는 차이도 있지만, 더 중요한 차이는 \\(P(Y|R,A)\\)라는 표현은 \\(Y\\)가 조건문 앞에, \\(P(R|Y,A)\\)라는 표현은 \\(R\\)이 조건문 앞에 있어, 확률을 표현하고자 하는 사건과 조건 사이의 위치가 뒤바뀌었다는 것이다. 즉, 프로퍼블리카와 COMPAS는 두 개의 자른 조건부 확률에 주목하고 있는 것이다. 프로퍼블리카는 재범으 저지르지 않았는데 고위험군으로 분류되는 ’억울한 일이 발생하는 오류’에 주목했다면, COMPAS는 고위험군이라는 분류가 얼마나 잘 재범자를 찾아낼 수 있는지를 말해주는 ’성능’에 주목했다고 볼 수 있다. 이렇게 다른 조건부 확률에 주목하게 되면 공정성에 대해 다른 평가를 하게 된다. 아니, 더 정확하게는 이 사태가 보여주는 것은 우리가 일반적으로 ’공정성’이라고 부르는 개념 속에는 여러 가지 다른 개념이 포함되어 있다는 것이다.\n앞서 우리는 적어도 3개의 공정성 개념을 본 셈이다. 이제, 수학 기호를 이용해 이를 조금 더 일반화해 보자. 이러한 일반화 작업을 통해 사실은 무수히 많은 공정성 개념이 존재할 수 있다는 사실을 알 수 있다. 더 나아가, 안타깝게도 대부분의 경우 이러한 다수의 공정성 개념들은 동시에 성립 가능하지 않는 경우가 많다."
  },
  {
    "objectID": "fml.html#수학적-정의",
    "href": "fml.html#수학적-정의",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "6.3 수학적 정의",
    "text": "6.3 수학적 정의\n다음의 공정성 정의들은 위의 프로퍼블리카와 COMPAS 간의 논쟁에서 등장한 수학적 공정성 개념을 조건부 확률을 통해 일반화한 것이다.\n\n6.3.1 비인지(Unawareness)\n\\[P(R=1|X,A)=P(R=1|X)\\]\n이 수학적 정의에서 중요한 것은 등호 왼편에는 조건부 확률의 조건분에 \\(A\\)가 포함되어 있는 한편, 오른쪽에는 그렇지 않다는 것이다. 이것이 의미하는 바는, \\(A\\)라는 민감한 속성이 포함되어 있든 포함되어있지 않든, 알고리즘의 예측 결과는 같아야 한다는 것이다. 다시 말해, 알고리즘이 민감 속성을 ‘모른채(unaware)’ 예측을 생산해야 한다는 것이다.\n이러한 공정성 요건은 매우 직관적이다. 특정한 민감 속성을 고려한 차별적 예측이 문제를 발생시키는 것이라면, 해당 민감 속성으 고려하지 않으면 되는 것이 아닐까? 이러한 직관성으로 인해, 비인지 조건은 기존 공정성과 관련된 여러 법률들과도 잘 어울린다. Barocas와 Selbst가 매우 영향력 있는 2016년 논문에서 지적한 바와 같이, 자동화된 알고리즘 이전부터 내려온 공정성과 관련된 법률들은 예측에서의 불평등한 대우(disparate treatment) 또는 예측의 불평등한 효과(disparate impact)를 규제한다. 여기서 전자는 예측 또는 판단 ’과정’에서 여러 집단을 불평등하게 다루어서는 안 된다는 것을 의미하기에, 실질적으로는 예측 대상이 특정 민감 속성을 가졌는지를 예측 과정에 포함시켰을 경우 이를 불평등 대우로 여기고 처벌하는 방식의 법 논리를 가지게 된다. 즉, 민감속성의 비인지를 요구하는 것이다. 이는 과거 법을 인공지능을 위한 예측에 그대로 활용할 수 있다는 대단히 큰 장점이 있다.\n그러나, 이러한 공정성의 요구에는 치명적인 단점이 있다. 바로 민감 속성과 통계적으로 상관관계를 갖는 대리변수들이 존재할 수 있다는 것이다. 예컨대, 특정 지역에 특정 인종이 집중적으로 거주한다면 (에미넴이 출현한 영화 &lt;8마일&gt;에 나오는 디트로이트를 떠올려보자), 우편번호라는 전혀 민감해보이지 않는 속성은 인종이라는 민감한 속성을 ‘높은 확률로’ 알아낼 수 있는 ‘대리변수(proxy)’가 된다. 특히, 과거에 비해 훨씬 더 많은 변수를 예측에 한꺼번에 사용할 수 있는 ’빅데이터’를 통한 예측 환경에서는 매우 많은 대리변수가 존재해서 민감 속성으 거의 정확하게 추론할 수 있는 경우가 많다. 따라서, 단순히 민감속성을 예측에 사용해서는 안 된다는 비인지 조건은 알고리즘의 공정성을 달성하기에는 ’지나치게 약한’ 조건일 가능성이 크다. 그 때문에, 많은 현대의 공정성 조건들을 이러한 대리변수의 존재를 전제한 경우가 많다.\n이는 어떤 의미에서는 절차적 공정성을 의미한다.\n\n\n6.3.2 통계적 동등성(Statistical Parity)\n\\[P(R=1|A=a)=P(R=1|A=b)\\]\n통계적 동등성은 문헌에 따라서, 독립성(Independence), 또는 인구적 동등성(demographic parity), 집단 공정성(group fairness)라고 부르기도 한다.\n이 정의는 프로퍼블리카의 COMPAS에 대한 첫번째 비판에서 이미 본 것으로, 집단 사이에 예측의 차이가 없어야 한다는 것을 의미한다. COMPAS와 조금 다른 예를 들자면, 자동화된 알고리즘이 대출 승인을 해 주는 경우, 대출 확률이 남성과 여성 간에 차이가 없어야 한다는 것이다. 이는 앞서 Barocas와 Selbst의 구분에서 두 번째 종류의 공정성 법률, disparate impact 요건과 관련된다. 즉, 예측 과정이야 어떻든, ‘결과적으로’ 집단 간에 차이가 없어야 한다는 주장이다.\n많은 경우, 이러한 요건은 글자 그대로 적용된다기 보다는, 근사식 형태로 요구된다. 즉, 두 집단 사이의 예측치 차이가 ’지나치게 커서는 안 된다’는 식이다. 이러한 방식은 기존 법안의 공정성 법률에서 종종 관찰되는데, 미국 법률의 four-fifth rule에 해당한다. 80%의 법칙이라고 옮겨도 좋을텐데, 어떤 집단도 가장 선정 확률이 높은 집단에 비해 선정확률이 80%이하로 내려가서는 안 된다는 것이다. 즉, 대출 승인률이 남성에게 50%라면 여성에게 적어도 40%는 되어야 한다는 것이다.\n어떤 과정을 통해서 예측을 하던간에 집단 간 같은 정도의 예측 결과를 요구한다는 점에서 통계적 동등성은 매우 강한 공정성 요건인 것처럼 보인다. 하지만, 해당 요구사항에는 몇가지 문제점이 있다. 가장 쉽게 생각할 수 있는 문제는, 실제 예측하고자 하는 진실 \\(Y\\)와 민감한 속성 \\(A\\) 사이에 상관관계가 있을 수 있다는 점을 고려하지 않는다는 것이다. 예컨대, 흑인과 백인 사이의 ‘평균적인’ 대출상환능력에는 실제 20% 이상의 차이가 있을 수도 있다. 이에 대해 미리 20%의 상한을 결정하는 것은 은행으로 하여금 손해를 감수할 것을 요구하는 셈이다. 물론, 이는 장기적인 사회적 변화를 위해서 기업이 감수해야 할 몫이라고 합의할 수 있는 부분이라고 할 수 있다.\n하지만, 더 중요한 문제는 이른바 ‘laziness’라는 문제이다. 즉, 평균적으로 예측치의 비율만 집단간 유사하게 맞추면 되기 때문에 예측이 얼마나 중요한지는 중요하지 않다는 것이다. 예를들어, 통계적 동등성에 따라 앞서의 예와 같이 40%의 대출 승인 비율을 준수해야 하는 집단이 있다면, 이들에게 얼마나 상환 능력에 따라 대출 승인을 해야 하는지는 통계적 동등성과 아무런 관련이 없다. 극단적으로 40%의 확률로 윗면이 나오는 동전을 던져서 랜덤하게 대출승인을 해 준다고 해도 통계적 동등성은 달성할 수가 있는 것이다. 이러한 문제점을 해결하기 위해서는 ’정확성’ 개념이 공정성 안으로 들어올 필요가 있는데, ’정확성’이 예측 결과(\\(R\\))가 실제(\\(Y\\))를 얼마나 잘 맞추는가에 대한 측정치인 이상, 공정성 정의 안에 \\(Y\\)가 포함되어야 한다는 것으 의미한다.\n또 한 가지 근본적인 문제점은 세상에는 실제 불평등이 존재한다는 것이다.\n앞으로 이야기할 공정성 개념은 주로 이 두 가지 문제를 해결하기 위한 개념들이라고 볼 수 있다.\n이러한 두 가지 문제점은 사실 여기서 핵심은 두 가지 문제이다. - Y를 도입해서 정확성에 대한 고려가 필요하다는 것. - 실제 세상에는 불평등이 존재한다는 것. 이 두 가지 문제는 앞으로도 계속 문제가 되고 회피하기 어렵다.\n\n\n6.3.3 분리성(Seperation)\n\n정확성이 왜 중요한가 - 고릴라 사태를 생각해봐라….\n\nequalized odd, 정확도 동등성(accuracy parity)\n참양성 동등 \\[P(R=1|Y=1, A=a)=P(R=1|Y=1, A=b)\\]\n아래는 위양성동등 \\[P(R=1|Y=0, A=a)=P(R=1|Y=0, A=b)\\]\n참음성 동등. \\[P(R=0|Y=0, A=a)=P(R=0|Y=0, A=b)\\]\n아래는 위음성동등 \\[P(R=0|Y=1, A=a)=P(R=0|Y=1, A=b)\\]\n이제 조건문에 \\(Y\\)가 등장했다. 이는 특정 정확도, 즉 진실에 대비해서 얼마나 예측치가 정확하게 결과를 알아냈는지에 대한 비율을 표현하는 네가지 정확도, 참양성률(True Positive Rate; TPR), 위양성률(False Positive Rate; FPR), 참음성률(True Negative Rate; TNR), 위음성률(False Negative Rate; FNR) 등이 집단간에 동등할 것을 요구하는 공정성의 정의이다.\n예컨대 참양성 동등의 경우, 민주주의에서 강조하는 특정한 공정성 개념, ’기회의 평등’과 직접적인 연관을 갖는다. 왜냐하면, 실재로 맞춤한 능력을 갖는 사람(\\(Y=1\\))이 알고리즘에 의한 예측에 의해 기회를 얻을(\\(R=1\\))확률이 집단 간에 동일(\\(P(R=1|Y=1,A=a)=P(R=1|Y=1,A=b)\\))해야 한다는 것이 바로 기회의 평등의 개념이기 때문이다.\n문제는 잘 기계학습에서이 네 가지도 하나가 성립한다고 해서 다른 정확도가 함께 올라가리라는 보장이 없을 뿐더러, 어떤 경우에는 상호 충돌하여 한 가지 정확도가 개선되면 다른 정확도는 하락하는 트레이드오프(trade-off) 관계가 성립하는 경우가 많다는 것이다. 따라서, 실질적으로는 여러 정확도 개념 중에서 예측의 대상이 되는 사람들에게 ’억울한 오류’를 막기 위한 정확도, 또는 오류율 개념에 주목하는 경우가 많다. 예컨대, 재범 위험성이 큰 사람은 실재 재범을 저지르지 않음(\\(Y=0\\))에도 ’양성(positive; \\(R=1\\))’으로 예측되는 경우 ’억울한 피해’가 발생할 수 있으므로, 잘못된 양성 판정, 즉, 위양성률에 주목하여, 집단 간에 위양성률이 동등하도록 하는 것을 최우선으로 한다. 반면, 대출 상환 능력을 예측하여 이를 바탕으로 대출 승인을 하는 경우, 실재로 대출을 갚을 수 있는 능력이 있음에도(\\(Y=1\\)), 대출을 승인을 허용하지 않는 음성 판정(\\(R=0\\))을 하는 경우, 이는 대출 신청자 입장에서는 억울한 판정이므로, 이렇게 잘못 음성 판정을 내릴 확률, 즉, 위음성률을 집단간에 동등하게 하는 것이 공정한 알고리즘의 우선적인 선택이 된다.\n여기서 ’우선적 선택’이라는 표현을 이해하는 것이 중요하다. 알고리즘의 공정성을 확보하고자 하는 조직은 결국 여러 공정성 개념 중에서 더 중요한 개념과 덜 중요한 개념 사이에서 조직의 목표에 따라 균형을 맞추는 선택을 해야 한다는 것이다. 이는 파래토 원리에 따른 균형이라는 마지막 테마와 직접적으로 연결된다.\n기본적으로 이 관점은 차별적 현실을 인정하자는 관점. 있는 능력 차이를 인정하고 그 안에서 정확도만 동일하면 된다…\n‘최우선’ -&gt; 결국 이것은 선택, 그리고 파레토….\n여기서는 비지도 학습은 뺄 것.\n\n\n6.3.4 충분성(Sufficiency)\n프로퍼블리카의 비판에 대한 COMPAS의 반비판, 즉, 재범 위험이 크다고 예측한 사람 중에서 실재 재범을 일으킨 확률이 인종 집단 간에 크게 차이나지 않는다는 주장은 분리성과는 상이한 정확도 개념을 이용하여 공정성을 정의하는 관점에 바탕을 두고 있다. 즉, 알고리즘이 양성 또는 음성이라고 예측했을 때 그것이 얼마나 실재와 일치하는지, 일종의 ’예측의 성능/성과’에 초점을 맞추는 관점이다. 충분성은 문헌에 따라 예측률 동등(Predictive Rate Parity)라고 부른다. 이를 수식으로 표현하면 다음과 같다.\n양성예측동등(Positive Predictive Parity) \\[P(Y=1|R=1,A=a)=P(Y=1|R=a,A=b)\\] 음성예측동등(Negative Predictive Parity) \\[P(Y=0|R=0,A=a)=P(Y=0|R=0,A=b)\\]\nRecall과 Precision 차이에 해당. -&gt; 머신러닝에서 이 두개는 전혀 동등하지 않으며, 대부분 동시에 성취 불가능한다.\n단점은 분리성과 유사하게 기존의 차별을 용인하는 문제.\n사실은 이러한 공정성 개념이 전부는 아니다. 위의 공정성 개념들을 일부 수정하고자 한 것. 예컨대 마이크로소프트의 Cynthia Dwork(윤리적 알고리즘 분야에서 최고의 권위를 가지고 있는 미국의 여성 컴퓨터공학자)와 동료들이 제시한 개인적 공정성(Individual Fairness)은 비슷한 특성을 가지고 있는 개인이 다른 예측 결과를 경험해서는 안 된다는 의미에서의 공정성 개념으로, 우리가 앞서 살펴본 공정성 개념들이 집단간 비교를 통해 정의되는 것에 비해서, 각 개인에게 공정한 경험을 보장하려고 하는 시도라는 점에서 의의가 있다. 또한, 영국의 앨런 튜링 연구소의 Chris Russell과 동료들이 제안한 인과적 공정성(Causal Fairness)은 민감한 속성이 다른 비민감 속성의 ’원인’으로 영향을 미쳐 최종적으로 예측 결과에 영향을 미치는 메커니즘을 고려하고자 하는 공정성 개념이다. 이러한 분야는 여전히 많은 발전을 이루고 있고, 논의되고 있는 측면이 커 여기서는 생략하도록 한다."
  },
  {
    "objectID": "fml.html#불가능성-정리-impossibility-theorem",
    "href": "fml.html#불가능성-정리-impossibility-theorem",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "6.4 불가능성 정리 (Impossibility Theorem)",
    "text": "6.4 불가능성 정리 (Impossibility Theorem)\n이렇게나 많은 공정성 개념이 존재한다는 것은 그 자체로 문제이지만, 코넬의 정보과학 대가인 Jon Kleignberg등이 명명한 바, 이른바 ’불가능성 정리’라고 부르는 수학적 결과는 더욱 더 큰 난점을 제기한다. 이 불가능성 정리에 따르면, 우리가 위에서 살펴본 공정성 정의들은 수학적으로 동시에 성립할 수 없다는 것이다. 이를 다시 표현하자면, 우리가 말하는 공정성이라는 용어 속에는 기실 다양한 차원의 공정성 개념들이 숨어 있는데, 이들은 본질적으로 서로 모순된다는 것이다. 따라서, 프로퍼블리카가 ’분리성’에 근거하여 제기했던 비판과 ’충분성’에 근거해서 Northpointe가 제시한 반박은 애초부터 엇갈릴 수밖에 없고, 그 입장 차이는 해소할 방법이 없다는 것이다!\n그렇다고 해서 이러한 결론이 공정성은 우리가 성취할 수 없는 목표라는 것을 의미하는 것일까? 그렇지 않다. 사실, 이렇게 서로 모순되는 공정성 개념이 우리의 ’공정성’이라는 용어의 일상적 용법에 숨어있었다는 것을 알아내는 것은 후퇴라기 보다는 한발 전진이라고 할 수 있다. 문제 해결의 전제는 우리가 마주한 문제가 무엇인지를 아는 것이기 때문이다. 이렇게 사회과학적 영역에서 수학적 조작화를 통해 인간 언어의 모호성을 극복하고 다양한 차원들이 존재함을 드러내서 더 나아가 일종의 불가능성 정리에까지 도달하는 것은 처음이 아니고, 오히려 사회과학의 진전에서 중요한 역할을 하였다.\n예컨대, 경제학, 정치경제학…. (내 영어논문에서…)"
  },
  {
    "objectID": "fml.html#파레토-경계를-통한-균형-찾기",
    "href": "fml.html#파레토-경계를-통한-균형-찾기",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "6.5 파레토 경계를 통한 균형 찾기",
    "text": "6.5 파레토 경계를 통한 균형 찾기\n문제가 양립할 수 없는 다수의 목표들이라면, 결론은 결국 목표들 간의 취사선택 또는 타협이 될 것이다. 그리고 수학저 조작은 ‘체계적 타협’을 위한 도구까지 마련해준다(Kearns). 이러한 타협을 달성하는데 핵심적으로 사용하는 개념적 도구는 파레토 효율성(Pareto Efficiency)라는 개념이다. 이 개념은 19세기와 20세기초에 주로 활동한 빌프레도 파레토(Vilfredo Pareto)라는 이탈리아의 사회학자이자 경제학자의 이름을 딴 것으로, 파레토 효율성 말고도 파레토의 이름이 붙은 다른 유명한 개념들도 많이 있습니다. 파레토 효율성은 사회적 최적상태(optimality)를 규정하는 하나의 방식으로, 대략적으로 “어느 누군가의 희생 없이는 다른 사람들의 상태에 대한 개선이 불가능한 상태”를 의미한다고 볼 수 있습니다. 이를 조금 더 일반화하자면, “하나의 목표에 대한 양보 없이 다른 목표들의 개선이 불가능한 상태”라고 생각할 수 있습니다. 그런데, 이러한 개념에 대한 설명이 그다지 ’최적상태’ 또는 ‘효율성’ 등의 표현과 어울리지 않는 것처럼 보이는 것도 사실이다. 사실 파레토 효율이라고 부를 수 있는 상태는 무수히 많이 존재한다. 오히려 파레토 효율성의 의의는 파레토 효율성 관점에서 ‘열등한’ 사회적 선택을 찾아내는 기준이 된다는 점에 있다고 볼 수 있다. 이를 이해하기 위해서 다음 그래프를 살펴보자.\n아래 그림은 다시 그리는 것이 나을 수 있겠다…\n Ho, D. E. & Xiang, A. (2020). Affirmative algorithms: The legal grounds for fairness as awareness,\nhttps://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/\n위의 그림은 자동화된 알고리즘을 이용하는 가상의 조직이 정확도(중 하나의 지표)와 공정성(중 하나의 지표)를 자동화된 판단을 통해 다렁하고자 하는 두 가지 목표라는 것을 가정하고 있다. 그래프 상의 각 점들은 그 조직이 선택할 수 있는 예측 알고리즘들이 성취할 수 있는 정확도와 공정성의 조합을 의미한다. 이제 가장 바깥에 있는 점들을 연결한 선을 살펴보자. 이 선 위에 존재하는 알고리즘은 ‘파레토 효율적’이다! 왜 그런가 예컨대 해당 선 위에 있는 알고리즘 A를 생각해보자. 이 알고리즘보다 공정성이 높은(즉, A보다 오른쪽에 위치한) 알고리즘을 선택하려면 어떻게 해야 할까? 일단, A가 위치한 경계선 오른쪽 바깥에 있는 알고리즘은 기술적인 한계로 인해 그 조직이 선택할 수 없는 알고리즘이다. 따라서, 즉, A에서 공정성과 정확성을 ’동시에’ 개선할 방법은 없다! 따라서, 공정성을 개선하기 위해서는 정확도를 희생하면서, 경계선 상 혹은 안쪽에 있는 다른 알고리즘을 선택하는 수밖에 없다. 같은 논리에 따라, 반대로 A알고리즘에서 정확성을 개선한 알고리즘을 선택하고 싶다면, 공정성을 다소 희생하는 수밖에 없다. 이제 “하나의 목표에 대한 양보 없이 다른 목표들의 개선이 불가능한 상태” 라는 파레토 효율성의 정의를 생각해보면, A는 정확히 해당 정의에 부합하는 알고리즘이라는 것을 알 수 있다. 사실, 이러한 논리는 경계선 상에 있는 모든 알고리즘에 적용된다. 따라서, 해당 경계선 상에 존재하는 알고리즘은 모두 ’파레토 효율적’인 알고리즘이다! 이러한 중요성 때문에 해당 경계선을 특별하게 ’파레토 경계(Pareto Frontier)’라고 부릅니다.\n이제 파레토 경계 안 쪽에 있는 알고리즘들, 예컨대 B와 같은 알고리즘에 대해 생각해보자. B와 같은 알고리즘은 정확도와 공정성을 ‘동시에’ 개선할 수 있다. 왜냐하면 B에서 오른쪽 상단 방향으로 파레토 경계 안 쪽에 더 좋은 알고리즘이 존재하기 때문이다. 이렇게 두 가지 목표를 동시에 개선하느 것을 ‘파래토 개선(Pareto Improvement)’라고 부른다. 이렇게 파레토 개선이 가능한 알고리즘은 어떠한 희생도 없이 개선이 가능하므로, 개선된 알고리즘에 비해 확실히 ’열등한’ 알고리즘이라고 볼 수 있다. 따라서, 파레토 개선이 가능한 알고리즘을 ’파레토 열등(Pareto Dominated)’하다고 한다. 이제, 파레토 효율적인 알고리즘(=파레토 경계상의 알고리즘)은 파레토 개선이 불가능한 알고리즘이라는 것을 알 수 있고, 파레토 경계 안 쪽에 존재하는 알고리즘은 확실히 파레토 열등한 알고리즘이라는 것을 알 수 있다. 따라서, 이러한 파레토적 사고 방식을 통해 단 하나의 최적 알고리즘을 알 수 없다고 하더라도, 적어도 파레토 열등한 알고리즘이 아니라, 파레토 곡선 상의 파레토 효율적 알고리즘 중에 선택이 이루어져야 한다는 것을 알 수 있다!\n그렇다면 파레토 경계상의 많은 알고리즘 중에는 어떠한 알고리즘을 선택해야 하는가? 여기서부터는 조직이 여러 목표들 사이의 경중을 어떻게 평가할 것인가에 대한 합의된 기준이 필요하다. 즉, 다수의 목표를 동시에 가장 최적 수준에서 달성할 수는 없으므로, 목표들 간의 ‘타협’이 이루어져야 하며, 이는 조직의 형태, 목적, 구성원의 생각, 제도 등에 따라 다를 수 있다는 것이다. 이러한 타협 역시 파레토 경계 위에서 단순히 ’감으로’ 이루어질 필요는 없다. 별도로 자세하게 다루어야 하는 내용이므로, 여기서 자세하게 다루기는 어렵지만, 해당 조직이 여러 목표 간에 어떻게 타협하는 것을 최선이라고 생각하는가, 혹은 그렇게 합의했는가를 일종의 조직의 선호를 나타내는 수학적 ’함수’로 표현할 수 있다. 그리고 그 함수는 다음과 같이 그래프로 표현 가능하다.\n\n\n\n무차별 곡선을 이용한 알고리즘 선택\n\n\n위의 그림에서 조직의 선호를 나타내는 곡선은 ‘무차별 곡선(indifference curve)’이라고 하는데, 해당 무차별 곡선의 형태는 정확성과 공정성 간의 타협에 대한 조직의 선호에 따라 달라진다. 이제 ’주어진 선호/합의’ 하에서 최적 알고리즘은 두 그래프가 만나는 점에서 결정된다. (경제학원론을 배운 학생이라면 이러한 사고 방식에 익숙할 것이다. 보지 못한 경우라도, 경제학원론 교과서의 소비자효용, 후생경제학 부분을 훑어보길 바란다. 이러한 선호 표현 방식과 그를 통한 의사 결정의 문제를 다루는 분야를 의사결정과학Decision Science이라고 부른다). 만약, 더 많은 정확성과 더 많은 공정성 개념을 고려하여 타협을 하고자 한다면, 위의 파레토 경계와 파레토 곡선은 더 이상 2차원의 그래프로 표현할 수는 없을 것이다. 그러나 시각화하기 어려운 고차원의 선택 문제라고 하더라도 동일한 논리를 통한 선택은 여전히 가능하다.\n지금까지 훑어본 다양한 공정성의 개념과 양립불가능성 정리, 그리고 파레토 효율성을 통한 알고리즘 선택 과정은 알고리즘 공정성에 대한 과학적 논의가 던져주는 부정적 전망과 부정적 전망을 동시에 보여준다. 모두 합리적으로 들리는 여러 차원의 공정성을 동시에 달성할 수 없다는 것, 그리고 공정성과 예측의 정확성 역시 자주 충돌한다는 것은 인간사회가 인공지능의 자동화된 예측과 결정에 의존하는데 위험성이 수반된다는 것을 의미한다. 하지만, 이러한 양립불가능성은 자동화된 결정시스템에 고유한 것이 아니다. 인간에 의해서 동일한 결정이 이루어진다고 하더라도, 동일한 문제는 발생한다. 다만, 그 전에는 이렇게 엄밀한 수학적 접근을 시도하지 않았기에 우리가 가지고 있는 딜레마를 정확히 보고 있지 못했을 뿐이다. 오히려 공정성의 다양한 정의를 밝히는 작업은 조직의 합의된 목표와 그 안에 포함된 타협을 체계적으로 알고리즘 선택에 반영할 수 있는 도구를 제공한다는 점에서 진보라고 할 수 있다. 우리에게 이러한 도구들이 있는 한, 선택은 늘 가능하다. 중요한 것은 사람과 조직이 좋은 목표를, 좋은 타협안을 만들어내는 것이다. 사람의 일을 기계가 대신 해 줄수는 없는 일이라는 교훈은 또 다시 반복된다.\n전체 워크플로우를 보여주는 그림"
  },
  {
    "objectID": "fml.html#남은-문제들",
    "href": "fml.html#남은-문제들",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "6.6 남은 문제들",
    "text": "6.6 남은 문제들\n우리가 위에서 살펴본 공정성 개념들은 ‘사후적으로라도’ 정답을 알 수 있는 기계학습, 즉, 지도학습(Supervised Machine Learning)을 사용하는 경우 예측 결과의 공정성을 개선하기 위해 사용할 수 있는 공정성 개념의 수학적 조작화라는 주제에 국한된 것이었다. 이외에도 인공지능 공정성을 다루는 다른 연구 영역들이 여전히 존재한다. 여기서는 교재의 목적상 이 모든 논의를 담지는 못하고, 그러한 논의들이 발전되고 있다는 것 정도를 지적하도록 한다. 관심있느 독자들은 챕터 말미의 ’더 읽을거리’와 ’참고문헌’을 참조하기를 바란다.\n\n6.6.1 비지도학습의 공정성\n가장 먼저 생각해볼 수 있는 것은 비지도학습(Unsupervised Machine Learning)에서의 공정성이다. 비지도학습은 우리가 위의 논의 속에서 사후적으로라도 알게 되는 것으로 가정했던 예측하고자 하는 진실, \\(Y\\)가 존재하지 않는 경우의 기계학습을 의미한다. 예컨대 온라인 소비자의 행동 데이터로부터 충성도가 높은 소비자를 찾아내는 상황을 생각해보자. ’충성도 높은 소비자’라는 것은 어차피 마케터의 관점에서 존재하는 관념일 뿐, 이를 명확하게 정의하기 어렵기 때문에, 사후적으로라도 ’충성도 높은 소비자’가 누구였는지를 알아내는 것은 어렵다. 이러한 경우에는 비슷한 행동을 보이는 소비자들끼릴 묶어내는 ’군집화(Clustering)’라는 비지도학습 기술을 사용한다.\n또 최근 주목받는 예로 인공지능에 의한 기계번역을 하는 경우를 떠올려보자. 언어의 요소(예컨대 단어)를 다차원의 수치로 표현한다음, 이 수치가 가까운 다른 언어의 요소로 대체하는 방식을 이용한다. 예컨대, ’개’와 ’dog’는 다른 언어에 속한 단어이지만, 숫자로 표현된 두 단어의 거리는 가깝기에 이를 이용한 컴퓨터는 번역을 할 수 있는 것이다. 이렇게 언어를 다차원의 수치로 표현한 것을 ’임베딩(Embedding)’이라고 하는데, 단어의 임베딩을 찾아내는 작업 역시 비지도학습에 해당한다. 사실 임베딩은 자동 번역뿐 아니라, 언어를 음악으로, 그림으로, 동영상으로 ’번역’하는 기술에도 다양하게 사용된다.\n임베딩 그림\n정확성에 기반한 공정성 개념이였던 분리성, 정확성의 수식에 \\(Y\\)가 포함되었었다는 것을 상기해보면, 그러한 공정성 개념은 \\(Y\\)가 존재하지 않는 비지도학습에는 적용되기 어려울 것이다. 그럼에도 불구하고, 비지도학습에는 다양한 공정성 문제가 발생한다. 예컨대. ’여성’이라는 단어의 임베딩은 ’간호사’의 임베딩에 더 가깝고, ’남성’이라는 단어의 임베딩은 ’의사’의 임베딩에 더 가깝게 도출된다면, 이는 임베딩에 편향이 반영되어있다는 것을 의미한다. 그리고 이는 인공지능이 이용하는 학습데이터, 즉, 인간이 만들어낸 데이터가 편향된 이상 인공지능을 자동으로 해결해주지 않는다. 따라서, 최근에는 이를 해결하기 위한 다양한 공학적 시도들이 있다. 관심있는 독자들은 키언스의 책이나, Bolukbasi 등 (2016) 등의 논문을 살펴보기 바란다.\n편행된 임베딩 그림\n\n\n6.6.2 데이터의 편향성\n또 한가지 중요한 주제는 데이터 그 자체가 가지는 편향이다. 앞서 언급한 바와 같이, 인공지능은 인간의 오류와 편견을 그대로 반복하거나, 더 나쁘게는 확대재생산한다. 이렇게 인간의 편향이 인공지능의 편향에 스며드는 가장 영향력 있는 통로는 알고리즘의 복잡한 디자인이라기 보다는 데이터 그 자체이다. 2016년, Barocas와 Selbst는 California Law Review에 데이터에 편향이 숨어드는 경로에 대한 매우 영향력 있는 논문을 게재하였다. 그러한 경로는 매우 다양해서, ‘좋은 근로자’, ’높은 금융 신용도’와 같이 알고리즘을 통해 예측하고자 하는 목표 그 자체를 정의하는데 있어 편향이 포함될 수 있다. 예컨대 ’좋은 근로자’를 정의하면서 흔히 남성성과 결부되는 속성을 포함시키는 경우가 그러하다. ’좋은 근로자’가 중립적으로 잘 정의되었다고 하더라도, 기존 사원 기록에서 특정 근로자를 좋은 근로자로, 특정 근로자를 부족한 근로자로 인간이 ’레이블링’하는 인간의 판단에도 편향이 포함될 수 있다. 예컨대, 과거 인사평가 기록을 인공지능의 학습데이터로 이용했는데, 남성 인사 평가자가 남성 사원에 대해 유리한 판단을 해 왔다면, 이러한 학습데이터를 이용한 인공지능은 편향적인 결정을 생산할 것이다. 또한, 데이터 수집 과정에서 특정 그룹이 과도하게 많이 데이터에 포함되거나, 적게 포함된 경우에도 데이터가 편향을 가지게 된다. 예컨대, 특정인종의 거주지역에 집중적으로 순찰을 수행하면서 발생한 기록을 인공지능의 학습 데이터로 이용했을 때(과대표집), 해당 인종의 범죄 위험을 높게 평가한다든지, 소수인종의 인터넷 이용 데이터가 다수인종에 비해 상대적으로 적어(과소표집) 소수 인종에 대한 예측이 지나치게 부정확한 예가 그렇다. 또, 직접적을 차별을 하지 않더라도, 특정 집단에게 유리한 개인 특성(예컨대, 출신대학, 출신지역 등)을 학습에 과도하게 많이 포함시키는 경우에도 편향이 발생할 수 있다.\n\n\n6.6.3 개선 방법 (Fair Algorihtm)\n마지막으로, 인공지능의 편향 위험이 관측되었을 때, 이를 수정하기 위한 방법에 대한 논의 역시 활발하다. 실질적으로 어떻게 인공지능을 어떻게 더 공정하게 만들 수 있을 것인지에 관한 논의이므로, 최근 컴퓨터공학자들의 노력은 이 부분에 집중되고 있다. 가장 기초적으로는 데이터에 대한 사전작업을 통해서 데이터에 숨은 편향성을 미리 교정한 후(전처리; preprocessing) 인공지능을 학습시키는 방법 (대표적으로는 Zemel et al.의 2013년 ICML 페이퍼), 인공지능 학습시, 위의 논의에서 정의한 공정성 요구조건을 일종의 학습의 제약으로 포함시키는 방법 (대표적으로는 Zafal et al.의 AISTAT 2017년 페이퍼), 인공지능의 학습이 종료된 후에 공정성 정의를 이용하여 학습 내용을 수정하는 방법 (이른바 후처리;postprocessing)으로 나눌 수 있다. 이 분야는 기술적인 지식을 요구하므로, 본 챕터에서 자세하게 다룰 수는 없지만, 기계하습의 원리에 대한 다소간의 이해가 있다면, 위에서 논의한 공정성의 정의들이 이러한 ‘공정한 알고리즘’ 개발에 직접적으로 사용되고 있다는 것을 알 수 있다.\n위의 정의들이 이러한 개선방법을 만들어내는데 도구로 포함된다는 것이 중요하다.\n요건 블로그 보면서 해결…\n해법: Preprocessing 등등… -&gt; 이 역시 ’그 외 토픽’에 포함시키는 것이…\n\n데이터셑의 공정성: Barocas & Selbst"
  },
  {
    "objectID": "fml.html#결론",
    "href": "fml.html#결론",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "6.7 결론",
    "text": "6.7 결론\n이 장에서는 활발하게 공정한 인공지능(FML) 논의에서 ’지도학습 맥락에서 공정성의 수학적 정의’를 주로 살펴보았다. 이러한 공정성 개념에 대한 수학접 접근을 통해 알게된 결론은, 우리가 일상적으로 사용하는 ’공정’의 개념에는 서로 모순되는 다른 차원의 공정성 개념이 포함되어 있다는 것이다. 이는 일견 우리가 일반적으로 옳다고 여기는 가치를 동시에 성취 불가능하다는 부정적 결론으로 보일 수 있지만, 꼭 그렇지는 않다. 오히려 수학적 접근을 통해 우리가 이미 가지고 있던 개념의 모호성을 더욱 분명하게 함으로써 논의를 한걸음 진전시키는 것이라고 볼 수 있다. 구체적으로, 공정한 인공지능 논의를 통해 공정성은 더 발달한 인공지능이 기술적으로 해결할 수 있는 것이 아니라, 모순되는 목표들 간에 인간이 도출해내야 하는 타협과 합의를 통해서 성취가능한 것이라는 것을 알게 되었으며, 그러한 타협과 합의가 존재한다면 공정성의 수학적 정의는 이를 컴퓨터가 이해할 수 있는 언어로 번역함으로써 알고리즘을 인간의 결정에 부합하도록 체계적으로 개선할 수 있는 통로를 마련해주며, 대안으로 개발되고 있는 공정한 인공지능 알고리즘들은 대부분 이러한 수학적 정의에 의존하고 있다.\n지금까지 본 바와 같이 인공지능의 편향은 인간 세계에 존재하는 편향이 반영된 것이며, 이를 해결하기 위한 방법 역시 인간의 합의에 달려있다. 수학적/공학적 논의는 인간 세계가 생각하는 공정을 기계가 이해할 수 있도록 하는 통로를 마련하는 것에 불과하다. 단, 이렇게 ㄷ 인공지능의 공정성을 인간의 불공정성의 단순한 반영으로 생각하는 일종의 수동적 접근을 전재한다. 이러한 수동적 접근은 분리성, 충분성의 정의에서 본 것과 같이 이미 세상에 존재하는 불평등은 주어진 것으로 받아들이는 경향이 있다. 그러나, 인간 세계의 불평등을 그대로 받아들이고 있는 인공지능이 인간의 개입없이 판별하는 영역이 늘어나면 늘어날 수록 그러한 불평등은 확대재생한 될 공산이 크다. 즉, 읟적인 교정을 위한 장치가 존재하지 않는 자동화된 결정은 현상 유지를 넘어서 사태를 악화시킬 가능성이 크다는 것이다. 따라서, 최근에는 affirmative action을 취하는 인공지능에 대한 논의가 발전하고 있으며, 여기에는 더욱 더 인간과 사회의 결정이 중요해진다. 개선은 적어도 인공지능이 혼자 할 수 없는 것. 인간이, 또는 인간과 인공지능의 ’연합’이 할 수 있는 것!!! 그것을 기계에게 떠넘기지 말고, 인간은 할 일을 피하지 말아야 할 것이다!"
  },
  {
    "objectID": "fml.html#더-읽을거리",
    "href": "fml.html#더-읽을거리",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "6.8 더 읽을거리",
    "text": "6.8 더 읽을거리\n\nBarocas & Selbst\n임베딩 교정\nBarocas 최신 교과서 등등….\n키언스\n알고리즘이 지배한다는 환상."
  },
  {
    "objectID": "fml.html#참고문헌",
    "href": "fml.html#참고문헌",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "6.9 참고문헌",
    "text": "6.9 참고문헌\n홍찬숙 (2021) 청년의 무엇이 ‘성평등 프레임에서 젠더갈등과 공정성 프레임으로’ 변화한 것인가? 키언스\nBolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.\nChouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2), 153-163.\nKleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807.\nDwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012, January). Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference (pp. 214-226).\nRussell, C., Kusner, M. J., Loftus, J., & Silva, R. (2017). When worlds collide: integrating different counterfactual assumptions in fairness. Advances in neural information processing systems, 30.\nhttps://afraenkel.github.io/fairness-book/content/05-parity-measures.html\nhttps://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb\nBarocas, S., & Selbst, A. D. (2016). Big data’s disparate impact. California law review, 671-732.\n공정한 알고리즘은\n샌델이 비판한 바와 같이 롤즈는 공정성으로 정의를 대체하기 위해서 얄팍하게 만든다..하지만, 공정하는 말을 둘러싼 최근의 논쟁을 보면… 이것은 공정한가? 저것은 공정한가? 예컨대, 할당제에 느끼는 청년 남성들의 볼공정함…이것은 무엇인가…이런 문제들은 알고리즘의 공정성의 문제에도 여전히 적용된다. 하지만, 이들은 거기서 멈출 수 없다. 수학으로 직접적으로 연결시켜야 함. - 따라서 수학적 정의를 추구 - 이를 통해 여러 공정성 개념들을 밝혀냄 -&gt; 동시 성립 불가능함을 밝혀냄. - 결국 제도의 문제.\nBarocas & Selbst 논문/교과서 롤스\n\n\n\n\nBarocas, Solon, and Andrew D Selbst. 2016. “Big Data’s Disparate Impact.” California Law Review, 671–732.\n\n\nDwork, Cynthia, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. “Fairness Through Awareness.” In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, 214–26."
  },
  {
    "objectID": "fml.html#footnotes",
    "href": "fml.html#footnotes",
    "title": "6  공정한 인공지능을 위한 기술적 해법",
    "section": "",
    "text": "프로퍼블리카는 기사와 함께 그들이 정보공개청구를 통해 취득하여 보도에 사용한 데이터를 모두 공개하고 있다. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing 참조.↩︎\n특정 확률에 따라 그 값이 결정되는 변수를 확률변수라고 한다.↩︎"
  },
  {
    "objectID": "xai.html#설명이-쉬운-알고리즘.",
    "href": "xai.html#설명이-쉬운-알고리즘.",
    "title": "7  설명 가능한 알고리즘",
    "section": "7.1 설명이 쉬운 알고리즘.",
    "text": "7.1 설명이 쉬운 알고리즘.\n먼저, 인공지능의 근간이 되는 기계학습, 그 중에서도 지도학습(supervised learning) 방법은 분류(classification) 문제에 대해 복습해보도록 하자. 분류의 문제는 어떠한 특성(feature)을 이용해 타겟(target)을 예측하는 문제를 의미한다. 기계학습을 처음 배울 때 하는 이미지 특성을 이용해 개와 고양이를 자동으로 나누는 알고리즘, 손글씨 이미지를 이용해 숫자를 파악하는 알고리즘 같은 것들이 분류 알고리즘이다. 설명으 편의를 위해, 여기서는 그 보다 더 간단한 알고리즘을 생각해보자. 즉, 두 가지의 특성을 이용해 0 또는 1의 판단을 하는 알고리즘이다. 첫번째 특성을 \\(X_1\\), 두번째 특성을 \\(X_2\\)라고 하자. 그리고 예측하고자 하는 \\(Y\\)는 0 또는 1의 값을 갖는데, 이를 예측하고자 하는 것이다. 비근한 예로, 소득(\\(X_1\\))과 신용점수(\\(X_2\\))를 이용해 대출을 상환할 수 있는지(\\(Y=1\\)), 그렇지 않은지(\\(Y=0\\))를 판단하는 알고리즘을 만드는 상황 그런 경우이다.\n이러한 경우에 적용할 수 있는 가장 간단한 분류 알고리즘 중 하나로 이야기되는 것 중 하나는 ’로지스틱 회귀’라는 방식이다. 이는 선형성을 갖는다.\n로지스틱 회귀 그림\n이를 수식으로 표현하면 다음과 같다. 그림으로 봐도, 수식으로 봐도, 즉시 설명이 가능하다. 그림에서 기울기에 해당하니까..\n그런데, 이런 간단한 분류 방법이 늘 현실적이지는 않을 것이다. 가장 먼저, 저 경계선이 곡선인 경우도 허용하고 싶을 것이다. 다음과 같이… 그림\n또, 다음과 같이 1로 예측되는 영역이 분리되어 있는 경우도 있을 수 있다.\n그림\n이는 로지스틱 회귀 분석과 같은 방법으로는 불가능하고, 이를 조금 더 복잡한 함수에 집어넣는다.\n수식\n이러한 방식 중 하나가 바로 우리가 잘 알고 있는 ’인공신경망(Artificial Neural Networks; ANN)이다. 이제 저 안에 들어가 있는 모수는 기울기라는 단순한 해석을 가지지 않는다. 정확히 말하면, 저 모수를 설명할 방법이 없다! 이를 blackboxedness라고 한다.\n결론은 더 나은 예측을 위한 모형의 ’유연성(flexibility)’를 얻기 위해 모형은 더 복잡해져야 하며, 그 댓가로 설명가능성이 떨어진다는 것이다. 즉, 모형의 예측력과 설명가능성 사이에는 트레이드오프 관계가 성리하는 것이다!\n분류 알고리즘 중에는 로지스틱 회귀 말고도 본질적으로 설명하기 쉬운 모형들이 있다. 예컨대 의사결정 나무 같은 것이 그렇다.\nDecision Tree 그림\n의사결정 나무 같은 것은 에전 인공지능 시대의 근간 이기도 했다. 그러나 현대적인 인공적인 인공지능에 사용하는 분류 알고리즘은 이를 믹스한 것. Random Forest같은 경우… 따라서 다음과 같은 트레이드 오프를 보일 수 있다.\n일본 트레이드 오프 그림"
  },
  {
    "objectID": "xai.html#설명-가는성을-높이는-기술-xai",
    "href": "xai.html#설명-가는성을-높이는-기술-xai",
    "title": "7  설명 가능한 알고리즘",
    "section": "7.2 설명 가는성을 높이는 기술: XAI",
    "text": "7.2 설명 가는성을 높이는 기술: XAI\n그렇다면, 예측력을 위해 우리는 설명을 포기해야 하는가? 그렇지 않다. 많은 인공지능 연구자들은 예측력과 복잡도를 보존하면서도 여전히 설명하기 위한 기술들을 개발하고 있다. 여기서 핵심적인 생각은, 에측을 위한 모형을 그대로 둔 채로, ’설명을 위한 장치’를 개발하는 것이다. ’설명가능한 인공지능(eXplainable AI; XAI)’라고 부르는 영역에서 활발하게 논의되고 개발되고 있는 모형들은 그 자체로 설명이 쉬운 기계학습 모형보다는, 예측모형에 더할 수 있는 설명 장치들이라고 할 수 있다.\n이러한 설명 장치들도 몇 가지 유형으로 구분할 수 있다. 그 첫번째는 모형의존형(model-specific) 기술과 모형불문형(model-agnostic) 기술의 구분이다. 모형의존형 기술은 인공지능에 사용되는 예측 모형이 특수한 기술을 따를 때만 작동할 수 있는 모형 설명 기술이다. 예컨대, 영상/이미지 관련 인공지능에 자주 사용되는 CNN (Convolutional Neural Network)에서 작동하는 GRAD-CAM, Score CAM, Grad-CAM++ 등의 기술이 그것이다. 모형의존형 기술은 주예측모형의 작동 방식을 응용하여 만들어진 것이므로, 모형-맞춤형 설명이 가능하다는 장점이 있다. 그러나 이러한 모형들은 예측모형에 바로 응용할 수 없을 뿐만 아니라, 주모형과 직접 연동되어 작동하는 경우가 많아, 주모형의 속도를 느리게 하는 단점을 갖는 경우가 많다. 반면, 모형불문형 기술은 주모형과 독립적으로 설계되고 작동하기 때문에 범용성이 높고, 주모형의 퍼포먼스를 하락시키지 않는 경우가 많다. 이 때문에 최근이 개발 경향을 범용성 높은 모형불문형 기술을 개발하면서 설명력을 가능한 모형의존형 기술에 근접시키도록 하는 방향으로 발전하고 있다. 따라서, 이 장에서는 모형불문형 기술에 초점을 맞추어 기술할 것이다.\n또 다른 구분으로는 전역적(global) 설명과 국소적(local) 설명 기술의 구분이 있다. 전역적 설명은 이용자가 복잡한 모형 전체의 구조를 이해할 수 있도록 단순화하는 방식을 의미하는 반면, 국소적 설명은 인공지능에 의해 내려진 특정한 판단의 이유를 제공하는 방식을 의미한다. 기게학습 모형을 이용하면서 이를 유지, 개선하기 위한 운용 목적이나, 보안상의 문제를 발생시킬 수 있는 외부 공격에 대해 강건한 모형을 만들기 위한 모니커링 목적으로는 전역적 설명이 자주 사용된다. 그에 반해, 인공지능이 내린 판단에 영향을 받는 일반 이용자들이, 예컨대 자동화된 대출 결정을 받거나, 국경에서 보안 검색의 대상이 되었을 때, 그러한 결정에 대한 설명을 요구하는 경우라면 모형 전체에 대한 설명은 일반인들에게 그다지 유용하지도 않을 것이며, 더 나아가 주어진 설명이 ’이해가능(interpretable)’하지 않을 수도 있다. 이러한 경우에는 특정 결정에 대한 국소적 설명이 더욱 유용할 수 있을 것이다."
  },
  {
    "objectID": "xai.html#대표적인-xai-기술",
    "href": "xai.html#대표적인-xai-기술",
    "title": "7  설명 가능한 알고리즘",
    "section": "7.3 대표적인 XAI 기술",
    "text": "7.3 대표적인 XAI 기술\nXAI는 빠르게 발전하고 있는 분야이기 때문에, 지금도 새로운 기술이 계속해서 개발되고 있지만, 여기서는 대표적으로 알려진 몇 가지 기술을 중심으로 전반적인 개념을 이해하고자 한다. 최근에도 자주 인용되는 XAI 기술을 전역적, 국소적 설명의 구분 방식에 따라 일별해 보면 다음과 같다.\n\n\n\n분류\nXAI 기술\n특징\n\n\n\n\n전역설명\nPartial Dependence Plot (PDP)\n특성의 변화가 기계학습 모델의 예측 결과에 반영되는 영향을 그래프로 표시\n\n\nAccumulated Local Effect (ALE)\n특성의 변화가 ‘평균적으로’ 예측 결과에 반영되는 영향을 표현한 것으로 PDP와 유사하지만, 조금 더 빠르게 계산 가능\n\n\nPermutation Importance\n특성을 무작위로 재정렬해 모델 예측 오차가 증가하는 것을 관찰하는 방식으로 특성과 결과의 관계를 밝힘\n\n\nGlobal Surrogate\nAI모형의 예측 방식을 유사하게 학습한 해석가능한 모형으로 대리 설명. 의사결정 트리가 자주 사용됨\n\n\n국소설명\nLocal Surrogate (LIME)\n이미지나 텍스트를 포함한 다양한 데이터에 대해 임의의 판별 AI모형의 예측을 선형 근사로 설명.\n\n\nShapley Additive exPlanations (SHAP)\n각종 데이터에 대응하는 AI모형의 예측에 대해 특성의 공헌도를 게임이론적 지표를 이용해 설명.\n\n\n반사실적 설명\n현재의 판단 결과를 바꿀 수 있는 가장 작은 특성값의 변화를 이용해 현재 판단 결과의 ‘원인’을 설명.\n\n\n\n\n7.3.1 전역적 설명\n여기서는 Permutation Importance 방식과 Global Surrogate에 대해서 이야기 해 보도록 하겠다. Permutation Importance는 많은 전역적 설명 방식과 유사하게, 예측 결과를 도출하기 위한 근거가 되는 특성(feature)들의 상대적 중요도(importance)를 계산하는 것을 목표로 한다. 설명 방식은 다음과 같다. 가장 먼저, 중요도를 측정하기 위한 특성을 먼저 선택한다. 그 다음 다른 특성의 값들은 주어진 데이터 그대로 둔 채, 선택된 특성값만을 무작위로 재배열한다. 이를 그림으로 표현하면 다음과 같다.\n 모든 다른 특징량은 1, 2, 3 등 원데이터에 주어진대로 배열되어 있지만, 중요도를 측정하고자 하는 특징량 E는 무작위 배열되어 10, 94, 48와 같이 규칙을 갖지 않는 순서로 재배열되어 있다. 이제 이렇게 (특징량 E만) 변형된 데이터를 이용해 설명하고자 하는 모형을 추정한다. 모든 모형 추정은 ‘오류율’을 생산하는데, 이렇게 변형된 데이터를 바탕으로 한 모형의 오류율과, 변형하지 않은 원 데이터를 바탕으로 추정한 같은 모형의 오류율을 비교해보면 전자의 오류율이 클 것이다. 이는 위의 예에서 특징량E에 포함된 예측에 사용되어야 할 유용한 정보를 무작위 배열을 통해 삭제한 것이나 다름 없기 때문에, 더 적은 정보(변형된 데이터)를 가지고 추정한 모형이 더 많은 정보(원데이터)를 이용해 추정한 모형보다 정확하지 않을 것이라는 것을 생각해보면 납득이 갈 것이다. 이제 변형 데이터로부터 도출된 오류율과 원데이터로부터 도출된 오류율의 비율, 즉, 특정 특징량을 무작위 배열함으로써 ’증간한 오류의 양이 얼마인가’를 측정하면, 반대로 그것은 해당 특성이 얼마나 예측에서 중요한 역할을 하고있었는가를 나타내는 지표가 된다. 이러한 과정을 하나의 특성(위의 예에서는 ’특징량 E’) 뿐만 아니라, 모든 특성에 대해 반복하면, 각 특성의 중요도를 파악할 수 있게 되는 것이다.\n이러한 과정을 통해, 다음과 같이 각 특성의 중요도를 시각화할 수 있다.\n\n\n\n특성별 중요도 표현\n\n\n위와 같은 시각화는 각각의 특성이 해당 모형의 예측에서 얼마만큼 중요한 역할을 하는지를 표현해주기에, 예측 모형이 아무리 복잡하더라도 그 모형이 작동하는 방식을 대체로 이해하는데 도움을 준다. 또한, 위의 중요도 게산 방식은 어떠한 예측 모형을 사용하는가와 전혀 관계없이 수행할 수 있다는 점에서, 모형불문형 기술이라고 할 수 있다. 더 나아가, 중요도 계산에 이용하는 오류율은 사용하는 모형, 데이터와 관계 없이 같은 단위(unit)를 가지기 때문에, 예측 모형 간의 비교 분석을 수행하는데에도 유용하다.\n물론 이러한 전역적 설명 방식은 고유한 한계를 갖는데, 다수의 다른 특성들 사이의 예측에 있어서의 의존관계를 파악하는데 도움을 주지 못하며, 각 특성이 특정 영역에서는 큰 중요도를 가지다가 다른 영역에서는 중요도가 떨어지는 등 ‘비선형적’ 중요도를 갖는 경우에도 이를 파악하는데 도움을 주지 못한다. 전역적 설명 방식은 본질적으로 복잡한 모형 전체를 단순화하여 표현하는 접근방식이기에 피하기 어려운 단점이다.\n또 다른 전역적 설명방식으로 Global Surrogate을 들 수 있다. 여기서 Surrogate이라고 함은 대리 모형, 즉, 복잡한 모형에 대한 조금 더 단순한 근사 모형을 의미한다. Global Surrogate이 사용하는 근사모형은 특정한 모형으로 미리 정해져 있지 않고, 근사모형의 이용자가 쉽게 이해할 수 있다고 믿는 한, 어떤 모형이라도 가능하다. Global Surrogate의 개략적인 아이디어를 그림으로 표현하면 다음과 같다.\n\n\n\nGlobal Surrogate의 간단한 예\n\n\n즉, 실제 모형은 예측의 정확도를 높이기 위해 0과 1을 구분하는 구불구불한 곡선이라면, 이를 이해하는 것이 쉽지 않으므로, 가능한 비슷한 예측결과를 만들어 낼 수 있는 근사모형은 선형 모형을 추정하여, 대략적인 설명을 한다는 아이디어이다. 물론, 선형 모형일 필요는 없고, 의사결정 트리와 같은 다른 유형의 이해가능성이 높은 모형을 이용할 수 있다.\n이 역시 주 모형의 작동 방식을 알지 못하더라도 구축 가능하다는 점에서 큰 장점을 갖지만, 근사 모형이 주 모형으로부터 대단히 멀 수 있다는 단점이 있다는 것을 즉각적으로 알 수 있다. 가깝게 하기 위해서는 근사 모형 역시 유연하게 만들어야 할 것이고, 그렇게 되면, 결국에는 근사 모형 역시 설명이 어려워진다는 원래의 함정을 빠져들게 된다.\n전역적 설명은 알고리즘 자체의 사전적 설명과 투명성 관점에서는 더 의미가 있는 방식일 것.\n\n\n7.3.2 국소적 설명\n앞서 설명한 바와 같이 국소적 설명은 예측 모형 전체를 일반 사용자에게 이해시키기 위한 목적이라기 보다는, 인공지능이 해당 사용자에게 부여한 특정 예측, 또는 결정이 이루어진 이유를 제공하기 위한 목적을 가지고 있다. 이미 논의한 바와 같이, 일반 이용자가 쉽게 이해할 수 있을 정도로 전역적 설명 기술을 적용하기 위해서는 복잡한 모형을 지나치게 단순화해야 한다는 문제점이 있으므로, 해당 이용자가 관심을 갖는 특수한 사안으로 설명의 범위를 국한하는 국소적 설명 방식이 최근 각광받고 있다. 또한, 국소적 설명 방식은 기업의 특정 결정에 대해 설명을 요구할 권리를 보장하는 기존의 소비자 보호 법안의 취지에 잘 부합한다는 장점이 있다. 이 장에서는 최근 각광받는 국소적 설명 기술인 LIME (Local Interpretable Model-agnostic Explanations)와 반사실적(Counterfactual) 설명 방식에 대해 간단히 논의해 보도록 할 것이다.\nLIME은 Global Surrogate 방식과 유사하게 대리/근사 모형(Surrogat)을 이용한 설명 기술이다. 그러나 Global Surrogate과의 결정적인 차이는, 근사 모형을 특정 예측치 근방(locality 또는 neighborhood)에서 생성한다는 것이다. 이 점을 더 쉽게 이해하기 위해서 다음과 같은 분류 문제와 근사 모형을 생각해 보자.\n\n\n\n국소적 근사모형\n\n\n여기서 회색과 붉은색으로 표시된 영역은 예측을 위한 기계학습 모형(주모형)이 -1(회색) 또는 +1(붉은색)으로 예측하는 구간을 의미한다. 두 영역의 경계를 보면 알 수 있듯이 그 경계는 매우 비선형적이다. 이러한 비선형적인 모형이 만들어낸 예측결과는 ’특징량 1과 특징량 2의 수치가 어떠어떠하기 때문에 +1 또는 -1로 예측하였다’와 같은 방식으로 설명이 어렵다. 이 때문에, 이 불규칙해 보이는 경계 전체를 선형적인 모형으로 근사한다면(즉, Global Surrogate을 도입한다면), 지나친 단순화로 인해 좋은 근사모형을 만들어낼 가능성이 낮다.\n때문에, LIME은 전체 모형에 대한 근사 모형을 포기하고, 특정 위치에서의 근사 모형만을 생성한다. 예컨대 위의 그림에서 별표에 해당하는 어떤 예측치가 있었다고 하자. 이 예측치는 회색 영역에 위치해 있으므로, -1이라는 값을 가질 것이다. 좀 더 쉽게 이해하기 위해서 구체적인 시나리오를 도입하자면, 소득(특징량1)과 신용점수(특징량2)의 특정한 조합(별표) 때문에 해당 소비자는 신용카드 발급이 자동적으로 거절(-1)된 경우라고 생각해보자. 이 때, 해당 소비자는 본인에게 불리한 판정(더 정확하게는 신용 위험이 높다는 ‘예측’)이 이루어졌으므로, 그 이유를 알려달라고 요구할 수 있을 것이다. 이 때, LIME은 근사 모형을 형성하되, 해당 소비자가 위치한 그 근방에서 예측 모형을 생성하자는 접근이다. 이 근사 모형은 위의 그림에서 점선으로 표현되어 있다. 해당 점선은 전체적인 불규칙한 예측 경계에 대해서는 (즉, Global Surrogate으로서는) 나쁜 근사모형이지만, 별표로 표시된 특정 예측치 ‘근방에서만큼은’ 상당히 좋은 근사모형이라는 것을 알 수 있다. LIME을 수행하는 구체적인 알고리즘은 다음과 같다.\n\n첫째, 설명 대상 데이터(별표)에 무작위 오류를 더하여, 설명 대상 데이터 근방의 가상 데이터를 만들어낸다 (이는 위의 그림에서 점들의 ’위치’로 표현되어 있다). 이를 기술적으로는 설명 대상 데이터를 섭동(perturbation)한다고 한다.\n둘째, 가상 데이터를 ’주모형’에 삽입하여 가상 데이터에 대한 AI 모형의 예측 결과를 얻는다 (위의 그림에서 예측 결과는 가상 데이터의 색깔에 해당한다).\n셋째, 가상 데이터와 그에 대한 예측결과를 이용하여, 비교적 단순한 근사모형을 만든다.\n\n이러한 방식을 이미지에 적용하면, 다음과 같은 재미있는 결과를 얻을 수 있다.\n 이는 LIME을 처음 제시한 Ribeiro 등의 논문(2016)에 수록된 것으로, 특정 오류에 대해서 근사모형을 만들어 보니, 허스키와 늑대를 구분하는 경계가 배경에 해당하는 영역이었다는 것을 알 수 있었다는 예시이다. 즉, 대상의 형태에 따라 허스키와 늑대를 오해한 것이 아니라, 배경의 눈 때문에 늑대라고 생각했다는 것이다.\n중요한 점은, LIME의 이러한 설명이 모형 전체가 그러한 오류를 가지고 있다는 것을 의미하지는 않는다는 것이다. 특정 오류에 대한 설명일 뿐이다.\nRibeiro, M. T., Singh, S., & Guestrin, C. (2016, August). ” Why should I trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144).\nLIME 역시 surrogate을 이용. 반사실은 요즘 핫한 주제!"
  },
  {
    "objectID": "xai.html#설명가능성과-법안과의-연관성",
    "href": "xai.html#설명가능성과-법안과의-연관성",
    "title": "7  설명 가능한 알고리즘",
    "section": "7.4 설명가능성과 법안과의 연관성",
    "text": "7.4 설명가능성과 법안과의 연관성\n아직 논란이 되고 있는 부분이라 (법학자들 사이에서) 자세히 논의하기는 어렵다. 하지만… EU - 설명가능성이 권리로 포함되어 있는가? (Edwards & Veale, 2018) US - 인공지능에게 요구하지 않은 기존의 법을 확장할 수 있는가.\n알고리즘 등록제 - ex ante -&gt; 이건"
  },
  {
    "objectID": "xai.html#더-읽을거리",
    "href": "xai.html#더-읽을거리",
    "title": "7  설명 가능한 알고리즘",
    "section": "7.5 더 읽을거리",
    "text": "7.5 더 읽을거리\n\n인터넷 교재\n인공지능 개론인가? (그 검은 책…)\n\n인공지능의 실패 -&gt; 예외상태 (슈미트, 발리바르, 등등등) -&gt; 예외상태가 주체성을 결정하는 것이 아닐까. 노동.\n책임의 분배.\n인공지능의 책임성…도덕적으로나, 피해 보상으로나, 책임성을 따지는 것은 매우 어렵다. 하지만, 그 전에 전제되어야 하는 것이 있다. 어떠한 판단을 했는가 하는것. 그것이 책임있는 인공지능, 혹은 책임있는 인공지능과 인간의 네트워크의 충분조건은 될 수 없을 지언정, 필요조건인 것은 사실.\n책임성 있는 인공지능…\n결정을 인공지능에 맡길 수록…. 원래 결정은 모두 인간이 하지 않았다 - 사실은 이에 대해서 많은 법들이 있어왔다. 인간이 하든, 하지 않든… 하지만, 문제는 더욱 복잡해졌다는 것.\n또, 인간과의 협업을 위해서…\n설명 가능한 인공지능 v. 설명을 위한 인공지능."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barocas, Solon, and Andrew D Selbst. 2016. “Big Data’s Disparate\nImpact.” California Law Review, 671–732.\n\n\nDwork, Cynthia, Moritz Hardt, Toniann Pitassi, Omer Reingold, and\nRichard Zemel. 2012. “Fairness Through Awareness.” In\nProceedings of the 3rd Innovations in Theoretical Computer Science\nConference, 214–26."
  }
]