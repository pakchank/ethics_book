[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "인공지능 시대의 윤리 또는 윤리적 인공지능 - 기술, 정책, 철학",
    "section": "",
    "text": "Preface\n공학자들도, 인공지능을 조금 더 윤리적으로 만들기 위해 나름의 해결책을 제시하기 위해 노력하고 있다. 이들의 목적은 인공지능의 판단 결과를 조금 더 공정하고, 설명가능하고, 프라이버시 보호하는 방향으로 만들기 위해 데이터 수집, 처리 방법, 사후 처리, 알고리즘 설계 등에 개입하는 것. 이러한 작업을 하기 위해서는 위와 같은 개념들은 수학적으로 정의할 수 있어야 한다. 실제로 상당한 성과를 만들어냈으며, XAI는 협업을 더 잘 가능하게 하고, FML은 법적 처방과, 차분 프라이버시는 실제 거대 테크기업들의\n또한, 이러한 해법의 부산물로, 우리는 윤리적 입을 위한 개념들, 특히 윤리성을 의미하는 공정성 등의 용어 안에 얼마나 다층의 개념들이 모호하게 뒤섞여 있었는지를 알게될 것이다…\n물론 이러한 방법들이 만병통치약은 아니다. 앞선 챕터들의 논의한 윤리적 문제들과 화합되지 않는 측면들을 살펴보자.\n\n공정거래 - 이렇게 네 가지! (리나 칸, 팀 우 등에 대해서도….)\nXAI\nFML\nPrivacy\n\n인공지능, 빅데이터, 기계학습의 관계에 대해서 쓰기…"
  },
  {
    "objectID": "labor.html#서론",
    "href": "labor.html#서론",
    "title": "1  인공지능과 노동",
    "section": "1.1 서론",
    "text": "1.1 서론\n1800년대 초공장 자동화가 노동을 대체할거라고 생각하여 기계를 파괴한 러다이트(Luddites) 운동을 시작으로 기술 혁신은 역사적으로 근로자들 사이에 일자리 대체에 대한 불안을 불러일으켜 왔다. 로봇 등 일자리 자동화가 인간 노동을 대체할 것이라는 두려움은 일부 현실이 되기도 하고 기술 혁신으로 인해 다른 부문의 노동 수요를 자극하여 새로운 일자리를 만들어내기도 했다. 그렇다면 인공지능(최근 생성형 인공지능으로 인해 촉발된) 기술 혁신은 노동 시장에 어떤 영향을 미칠까?\n노동 시장에 대한 혁신의 효과를 정확하게 측정하는 것은 매우 복잡한 문제이다. 제2차 세계대전 이후 미국은 40년 동안 혁신으로 인해 노동 수요가 강화되었고, 이후 40년 동안 성장이 둔화되었다. 전기, 내연 기관과 같은 2차 산업 혁명의 주요 발명품은 복제할 수 없을 만큼 엄청난 경제적 영향을 미쳤다. 연구에 따르면 제품 혁신(새롭거나 향상된 제품/서비스)은 프로세스 혁신(제품/서비스 생성 방식의 개선)보다 생산성과 경제적 성과를 더 높이는 경향이 있다. 노동 시장을 연구하는 학자들에 따르면 노동력을 강화(labor-augmenting)거나 인간이 수행하지 않거나 수행할 수 없는 작업을 수행하는 데 초점을 맞춘 기술 혁신은 결국 총 노동 수요를 자극하여 더 많은 일자리의 창출로 이어진다고 주장한다. 이러한 혁신은 프로세스 혁신보다 제품 혁신과 더욱 밀접하게 연계된다고 보았다. 반대로, 인공지능과 같은 기존 인간의 작업 수행에 초점을 맞춘 노동 자동화 기술은 프로세스 혁신과 더 유사하며 총 노동 수요를 감소시킬 가능성이 더 높다.\n하지만, 혁신을 순전히 제품이나 프로세스, 노동 증대 또는 노동 자동화로 분류하는 것은 복잡한 문제를 너무 단순하게 푸는 것과 같다. 예를 들어, 자동차 자체는 제품 혁신일 뿐만 아니라 운송이라는 과정에 대한 프로세스 혁신이기도 하기 때문이다. 혁신이 노동에 미치는 영향은 대체(자동화를 통해 노동 수요 감소), 회복(새로운 작업 창출 및 그에 따른 노동 수요 창출), 생산성 효과로 분류된다. 노동 수요에 대한 혁신의 전반적인 효과는 생산성 효과의 규모에 따라 결정되는데, 이는 높은 임금의 노동을 혁신이 대체할 때 가장 강력하다고 할 수 있다.\n인류는 현재 인공지능(AI)이라는 거대한 혁명의 중심에 서 있다. 이 혁명은 단순히 기술 발전을 넘어 우리의 일상생활, 직업의 본질, 심지어 우리가 자신과 세계를 인식하는 방식까지 근본적으로 변화시키고 있다. 따라서 인공지능이 노동 시장에 미치는 효과는 개인의 생계, 기업의 운영 방식, 국가 경제의 구조에 이르기까지 매우 광범위할 수 밖에 없다. 이 변화를 이해하는 것은 단순한 호기심, 즉 ’인공지능이 어떤 직업을 대체할 수 있을까?’에 대한 답을 찾는 것을 넘어, 우리 모두에게 미래 사회를 준비하는 실질적인 필요성이 되다. 본 챕터에서는 ‘인공지능과 노동’이라는 주제를 통해 인공지능의 발전이 사회에 미치는 영향을 깊이 있게 탐구하고자 한다.\n먼저 인공지능에 대한 기본적인 이해부터 시작해보려고 한다. 인공지능은 컴퓨터나 기계가 인간의 지능적인 행동을 모방하고, 복잡한 문제를 해결하며, 의사결정을 내릴 수 있는 고도의 기술이다. ****이 기술은 이미 여러분의 일상 속 깊숙이 자리 잡고 있다. Siri와 Bixby와 같은 스마트폰의 개인 비서 기능부터 의료 분야의 진단 시스템, OTT의 맞춤형 콘텐츠 추천, 금융 서비스의 자동화된 고객 상담까지 다양한 형태로 우리 삶에 영향을 미치고 있다.\n하지만 인공지능이 인류에 가장 큰 영향을 주는 영역은 노동 시장일 것이다. 일부는 인공지능을 일자리를 파괴하는 위협으로 보지만, 다른 이들은 기존의 일을 혁신하고 새로운 기회를 창출하는 동력으로 여기곤 한다. 실제로 인공지능은 특정 작업을 자동화함으로써 일부 직업을 대체할 수 있으나, 동시에 새로운 직업을 창출하고, 기존 직업의 효율성을 높이며, 전례 없는 혁신을 가져올 수 있다.\n\n예를 들어, 의료 분야에서는 인공지능이 다양한 방식으로 활용되고 있다.인공지능은 환자의 위치를 정확하게 파악하고 CT 이미지를 재구성하는 데 도움을 주어 방사선과학과의 효율성을 향상시키거나, 심장 기능을 평가하는 데 사용되는 초음파 측정의 복잡성을 줄이는 데 도움을 주고 있다.1\n\n\n제조업에서는 인공지능이 생산 과정을 최적화하고 불량률을 줄이는 데 기여하고 있다.예를 들어, 지멘스(Siemens)는 인공지능과 기계 학습을 활용하여 산업 자동화와 데이터 분석을 개선하고 있고, IBM은 클라우드, 인공지능, 기계 학습 도구를 제공하여 생산 시간과 비용을 줄이는 데 도움을 주고 있다.2\n\n그러나 인공지능의 발전이 가져오는 변화는 단순히 기술적인 측면에 그치지 않는다. 우리는 어떤 일자리가 사라지고, 어떤 일자리가 변화할지, 그리고 새로운 일자리는 어떻게 생겨날지를 예측하고 대비해야 한다. 이 과정에서 정부, 기업, 교육 기관, 개인 등 모든 이해관계자의 역할이 매우 중요하다. 정부는 적절한 정책과 규제를 통해 인공지능의 긍정적인 측면을 촉진하고 부정적인 영향을 최소화해야 하며, 기업은 직원들의 재교육과 재취업 지원 프로그램을 제공하여 변화하는 시장 요구에 부응해야 한다. 또한, 교육 기관은 학생들에게 미래의 노동 시장에 필요한 기술과 지식을 제공하여 이러한 변화에 대비할 수 있도록 해야 한다. 마지막으로 우리 모두 개인 차원에서 이러한 변화를 단순히 관찰자의 입장에서 보는 것이 아니라, 적극적으로 이해하고, 준비하며, 올바른 방향으로 이끌어 갈 필요가 있다."
  },
  {
    "objectID": "labor.html#인공지능의-진보와-일자리-자동화",
    "href": "labor.html#인공지능의-진보와-일자리-자동화",
    "title": "1  인공지능과 노동",
    "section": "1.2 인공지능의 진보와 일자리 자동화",
    "text": "1.2 인공지능의 진보와 일자리 자동화\n\n1.2.1 인공지능 기술의 급격한 발전과 현재의 다양한 적용\n많은 사람들이 처음으로 인공지능이 대단하다고 느끼는 순간은 대체로 인공지능이 체스나 바둑과 같은 게임에서 인간 챔피언을 이기는 장면이었을 것이다. 예를 들어, 1997년 IBM의 ’딥 블루’가 세계 체스 챔피언 가리 카스파로프를 이겼고, 2016년에는 Google의 ’알파고’가 바둑의 세계 챔피언 이세돌을 이겼다. 물론 이세돌 9단은 알파고와의 4번째 대국에서 ’신의 한수’를 만들어 승리했지만, 이러한 순간들은 인공지능이 단순한 계산을 넘어 인간의 전략적 사고를 모방하고, 때로는 뛰어넘을 수 있는 능력을 가지고 있음을 전 세계에 알리는 신호탄이 되었다.3\n그러나 실제로 인공지능은 체스나 바둑과 같은 게임에서의 승리에 그치지 않고, 이미 우리 일상생활에 깊숙이 침투해 있다. 예를 들어, 음성 인식 기술을 통해 우리는 스마트폰이나 가정용 스마트 스피커에게 명령을 내릴 수 있다. Siri, Alexa, Google Assistant와 같은 AI 비서들은 단순한 명령 수행에서부터 일정 관리, 뉴스 업데이트 제공, 심지어 감성적인 대화에 이르기까지 다양한 작업을 수행하고 있다. 이러한 기술들은 우리의 생활 방식을 변화시키며, 더 편리하고 연결된 삶을 가능하게 한다.\n의료 분야에서 AI의 역할은 더욱 중요해지고 있다. AI는 환자 데이터를 분석하여 질병을 진단하고, 적절한 치료 방법을 제안하며, 심지어 새로운 약물을 개발하는 데 기여하고 있다. 예를 들어, 구글의 DeepMind는 ‘알파폴드’ 프로그램을 통해 단백질의 3D 구조를 예측하는 데 혁명적인 진전을 이루었다. 이러한 발전은 맞춤형 의학과 치료법의 개발을 촉진시켜, 더 효과적이고 개인화된 의료 서비스를 제공할 수 있게 한다.4\n자동차 산업에서는 자율 주행 차량의 등장이 가까운 미래의 현실로 다가오고 있다. Tesla, Waymo, Uber와 같은 회사들은 이미 도로에서 자율 주행 테스트를 진행하고 있다. 이 기술이 완전히 성숙되고 널리 보급되면, 교통 사고의 감소, 교통 체증의 해소, 운전자의 편의 증진 등 많은 긍정적인 변화를 가져올 것으로 기대된다. 자율 주행 차량은 또한 이동성에 제한이 있는 사람들에게 새로운 기회를 제공하고, 물류 및 배송 산업에도 혁신을 가져올 것이다.\n\n\n1.2.2 인공지능이 일자리와 직업 구조에 미치는 광범위한 영향\n일부 전문가들은 인공지능을 ’일자리 파괴자’로 보는데, 특히 반복적이고 예측 가능한 작업을 수행하는 직업들이 위험에 처해 있다고 지적한다. 예를 들어, 제조업에서는 로봇과 AI 시스템이 인간 노동자를 대체하여 더 빠르고 정확하게 작업을 수행할 수 있는데, 이러한 자동화는 생산성을 크게 향상시킬 수 있지만, 동시에 기존의 일자리를 줄일 수 있는 이중적인 특성을 가지고 있다. 고객 서비스 분야에서는 AI 챗봇이 기본적인 문의 사항을 처리하고, 효율적인 서비스를 제공하면서, 전통적인 콜센터 직원의 역할을 변화시키고 있다.5\n반면, 다른 전문가들은 인공지능을 ’기회의 창출자’로 보고, AI가 담당할 수 있는 반복적인 작업들을 자동화함으로써, 인간 노동자들이 더 창의적이고 전략적인 작업에 집중할 수 있게 될 것이라고 주장한다. 예를 들어, 프롬프트 엔지니어, 인공지능을 활용한 콘텐츠 기획자와 같은 새로운 직업들이 등장하고 있다. 이러한 직업들은 기술 발전의 이점을 활용하며, 경제와 사회에 새로운 가치를 창출한다. 또한, 의료 분야에서 AI가 의사와 간호사의 진단과 치료 결정을 보조함으로써, 의료 서비스의 질을 향상시키고, 환자에게 더 나은 치료를 제공할 수 있게 되었다. 이러한 변화는 기존의 직업을 더 효율적이고 효과적으로 만들며, 새로운 전문 분야와 기회를 창출할 것이다.6\n\n\n1.2.3 자동화가 가능한 일자리는 인공지능으로 인해 대체 가능하다\n일자리 자동화는 인공지능(AI)과 로봇공학의 발전으로 인해 특정 작업이 기계에 의해 수행되는 현상을 말한다. 이 현상은 제조업을 시작으로 의료, 금융, 서비스업에 이르기까지 다양한 분야에서 점점 더 두드러지게 나타나고 있다. 자동화의 가장 대표적인 예로는 자동차 제조업에서의 로봇 팔 사용을 들 수 있다. 이 로봇들은 사람보다 훨씬 빠르고 정확하게 차량을 조립하고, 무거운 부품을 들어 옮기며, 위험한 작업을 수행하여 인간 노동자의 부상 위험을 줄인다. 그러나 이러한 기계의 도입은 전통적인 조립 라인에서의 수많은 일자리를 줄이는 결과를 가져왔다. 사람들은 이제 더 복잡하고 창의적인 업무에 집중할 수 있게 되었지만, 동시에 많은 사람들이 기존의 일자리를 잃게 된 것이다.\n서비스업에서의 자동화는 주로 ’소프트웨어 로봇’이나 AI를 통해 이루어지고 있습니다. 예를 들어, 은행업계에서는 전통적인 창구 직원의 역할이 ATM과 온라인 뱅킹 시스템에 의해 대체되고 있다. 이러한 기계는 24시간 동안 고객의 요구에 응답할 수 있으며, 오류의 가능성을 줄이면서 많은 루틴 업무를 처리할 수 있다. 더 나아가, AI 기반의 챗봇은 고객 문의에 대응하고, 간단한 금융 상담까지 제공할 수 있는데, 이는 고객 서비스의 효율성을 높이고, 인간 직원이 더 복잡하고 전략적인 작업에 집중할 수 있게 한다. 그러나 이러한 변화는 전통적인 고객 서비스 직종의 일자리 수를 줄이는데 큰 기여를 하고 있다.\n\n\n1.2.4 자동화로 인해 기존 인간의 역할이 바뀔 것이다\n한편, 기존의 자동화가 가능한 일부 직업들은 인공지능에 의해 대체되기보다는 역할이 변화하는 형태로 진화할 것이다. 전통적으로 회계사는 기본적인 계산과 데이터 입력과 같은 반복적인 작업을 수행해왔다. 하지만 이제 이러한 작업은 인공지능에 의해 자동가 가능하다. 그렇다고해서 회계사라는 직업이 사라지지는 않는다. 오히려 회계사로 하여금 더 복잡한 분석, 전략적 의사결정 지원, 고객 상담과 같은 더 높은 수준의 작업에 집중할 수 있는 기회를 제공할 것이다. 인공지능은 데이터를 빠르게 처리하고 데이터 베이스에 저장할 수 있는 능력을 가지고 있지만, 그 데이터 안에서 의미 있는 정보를 분석하고 이를 사용자(회계사)에게 제공하는 능력은 제한적이다. 회계 전문가들은 이러한 인공지능의 능력을 활용, 기업의 방대한 데이터를 분석하여 중요한 통찰력을 얻을 수 있으며, 이 정보를 기반으로 보다 전략적인 결정을 내릴 수 있다. 더 나아가, 회계 전문가들은 이 과정에서 얻어진 자료를 바탕으로 조직의 자문 역할을 강화할 수 있게 된다. 특히, 블록체인 기술과 일부 AI 도구를 활용함으로써, 회계 감사의 안전성과 보안성이 크게 향상될 것으로 기대된다. 이러한 기술의 도입은 회계 업무의 효율성과 정확성을 높이는 동시에 조직 전반의 신뢰성을 증진시킬 것이다.이처럼 자동화는 기존 직업의 역할을 변화시키고, 직업을 보다 전문화되고 창의적인 방향으로 발전시킬 수 있는 잠재력을 가지고 있다.7\n\n\n1.2.5 취약한 직업군과 견고한 직업군\n인공지능의 위협을 가장 많이 받는 직업들은 일반적으로 높은 수준의 반복적 작업을 포함하고 예측 가능한 환경에서 일하는 직업들이다. 이러한 직업군에는 제조업 노동자, 콜센터 직원, 일부 사무직, 그리고 운전 기반의 직업들이 포함된다. 이들 직업에서는 AI나 로봇이 인간보다 더 빠르고 정확하게 작업을 수행할 수 있으며, 피로나 오류의 위험이 없다는 장점이 있다. 따라서 이러한 직업들은 인공지능이 대중화될 수록 사라질 위험이 높은 취약한 직업군이라고 할 수 있다.\n반면, 창의성, 복잡한 사회적 상호작용, 고도의 비판적 사고를 요구하는 직업들은 AI가 쉽게 대체하기 어려운 영역이다. 예술가, 과학자, (직관을 통해) 전략적 결정을 내리는 관리자 등은 인간만의 독특한 능력을 필요로 하는 영역에서 활동하며, 이러한 직업들은 AI의 발전에도 불구하고 여전히 중요하고 가치 있는 역할을 수행할 것이다."
  },
  {
    "objectID": "labor.html#생성형-인공지능과-일자리",
    "href": "labor.html#생성형-인공지능과-일자리",
    "title": "1  인공지능과 노동",
    "section": "1.3 생성형 인공지능과 일자리",
    "text": "1.3 생성형 인공지능과 일자리\n파운데이션 모델(Foundation model)을 사용하여 텍스트, 이미지, 코드와 같은 새로운 콘텐츠를 생성하는 기술인 생성형 인공지능은아직 초기 단계이지만 노동 시장에 미치는 잠재적 영향은 상당할 것이다. 1980년 이후 혁신은 저임금 및 중임금 일자리 자동화에 중점을 두어 상대적으로 낮은 비용 절감으로 인해 생산성 효과가 제한되었지만 생성형 인공지능으로 촉박된 노동 시장의 반응은 고임금 근로자에게 더 높은 위험을 초래하고 잠재적으로 상당한 비용 절감으로 이어져 결과적으로 더 강력한 생산성 효과와 총 노동 수요를 증가시킬 수 있다는 점에서 기존의 기술 혁신발 노동 시장의 변화와는 결이 다를 것으로 예상된다.\n생성형 인공지능은 일반적으로 노동 수요 증가 가능성이 낮은 프로세스 혁신으로 분류된다. 이는 상당한 생산성 효과를 가져옴과 동시에 많은 직업이 사라질 수 있음을 시사한다. 범용 기술인 생성형 인공지능은 코드 작성부터 사기 탐지(Fraud detection)까지 다양한 산업과 작업에 걸쳐 광범위한 영향을 미치며 생산성 향상을 이루어낼 것으로 기대되고 있다.\n이때, 인구 통계학적 변수와 국가마다 다르게 진화하는 인공지능에 대한 제도들은 생성형 인공지능이 노동 수요에 미치는 영향을 다르게 만들 수 있는 요소들이다. 노동력 부족은 자동화를 장려하며, 이는 노동력이 노령화되거나 감소하는 국가에서 생성형 인공지능의 채택과 응용이 더 많아지고 결과적으로 생산성 효과가 더 커질 수 있음을 시사한다. 그러나 근로자 보호와 노조의 힘이 강한 제도를 가진 국가(또는 지역)에서는 인공지능의 노동 대체 효과가 약해 직업적 업무가 보다 순차적으로 전환되고 단순히 업무를 자동화하는 대신 기존 고용된 일자리 노동자들의 생산성 향상을 위해 생성형 인공지능을 사용하는 데 중점을 둘 것이다. 노동자 보호가 강화되면 비용 절감 효과가 낮아 국가 전반적인 생산성 효과가 감소할 수 있지만, 노동 시장에서 보다 균형감 있고 지속 가능한 전환으로 이어질 수도 있다.\n생성형 인공지능은 노동 시장에서 불평등(Inequality)에 영향을 미칠 수 있다. 고소득 직종과 산업이 가장 위험에 처해 있기 때문에 이들 근로자를 대체하면 처음에는 불평등이 줄어드는 것 처럼 보일 수도 있을 것이다. 연구에 따르면 생성형 인공지능을 도입하면 상위 1%에 속하는 사람들은 큰 영향을 받지 않을 수도 있지만 상위 1%와 10% 사이의 소득 격차는 줄어들 수 있다고 한다. 고소득 기술을 AI로 대체하면 저임금 기술에 대한 수요와 임금이 증가하여 임금 격차가 더욱 줄어들기 때문이다. 그러나 이것이 보편적인 불평등 완화로 이어질지는 미지수이다. 생성형 인공지능으로 대체되는 고소득 직종이 많이 분포한 지역은 경제적, 사회적으로 어려움을 겪을 수 있기 때문이다.8"
  },
  {
    "objectID": "labor.html#인공지능이-불러온-사회-변화-트렌드",
    "href": "labor.html#인공지능이-불러온-사회-변화-트렌드",
    "title": "1  인공지능과 노동",
    "section": "1.4 인공지능이 불러온 사회 변화 트렌드",
    "text": "1.4 인공지능이 불러온 사회 변화 트렌드\n\n1.4.1 고용 없는 노동의 현실화\n현대 산업에서 자동화와 인공지능의 급속한 발전은 노동 시장에 본질적인 변화를 가져오고 있다. 기계와 소프트웨어의 작업 복잡성 증가로 인해 다양한 산업에서 여러 수준의 자동화가 도입되고 있으며, 이러한 인공지능으로 촉발된 디지털 전환이 고숙련 일자리 증가 및 저숙련 일자리 감소로 이어지는 숙련 편향적 기술 변화를 초래하고 있다(Balsmeier & Woerter, 2019). 국내에서도 약 55~57%의 일자리가 향후 컴퓨터로 대체될 가능성이 높은 것으로 나타나며, 일부 연구에서는 이 비율이 70.6%에 달한다고 보고되었다(김세움 2015; 박가열 외 2016)\n자동화, 로봇화, 컴퓨터화의 발전은 반복업무기반(routine-based) 직업의 인력 수요 감소를 예측하며, 특히 교통, 유통, 사무직 등이 디지털 전환에 따라 높은 위협을 받는 분야로 분석됩니다. 미국에서는 약 47%의 일자리가 고위험군에 속한다고 보고되었다(Frey & Osborne, 2017). 그러나 동시에 프로그래밍, 데이터 분석과 같은 비반복적 인지 기반 직무의 일자리는 증가할 것으로 예측된다(Acemoglu & Restrepo, 2018).\n인공지능은 또한 노동의 질적 측면에서도 변화를 가져온다. 특히 플랫폼 노동의 증가는 새로운 형태의 일자리와 노동거래 방식을 등장시켰으며, 이러한 경제는 네트워크를 통한 직접적인 일자리 중개 및 배정에 관여한다. 플랫폼 노동은 서비스 또는 가상재화 거래, 디지털 플랫폼을 통한 일거리 구함, 대가나 보수 중개, 그리고 특정인이 아닌 다수에게 열려 있는 일감 중개 등을 특징으로 한다. 이러한 경제는 코로나19의 영향으로 더욱 활성화되었으며, 특히 광의의 플랫폼 종사자 수가 2021년 220만 명에서 2022년 292만 명으로 증가하기도 하였다(고용노동부, 2022).\n플랫폼 노동의 확대는 고용 없는 노동의 장기적인 경제성장과 효율성 증가 잠재력을 가지고 있지만, 이는 또한 사회 안전망의 변화와 교육 체계에 대한 새로운 요구를 제기한다. 플랫폼 노동자에 대한 보호 제도 관련 논의가 발전했지만, 전통적 일자리에서 플랫폼 일자리로의 이동은 노동시장 내 인력 수급 및 운용 문제를 야기할 수 있다. 이러한 노동 시장의 편차와 불균형에 대응하기 위해서는 인간의 삶의 질을 향상시키는 지속 가능한 발전을 중심으로 한 정책적 방안이 필요하다(민순홍, 2023).\n또한, 알고리즘에 의한 노동자 통제와 관련한 논의도 중요하다. 최근 인공지능 플랫폼 노동에 일감을 배분하고 통제하는 데 인공지능 알고리즘을 적용하는 형태가 등장했기 때문이다. 이는 다이내믹 프라이싱(Dynamic pricing)을 통한 이윤 극대화 목적으로 활용되지만, 노동과 관련한 공정성 및 투명성 측면에서 문제가 제기되고 있다(장진희·노성철·현종화, 2022).\n결론적으로, 디지털 전환에 따른 다양한 노동 변화에 대응하기 위해서는 정부와 기업 간의 적절한 정책적 협력이 필수적이다. 기술 자동화가 대체하는 것은 ’직업’이 아니라 ’직무’임을 인식하고, 새롭게 등장하는 일자리에 대한 다방면의 고려가 필요하다. 또한, 플랫폼 노동과 관련하여 윤리적인 기술 활용과 적절한 규제의 중요성을 인식하고, 이에 대한 논의를 적극적으로 이어나가야 한다.\n\n\n1.4.2 인간노동 대체의 소비시대\n현대 소비자는 편의와 개인화된 서비스를 요구하며, 특히 코로나19와 인공지능의 발달로 인한 비대면화와 온라인화는 디지털 전환을 가속화하고 있다. 이는 전통적인 소비 패턴의 변화뿐만 아니라, 새로운 서비스와 비즈니스 모델의 등장을 촉발시켰다. 인공지능과 로봇은 판매부터 로지스틱까지 다양한 분야에서 활용되며, IoT·AI의 발달로 즉각적이고 개인화된 소비자 참여가 가능해졌다. 이에 따라 구매 채널 방식도 옴니채널이나 온디맨드 서비스 등으로 전환되는 추세다(한국마케팅연구원, 2022).\n유통업계는 AI를 활용한 ‘초개인화’ 서비스를 선보이며 소비자의 취향과 생각을 분석해 상품과 서비스를 제공하고 있다. 이는 ‘나만을’ 위한 맞춤 상품 소비를 가능하게 하며, 기업들은 구매 전환율을 높이고 차별화된 쇼핑 환경을 제공함으로써 시장에서 경쟁력을 확보하고자 한다. 이러한 디지털 전환은 노동과 소비시장의 구조 변화를 가져온다. 알고리즘 소비의 상대적 유익성, 소비자가 알고리즘 시장에서 가지는 지배력, 비용 절감이나 효용 비율이 소비자의 후생에 미치는 영향 등이 중요한 요소다. 알고리즘 도입이 증가할수록 과거의 노동력 투입이나 인지적 노력보다 더욱 적은 비용으로 빠르고 효과적인 삶의 수행이 가능해진다.\n인공지능 알고리즘은 경제활동에서의 비효율을 축소해 소비효용을 개선할 것으로 전망된다. 이는 정보를 낮은 비용으로 분석해 선택 다양성을 확보하고, 소비자 의사결정을 보완하며, 개인 특성에 따른 편향성을 감소시키는 등의 방식으로 이루어진다(Picht and Freund ,2018). 디지털 전환에 따른 소비 패턴의 재편은 개인적 측면뿐만 아니라 지역 및 사회 전반에 영향을 미칠 수 있다. 개인별 맞춤 가격 설정은 사업자에게는 새로운 기회를 창출할 수 있으나, 소비자 잉여의 축소와 같은 부정적 측면도 있다(허민영, 임병권, 2021). 기술 발전과 정보량 증가에 따라 알고리즘이 개인의 지불의사가격에 근접해 가격을 설정하는 사업모델이 확대될 것으로 예상된다.\n이러한 변화는 사회적 책임과 윤리를 중시하는 새로운 요구사항을 제기한다. 개인화된 가격으로 인한 소비자 이익 감소, 가격의 공정성과 시장의 신뢰, 기업의 개인정보 활용 범위와 윤리성 등 새로운 소비자 문제를 검토할 필요가 있다. 또한, 인공지능 알고리즘이 수집하는 개인정보와 그것의 활용에 대한 윤리적, 규범적, 철학적 고민이 필요하다.\n인공지능 시대의 소비 패턴 변화에 대응하기 위해서는 기업과 정부 간 협력이 필수적이며, 소비자 보호를 위한 적절한 규제가 중요하다. 이러한 미래의 지능정보사회에서 나타날 법적 이슈는 소비자의 안전을 보장할 개인정보 보호 강화와 새로운 산업에 대한 규제 완화 요구라는 상호 모순된 법제적 측면이 병존할 수 있다. 인공지능 알고리즘을 활용한 시장의 발전은 정보규율에 의해 결정되며, 개인정보의 수집 및 활용과 보호 사이에 존재하는 상충관계를 균형잡는 것이 중요하다.\n\n\n1.4.3 유목적 관계의 시대\n글로벌화와 디지털 기술 발전은 국가 간 경제 통합을 촉진하고 글로벌 네트워크를 가능하게 했다. 이는 다문화와 다양성을 중요한 가치로 부각시키며 사람들 사이의 연결을 강화했다. 최근 글로벌 대기업들은 다양성을 중요한 가치로 여기며 ’다양성 보고서’를 작성하는 등 인력 구성에 있어 연령, 성별, 민족, 성적 취향 등의 다양성을 존중하고 있다. 매킨지리포트에 따르면 다양성이 높은 기업은 그렇지 않은 기업보다 재무 수익이 높다고 한다(McKinsey, 2015). 이는 다양성이 기업 경쟁력으로 이어지기 때문이다.\n원격 노동과 프리랜서 경제는 노동 시장의 유연성을 증대시켰다. 교육과 업무 스킬의 전세계적 적용은 인력 이동을 촉진하고, 이민과 인력 이동은 새로운 경제 기회를 제공한다. 디지털 시대에는 컴퓨터의 등장으로 일하는 방식이 근본적으로 변화했으며, 직원들은 서로 다른 연관된 업무를 비동시적으로 일할 수 있게 되었다. 이는 근무의 유연성을 높이고 공간에 대한 종속성을 완화시켜 ‘디지털 노마드’ 현상을 가져왔다(정민재·이정교, 2018).\n가상 커뮤니티와 사회적 네트워킹은 협력과 공유 문화의 확산을 촉진하며 사람들의 삶의 질을 향상시켰다. 이는 모바일 기반의 전자상거래의 편재성, 편리성, 즉흥성, 경제성 등에서 확인할 수 있듯이 시간과 공간의 제약에서 벗어나 다양한 가치를 제공하는 구매채널로 자리 잡았다(김경희, 2018).\n유목적 관계의 시대는 국제 협력과 다자간 협상의 필요성을 강조하며, 정보 보안과 개인 정보 보호가 중요한 이슈가 되었다. 클라우드, AI 등의 기술 적용과 비대면 업무 등이 일상화되어가고 있지만, 사이버 위협도 빠르게 증가하고 있다. 예를 들어 줌바밍 해킹 사건처럼 화상 회의 중에 해커가 침투하여 내부 정보를 빼돌리거나 불법 게시물을 올리는 사건이 문제가 되었다. 이러한 취약성은 회사 내부의 중요 자료 유출에 대한 우려와 통제되지 않는 위험성을 초래한다. 따라서 디지털대전환시대의 소비 패턴 변화에 대응하기 위해서는 기업과 정부 간 협력이 필수적이며, 소비자 보호를 위해 적절한 수준의 규제가 중요하다. 이와 더불어 인공지능 알고리즘을 활용한 시장의 발전은 정보규율에 의해 결정되며, 개인정보의 수집 및 활용과 보호 사이에 존재하는 상충관계를 균형잡는 것이 중요하다."
  },
  {
    "objectID": "labor.html#논의와-대응책",
    "href": "labor.html#논의와-대응책",
    "title": "1  인공지능과 노동",
    "section": "1.5 논의와 대응책",
    "text": "1.5 논의와 대응책\n\n1.5.1 다양한 관점에서의 논의\n인공지능(AI)과 노동에 대한 논의는 사회의 다양한 분야와 각자의 전문 분야에 따라 다른 관점을 제공한다. 경제학자들은 인공지능이 경제 성장과 생산성, 그리고 노동 시장에 미치는 영향을 분석한다. 이들은 AI가 일자리를 대체할 가능성, 새로운 일자리 창출, 임금 격차와 같은 경제적 파급 효과를 연구한다. 사회학자들은 사회 구조와 노동 시장에 대한 변화를 탐구하며, 인공지능이 사회적 계층, 교육, 불평등과 어떻게 상호작용하는지를 분석한다. 철학자와 윤리학자들은 인공지능의 발전이 인간의 존엄성, 자율성, 윤리적 선택에 미치는 영향을 논하며, AI의 책임과 도덕적 한계에 대해 논의한다. 기술 전문가들은 AI 기술의 발전 가능성과 한계를 탐색하고, 새로운 혁신과 기술적 도전을 이해하기 위해 노력한다. 이러한 다양한 관점의 논의는 인공지능이 노동에 미치는 복합적인 영향을 이해하고, 보다 폭넓고 균형 잡힌 시각을 갖는 데 중요하며, 모든 관점에서의 포괄적 논의가 필요하다.\n예를 들어, 경제학적 관점에서는 일자리 자동화가 생산성을 향상시킬 수 있지만, 동시에 불평등을 심화시킬 수 있다는 점을 지적한다. 이는 특히 저숙련 노동자나 반복적인 작업을 수행하는 사람들이 AI에 의해 가장 먼저 영향을 받을 수 있음을 의미한다. 사회학적 관점에서는 인공지능이 노동 시장에서 일부 집단에게 더 큰 영향을 미칠 수 있으며, 이는 사회적 긴장과 분열을 야기할 수 있음을 강조한다. 예를 들어, 특정 지역이나 산업에서의 일자리 감소는 지역 경제에 심각한 영향을 미칠 수 있으며, 이는 사회적 불안정성을 증가시킬 수 있다는 것이다.\n\n\n1.5.2 정부, 기업, 교육기관의 역할\n인공지능과 노동의 상호작용에 대응하기 위해서는 정부, 기업, 교육기관 등 모든 이해관계자의 역할과 책임이 매우 중요하다. 정부는 적절한 정책과 규제를 통해 인공지능의 긍정적인 측면을 촉진하고 부정적인 영향을 최소화해야 한다. 이는 국가 차원에서의 전략적 계획을 포함하며, 인공지능 관련 정책, 법률, 윤리 지침을 개발하는 것을 의미한다. 예를 들어, 정부는 재교육 및 재취업 프로그램을 지원하여 노동 시장의 변화에 대비할 수 있도록 해야 하며, 인공지능의 윤리적 사용을 위한 법적 틀을 마련해야 할 것이다. 또한, 정부는 사회 안전망을 강화하여 기술 변화로 인해 일자리를 잃은 사람들이 생계를 유지할 수 있도록 지원해야 한다.\n기업은 인공지능을 도입하면서 발생할 수 있는 윤리적, 사회적 문제를 고려해야 한다. 이는 기업의 사회적 책임(CSR) 활동의 일환으로, 직원들의 재교육과 재취업 지원 프로그램을 제공하는 것을 포함한다. 기업은 기술 발전에 따른 직원의 직업 안전과 복지를 보장하기 위한 노력이 필요하며, 이를 위해 직원들과의 소통과 협력을 강화해야 할 것이다. 또한, 기업은 인공지능의 윤리적 사용을 위한 내부 지침과 프로토콜을 개발하고, 이해관계자들과의 협력을 통해 지속 가능한 방식으로 기술을 채택해야 한다.\n교육기관은 학생들에게 미래의 노동 시장에 필요한 기술과 지식을 제공하는 중요한 역할을 한다. 이는 단순히 기술적 기술을 가르치는 것뿐만 아니라, 비판적 사고, 창의성, 사회적 기술과 같은 보편적 기술을 강조하는 것을 포함한다. (이 책이 그러한 역할을 하기 바란다). 교육기관은 평생 학습의 중요성을 강조하고, 학생들에게 유연한 학습 기회를 제공함으로써 빠르게 변화하는 노동 시장에 대응할 수 있게 해야 한다. 이는 전통적인 교육 시스템을 넘어서 온라인 학습, 직업 훈련 프로그램, 계속 교육 과정 등 다양한 형태의 교육을 포함할 수 있다.\n\n\n1.5.3 인공지능과 노동의 미래에 대한 심층적 전망\n인공지능(AI)과 노동의 상호작용은 앞으로 수십 년 동안 우리 사회와 경제에 광범위하고 깊이 있는 영향을 미칠 것이다. 이 변화의 파도는 일부 직업을 사라지게 하거나 근본적으로 변화시킬 것이며, 동시에 새로운 직업과 기회를 창출할 것으로 보인다. 이러한 변화는 우리에게 도전을 제시할 뿐만 아니라, 적절하게 대응한다면 무한한 가능성을 열어줄 기회로도 작용할 수 있다. 즉, 인공지능의 발전은 생산성 향상, 혁신 촉진, 삶의 질 개선과 같은 긍정적인 잠재력을 가지고 있는 동시에 불평등의 심화, 직업 안정성의 감소, 윤리적 및 사회적 문제와 같은 도전도 함께 제기하고 있다."
  },
  {
    "objectID": "labor.html#더-읽을-거리와-생각해-볼-문제",
    "href": "labor.html#더-읽을-거리와-생각해-볼-문제",
    "title": "1  인공지능과 노동",
    "section": "1.6 더 읽을 거리와 생각해 볼 문제",
    "text": "1.6 더 읽을 거리와 생각해 볼 문제\n\n1.6.1 더 읽을 거리: 2023년 미국 작가 조합 파업\n\n2023년 5월 2일부터 9월 27일까지 148일 동안 진행된 미국 작가 조합 파업은 인공지능과 노동의 관계에 대한 중요한 시사점을 제공한다. 파업의 주요 쟁점은 인공지능의 발전으로 인해 작가들의 일자리가 위협받을 수 있다는 우려였다. 작가 조합은 인공지능을 활용한 콘텐츠 제작을 규제하고, 작가들의 최저임금을 인상하는 등의 요구를 했다.\n파업은 148일 만에 잠정 합의로 종결되었다. 합의안에는 인공지능을 활용한 콘텐츠 제작에 대한 규제 강화, 작가들의 최저임금 인상, 작가들의 단체교섭권 강화 등이 포함되었다.\n이 파업은 인공지능이 노동시장에 미치는 영향에 대한 노동자들의 우려를 반영한 것이다. 인공지능은 노동의 생산성을 높이고 새로운 일자리를 창출하기도 하지만, 기존 일자리를 대체하기도 한다. 따라서 인공지능과 노동의 관계를 조화롭게 발전시키기 위해서는 인공지능의 부정적 영향에 대한 대책을 마련하는 것이 필요하다.\n2023년 미국 작가 조합(WGA) 파업은 디지털 시대의 복잡한 노동 문제를 상징하는 중요한 사건이다. 이 파업은 OTT 시장의 확장과 작가들의 업무량 증가에도 불구하고 임금이 줄어들었다는 불만에서 시작되었다. 이와 관련하여, 작가들은 재방료 기준 확립, 원고료와 출연료 인상, AI를 통한 인력 감축 철회 등을 요구했다. 특히, AI 기술의 발전이 각본 작업에 미치는 영향과 OTT 드라마의 재방료 문제는 논쟁의 핵심이었다.\n\n\n\n1.6.2 인공지능과 노동의 쟁점\n인공지능과 노동의 관계에 대한 쟁점은 크게 두 가지로 나눌 수 있다.\n\n첫 번째 쟁점은 인공지능이 노동시장에 미치는 영향에 대한 것이다. 인공지능이 노동의 생산성을 높이고 새로운 일자리를 창출하기도 하지만, 기존 일자리를 대체하기도 한다. 따라서 인공지능이 노동시장에 미치는 영향에 대한 분석은 매우 중요하다.\n두 번째 쟁점은 인공지능과 노동의 관계를 조화롭게 발전시키기 위한 방안에 대한 것이다. 인공지능과 노동의 관계를 조화롭게 발전시키기 위해서는 인공지능의 부정적 영향에 대한 대책을 마련하는 것이 필요하다. 또한, 인공지능을 활용한 새로운 일자리를 창출하기 위한 노력도 필요하다.\n\n\n\n1.6.3 생각해 볼 문제: 2023년 미국 작가 조합 파업과 관련하여\n기술 발전과 창작 환경: 2023년 미국 작가 조합 파업에서 작가들은 AI의 사용을 제한하고, OTT 드라마의 재방료 문제를 제기했다. 이러한 요구는 기술 발전이 창작 환경에 어떤 영향을 미치는지에 대한 중요한 질문을 제기한다.\n\n기술의 발전이 창작자의 권리와 수익을 어떻게 변화시키고 있는지, 그리고 이에 대한 공정한 해결책은 무엇이라고 생각하는가?\n\n\n1.6.3.1 디지털 시대의 재방료와 로열티\n\nOTT 시대의 등장으로 전통적인 재방송 개념이 사라졌고, 작가들의 수익 구조에 변화가 생겼다. 이러한 변화 속에서 작가와 창작자들의 재방료와 로열티를 어떻게 보호하고 보장해야 할까?\n또한, OTT 플랫폼이 작품별 조회수와 같은 데이터를 공개하지 않는 것에 대해 어떻게 생각하는가? 노동 운동과 기술 변화에 대한 대응: 미국 작가 조합의 파업은 기술 변화에 대응하는 노동 운동의 중요한 사례이다. 미래에 기술의 변화가 더욱 가속화될 것으로 예상될 때,\n노동 운동이나 기타 사회적 운동이 이러한 변화에 어떻게 효과적으로 대응해야 한다고 생각하는가?\n또한, 노동자의 권리 보호를 위해 어떤 전략이 필요할까?\n\n\n\n1.6.3.2 Food for thoughts\n\n2023년 미국 작가 조합 파업의 결과는 인공지능과 노동의 관계에 어떤 영향을 미쳤을까?\n인공지능이 노동시장에 미치는 영향에 대해 긍정적, 부정적 측면을 각각 생각해 보자.\n인공지능과 노동의 관계를 조화롭게 발전시키기 위한 방안을 제시해 보자."
  },
  {
    "objectID": "labor.html#부록",
    "href": "labor.html#부록",
    "title": "1  인공지능과 노동",
    "section": "1.7 7. 부록",
    "text": "1.7 7. 부록\n본 챕터는 아래 노션 페이지에서 확인 가능하며, 계속해서 내용이 수정, 보완될 예정입니다. 내용과 문법에 오류가 있으면 코멘트 달아주시고, 아래 메일 주소로 보내주시면 감사하겠습니다.\nhttps://www.notion.so/cjleeskku/32a45c406c9d4390b90df16fe2a14641?pvs=4\n메일 주소: changjunlee@skku.edu"
  },
  {
    "objectID": "labor.html#참고문헌",
    "href": "labor.html#참고문헌",
    "title": "1  인공지능과 노동",
    "section": "1.8 8. 참고문헌",
    "text": "1.8 8. 참고문헌\n길은선·송영진·신위뢰 (2019). &lt;고용 없는 성장의 특성 및 산업별 분석&gt; (연구보고서 2019-913). 산업연구원.\n김세움 (2015). &lt;기술진보에 따른 노동시장 변화와 대응&gt;. 한국노동연구원.\n민순홍 (2023). &lt;플랫폼 노동 선택의 결정 요인과 플랫폼 종사자의 직업 이동 경로 분석&gt; (연구자료 2023-01). 산업연구원.\n박가열‧천영민‧홍성민‧손양수 (2016). &lt;기술변화에 따른 일자리 영향 연구&gt;. 한국고용정보원.\n이금노 (2018). &lt;인공지능 알고리즘 기반 경제에서의 소비자문제 연구&gt; (정책연구 18-17). 한국소비자원.\n이문호 (2020). 4차 산업혁명을 둘러싼 쟁점들 -’노동사회학적 관점’에서-. 노동연구, 40, 47-86.\n장진희·노성철·현종화 (2022). &lt;플랫폼노동의 알고리즘 현황과 대응방안 - 알고리즘의 공정성과 투명성, 노동자 통제를 중심으로&gt; (연구총서 2022-08). 한국노총중앙연구원.\n최병록 (2017). &lt;4차 산업혁명시대의 소비자이슈와 소비자정책&gt;. 한국기술혁신학회 추계학술대회.\n한국마케팅연구원 (2022). AI 기술과 초개인화 서비스. &lt;마케팅 2022&gt;, 56권 8호, 26-36.\n허민영·임병권 (2021). &lt;코로나 이후 디지털 전환 가속화에 따른 소비자정책 방향 연구&gt; (정책연구 21-01). 한국소비자원.\nAcemoglu, D., & P. Restrepo. (2018). The race between man and machine: Implications of technology for growth, factor shares, and employment. American Economic Review, 108(6), 1488-1542.\nBalsmeier, B., & M. Woerter. (2019). Is this time different? How digitalization influences job creation and destruction. Research Policy, 48(8), 103765. https://doi.org/10.1016/j.respol.2019.03.010.\nCharles, K.K., E. Hurst, & M.J. Notowidigdo (2013). Manufacturing decline, housing booms, and non-employment, Technical Report, NBER Working Paper, 18949, National Bureau of Economic Research.\nFrey, C.B. and M.A. Osborne (2017), “The future of employment: How susceptible are jobs to computerisation?,” Technological Forecasting & Social Change, 114, 254-280.\nGal, M., & Elkin-Koren, N. (2017). Algorithmic Contracts.Harvard Journal of Law and Technology,30, 309.\nPicht, P. G., & Freund, B. (2018). Competition (law) in the era of algorithms.Max Planck Institute for Innovation & Competition Research Paper, (18-10)."
  },
  {
    "objectID": "labor.html#footnotes",
    "href": "labor.html#footnotes",
    "title": "1  인공지능과 노동",
    "section": "",
    "text": "10 real-world examples of AI in healthcare↩︎\n11 AI in Manufacturing Examples to Know | Built In↩︎\nHow IBM’s Deep Blue Beat World Champion Chess Player Garry Kasparov Google’s AlphaGo wins final Go game against Lee Sedol↩︎\n딥마인드, 단백질 생성 AI ‘알파폴드’ 최신 버전 공개↩︎\nAI in Manufacturing: How It’s Used and Why It’s Important for Future Factories The Top 5 Benefits of Using Chatbots in Customer Service Teams↩︎\n10 new jobs created with AI in the workplace↩︎\n[Biz Focus] AI가 회계사 대체한다고?…오히려 귀한몸 된다 - 매일경제↩︎\nGenerative AI and the labor market: A case for techno-optimism↩︎"
  },
  {
    "objectID": "tai.html#인공지능-신뢰성-문제",
    "href": "tai.html#인공지능-신뢰성-문제",
    "title": "2  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "2.1 인공지능 신뢰성 문제",
    "text": "2.1 인공지능 신뢰성 문제\n인공지능(artificial intelligence, AI) 기술이 빠르게 발전하면서 사회 전반에 AI 전환(AI transformation, AIX)에 대한 기대가 커지고 있다. 그러나 다른 한편으로는 AI가 초래할지도 모를 부정적 영향에 대한 우려도 적지 않다.\nAI는 알고리듬에 의한 모델링 편향(modeling bias) 문제, 학습데이터에 의한 교육 편향(bias in training) 문제, AI를 악용하는 사람이나 조직의 문제 등으로 공정성 문제가 발생할 수 있다(손영화, 2023). 그 결과 AI가 편견을 학습해 범죄 예측이나 채용, 복지 제공 등에서 성이나 인종, 국적 차별을 하는 사례가 보고됐다. 학습데이터에 포함된 개인정보를 유출한다거나, 자살이나 위험한 놀이를 권유하거나, 체스 게임 중에 인간을 위협하고 실제 상해를 입히는 사례도 있었다 (한국정보통신기술협회, 2023; 손영화, 2023). 챗GPT의 환각(hallucination), 생성 AI를 활용한 가짜 뉴스, AI 생성물을 논문이나 시험, 과제물, 창작물 등에 비공개로 사용하는 것 등도 AI의 신뢰도를 훼손한다. 전쟁에 사용되는 킬러 로봇(killer robots)에 대한 우려도 커지고 있다(Beutel, Geerits, & Kielstein, 2023; Krishnan, 2016).\n게다가 이러한 문제를 개선하고자 하더라도 AI의 설명가능성 문제가 제기된다. 많게는 수조 개의 매개변수를 학습하는 딥러닝 방식이 일반화되면서 AI가 왜 그런 결과를 내놓았는지를 이해하기 어렵기 때문이다(고학수 등, 2021). 때문에 2024년 1월 현재, 딥러닝의 대부인 요슈아 벤지오(Yoshua Bengio) 캐나다 몬트리올대학교 교수와 미래학자인 유발 하라리, 일론 머스크 테슬라 사장, 에마드 모스타크 스테빌리티AI 사장 등 3만3000명이 넘는 인물들이 GPT-4를 넘어서는 AI 개발을 6개월 간 중단하자는 서한에 서명하기도 했다(Futrue of life, 2023.3). AI의 성능 개선 속도가 너무 빠르기 때문에 예기치 못할 AI의 해악에 대응할 시간을 벌자는 취지다.\nAI가 야기할지도 모르는 정치적, 경제적, 사회적, 문화적, 국제적 측면의 전방위적 문제에 대한 논의는 AI 윤리, AI 공정성(fairness), 설명가능한 인공지능(eXplainable AI, XAI), 책임 있는 AI(responsible AI) 등의 논의를 거쳐 일단 신뢰할 수 있는 인공지능(trustworthy AI, TAI)이라는 개념 아래 종합되는 추세다.\n이 장에서는 TAI의 개념과 전개, 그리고 저널리즘 분야에 적용 가능성을 살펴보도록 한다. 내용을 간략하게 살펴보면 다음과 같다. 우선 2절에서는 학제적인 신뢰 개념을 검토하고 이를 바탕으로 TAI를 정의한다. 우선 신뢰의 하위 개념으로는 신뢰성과 신뢰도를 살펴본다. 다음으로 신뢰의 두 유형으로 인간 신뢰와 기계 신뢰를 논의한다. 이어 TAI의 개념을 신뢰성과 신뢰도, 인간 신뢰와 기계 신뢰 측면에서 개념화한다. 3절에서는 TAI의 논의가 어떻게 전개됐는지 유럽연합 집행위원회(European Commission, EC), 경제협력개발기구 (Organisation for Economic Co-operation and Development, OECD), 그리고 국내에서 진행된 TAI 논의를 중심으로 살펴본다. TAI의 논의는 윤리적인 측면에서 기술적인 측면으로 구체화됐다. 4절에서는 TAI의 구체적인 영역(domain)으로서 저널리즘 분야를 놓고, TAI가 저널리즘 분야의 AI, 즉 저널리즘 AI에 어떻게 적용될 수 있는지 살펴본다. 저널리즘 AI의 신뢰성은 AI로서의 신뢰성 외에도 언론인과 언론사의 신뢰성을 제고함으로써 달성된다. 이를 위해서는 언론인이 저널리즘 AI의 기술적 루프(loop) 속에 참여해야 한다."
  },
  {
    "objectID": "tai.html#신뢰할-수-있는-인공지능의-개념",
    "href": "tai.html#신뢰할-수-있는-인공지능의-개념",
    "title": "2  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "2.2 신뢰할 수 있는 인공지능의 개념",
    "text": "2.2 신뢰할 수 있는 인공지능의 개념\n\n2.2.1 신뢰의 정의\nTAI를 논의하기에 앞서 신뢰의 개념을 살펴보자. 우선 신뢰는 신뢰자(trustor)가 신뢰 대상(trustee)을 믿는 것이다. 그러나 신뢰는 단순히 믿는 것 이상을 의미한다. 신뢰에 대한 심리학, 사회학, 경제학 분야의 연구들을 검토한 바에 따르면, 신뢰는 타인의 의도(intention)나 행동(behavior)에 대한 긍정적인 기대(positive expectation)에 근거하여 취약성(vulnerability)을 수용하려는 의지에 따른 심리적 상태로 정의할 수 있다(Rousseau et al, 1998).\n무엇보다 신뢰는 위험(risk)이 존재한다. 이러한 면에서 신뢰는 확신(confidence)과 다르다. 확신에는 위험 감수가 없다. 그러나 신뢰에는 위험 감수가 존재한다(Luhmann, 2000). 위험은 신뢰 대상의 의도와 미래의 행동이 불확실성(uncertainty)을 갖기 때문에 발생한다. 즉 신뢰 대상은 반사회적인 의도와 행동을 할 수 있다. 이러한 위험이 있는 상태에서 신뢰자는 신뢰 대상에 대한 긍정적 기대를 바탕으로 자신의 취약성을 받아들일 때 신뢰자는 신뢰 대상을 신뢰한다고 할 수 있다. 위험은 신뢰의 기회를 마련한다. 신뢰는 위험 감수에 도움을 준다. 위험 감수는 신뢰를 강화한다(Coleman, 1990).\n신뢰는 상호적이다. 신뢰를 위해서는 신뢰자가 신뢰 대상을 신뢰할 뿐만 아니라, 신뢰 대상이 신뢰자를 신뢰해야 한다. 즉 신뢰 관계에서 신뢰자는 신뢰 대상으로, 신뢰 대상은 신뢰자로 바뀔 수 있다.\n신뢰의 기능으로는 거래 비용(transaction cost)을 감소시키고 조직 통합을 강화한다는 점을 들 수 있다(Gambetta, 1988; Meyerson et al., 1996; Shapiro et al., 1992). 사실 완전한 확신을 얻기란 쉽지 않다. 때문에 신뢰는 불확실성 상황에서 협력의 지름길 역할을 한다.\n신뢰는 제도적으로 공급되어야 한다. 신뢰는 오랜 시간 동안 형성되고, 안정과, 해체, 그리고 재부상하는 과정을 거친다(Fukuyama, 1995; Miles et al., 1995). 신뢰는 훼손되기 쉬우며 때문에 과소 공급되는 공공재와 같다. 신뢰는 법적 통제에 의존하지는 않는다. 신뢰를 지키지 않았을 때 처벌하는 식의 법적 통제는 오히려 신뢰를 훼손시킬 수도 있다(Sitkin & Bies, 1993). 그러나 신뢰를 뒷받침할만한 제도적 장치는 신뢰 증진에 도움이 된다(Nooteboom, Berger, & Noorderhaven, 1997).\n\n\n2.2.2 신뢰성과 신뢰도\n신뢰 아래의 긍정적 기대는 다양하다. 우선 상대가 자신에게 이로운 행동을 할 것이라는 호의(benevolence)에 대한 기대가 있다. 신뢰 대상이 사회적, 도덕적 원칙을 지킬 것으로 믿는 진실성(integrity)에 대한 기대도 있다. 신뢰 대상이 기대하는 바를 실행할 수 있으리라는 능력(competence)에 대한 기대도 포함된다(김길수, 2020; Mayer et al., 1995). 호의를 갖고 있지만 진실성이 없다면 그것은 정파성을 띄게 된다. 상대방이 호의와 진실성을 갖고 있다고 하더라도 이를 실현할 능력이 없다면 신뢰할만한 대상이 되지 않는다.\n신뢰와 관련된 개념으로 신뢰도(credibility)와 신뢰성(trustworthiness)이라는 용어가 사용된다. 이러한 용어들이 혼용되는 경향이 있지만, 대체로 신뢰도는 신뢰자의 속성, 신뢰성은 신뢰 대상의 속성을 일컫는 말로 구분할 필요가 있다. 신뢰도는 신뢰자가 신뢰 대상을 얼마나 신뢰하는지를 나타내는 심리적 수준을 의미한다. 이는 신뢰의 개인심리적 측면으로 볼 수 있다. 신뢰성은 신뢰 대상의 의도와 행동, 그리고 능력과 관련된다. 즉 신뢰성은 신뢰의 사회제도적 측면에 더 초점을 두고 있다.\n예컨대 언론 신뢰는 “언론사가 만족스러운 방식으로 기능을 수행할 것이라는 기대를 바탕으로 뉴스 콘텐츠를 기꺼이 받아들이려는 수용자의 의향”이라고 정의할 수 있다(Hanitzsch et al., 2018). 여기서 수용자의 의향은 언론 신뢰의 개인심리적 측면, 즉 신뢰자의 개인심리적 신뢰도를 의미한다. 언론사의 기능 수행은 언론 신뢰의 사회제도적 측면, 즉 신뢰 대상으로서 언론사라는 제도의 신뢰성을 뜻한다. 신뢰도는 수용자의 인지적 반응과 정서적 반응을 포괄한다. 각각에 대응하여 신뢰성은 사실성과 공정성을 모두 충족해야 한다. 즉 신뢰성 문제는 언뜻 보면 기능에 치우친 것처럼 보일 수 있으나 실은 가치 문제를 포함한다.\n\n\n2.2.3 인간 신뢰와 기계 신뢰\n전통적으로 신뢰는 개인 수준이든 집단 수준이든 인간에 대한 신뢰(trust in people)의 측면에서 다뤄졌다. 그러나 현대 사회에서는 기술을 신뢰 대상으로 하는 기술 신뢰(trust in technology)의 중요성이 커지고 있다. 특히 디지털 시대에 기술이 인간의 역할을 점점 더 많이 대체하면서 기술 신뢰에 대한 연구가 늘고 있다(김길수, 2020).\n기술 신뢰는 인간 신뢰와 다른 점이 있다. 우선 기술 자체는 의도를 갖고 있지 않다. 때문에 기술은 신뢰 대상이 아니라든가, 기술 신뢰를 다룰 때 일종의 능력인 성능을 중시하는 경향이 있었다. 그러나 기술도 인간과 마찬가지로 위험 요소를 갖고 있다. 즉 기술 역시 신뢰 대상으로 간주할 수 있다(김길수, 2020).\n전통적으로 사회과학에서 기술 신뢰는 기술을 활용하는 개인이나 조직을 대상으로 했으나 최근에는 서비스나 기술 자체에 대한 신뢰 문제로 확대되는 추세다(Jarvenpaa & Leidner, 1999; Mcknight et al., 2011; McKnight et al., 2002). 다른 한편 공학적으로 기술 신뢰는 시스템의 성능 문제를 중심으로 다루어졌으나 최근에는 기술의 사회적 영향력을 고려하고 기술의 사회적 구성을 고민하는 방향으로 발전하고 있다. 종합하면, 기계 신뢰는 기계 자체에 대한 신뢰성을 바탕으로 인간 신뢰를 부분적으로 통합하는 형태를 띄고 있다.\n\n\n2.2.4 신뢰할 수 있는 인공지능의 개념\n신뢰의 개념에 비추어 볼 때, TAI가란 신뢰성을 갖춘 AI, 그리고 이를 통해 인간이 신뢰도를 갖는 AI으로 볼 수 있다. 신뢰성을 갖춘 AI는 기본적으로 목표 과업(task)을 만족스러울만한 성능으로 수행해야 한다. 예컨대 객체 탐지를 위한 모델은 개와 고양이를, 행동 인식을 위한 모델은 걷기와 달리기를, 상황 이해를 위한 모델은 화재나 교통 사고를 빠르고 정확하게 파악해야 한다. 프롬프트를 입력하면 이미지를 만드는 멀티모달 AI(multimodal AI) 모델은 사용자가 “불 속에서 달리는 고양이”를 그리라고 했을 때 사용자 의도에 부합하는 영상을 생성해야 한다. 더 나아가 신뢰성을 가진 AI는 AI 기술과 서비스 자체의 신뢰성과 함께, AI를 기획, 개발, 운영하는 인간이나 집단의 신뢰성을 포괄한다. 이러한 신뢰성에는 신뢰자인 사용자에 대한 신뢰성 역시 포함된다. 즉 AI 신뢰성에는 사용자가 AI를 악의적으로 사용하지 않을 것이라는 기대도 포함된다.\nTAI는 기계 신뢰에 속한다. 우선 AI를 기획, 개발, 운영하는 인간이나 집단의 신뢰성을 생각해볼 수 있다. 이들은 의도를 가질 수 있다. 신뢰자는 신뢰 대상인 인간 기획자, 개발자, 운영자의 선의를 기대하는 방식으로 신뢰도를 갖는다. 다음으로 AI의 신뢰성을 살펴보자. AI는 실제로는 의도를 갖고 있지 않다. 즉 불확실성이 없다. 때문에 AI 자체는 엄밀한 의미에서 신뢰 대상이 될 수 없다고 간주할지 모른다. 그러나 AI는 두 가지 측면에서 의도를 고려해야 한다. 우선 AI는 종종 의도를 가진 것처럼 느껴진다. AI는 기계 중에서도 가장 자율적으로 실행된다. 인간이 개입하도록 따로 설계하지 않는 한, 즉 인간이 루프 속에 들어가고(human in the loop), 인간이 최종 결정하는 식의 인간 중심적으로(human-centered) 설계되지 않는 한 AI는 매우 높은 수준 자동으로 의사 결정을 할 수 있도록 만들어진 자율적이고 지능적인 자동장치(automata)이자 에이전트(agent)이다. 특히 딥러닝을 포함하는 기계학습 방식의 AI는 학습을 통해 입력 값과 출력 값의 쌍으로 이루어진 데이터세트를 바탕으로 의사 결정에 필요한 함수를 스스로 찾는다. 그리고 스스로 찾은 함수를 이용해 입력 값에 대한 출력 값을 예측, 분류, 생성한다.\n또 하나는 AI는 설사 그것을 기획, 개발, 운영, 사용하는 인간이 선한 의도를 갖고 있다고 하더라도 실제로는 해악이 되는 결과를 만들어 낼 수 있다. 이는 우선 AI의 성능 문제일 수 있다. 즉 인간의 선의를 AI가 제대로 구현하지 못할 수 있다. 예컨대 기계 번역기가 영어를 한국어로 제대로 번역하지 못할 수 있다. 이는 성능 개선을 통해 해결할 수 있다. 더 중요한 문제는 AI가 창발적 해악(emergent harm)을 초래할 수도 있다는 점이다(박도현, 2021). AI는 기본적으로 학습데이터 자체가 아니라 학습데이터의 극히 유한한 질서, 즉 특징(feature)을 무한한 가능성을 가진 벡터 공간(vector space)에 임베딩(embedding) 내지 인코딩(encoding)하고, 이를 바탕으로 새롭지만 극히 유한한 상황에 대해 예측하는 식으로 판별(discriminative model) 또는 생성(generative model)한다. 이 과정에서 AI는 학습데이터 자체와는 다른 새로운 상황을 판별하고 생성할 수 있다. 이를 창발로 부를 수 있다. 이러한 창발은 의도에 부합할 수도 있고 그렇지 않을 수도 있다. 즉 인간이 선의를 갖고 AI를 기획, 개발, 운영하고, 사용자도 선의를 갖고 AI를 사용한다고 하더라도, AI가 해악을 산출할 수도 있다.\n정리하면 AI는 AI과 관련된 인간의 불확실성, AI 성능의 불확실성, AI의 창발적 해악에 대한 불확실성을 갖는다. 따라서 TAI는 AI과 관련된 인간과 조직에 대한 신뢰성, AI의 성능 측면의 신뢰성, 그리고 AI가 인과적 해악(causal harm)은 물론 창발적 해악도 산출하지 않을 것이라는 신뢰성을 갖춰야 한다. 이러한 신뢰성을 바탕으로 AI에 대한 신뢰도가 높아지면 AI 신뢰 역시 높아지게 된다."
  },
  {
    "objectID": "tai.html#신뢰할-수-있는-인공지능의-전개",
    "href": "tai.html#신뢰할-수-있는-인공지능의-전개",
    "title": "2  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "2.3 신뢰할 수 있는 인공지능의 전개",
    "text": "2.3 신뢰할 수 있는 인공지능의 전개\nAI의 사회적 영향력에 대한 논의는 AI 윤리, 설명가능한 인공지능 측면에서 진행되다가, TAI의 논의로 종합되는 추세이다. 이 절에서는 TAI 논의의 전개를 TAI 관련 주요 가이드라인인 EC의 신뢰할 수 있는 인공지능 윤리 가이드라인(Ethics guidelines for trustworthy AI), OECD의 AI 권고안(Recommendation of the Council on Artificial Intelligence), 그리고 한국정보통신기술협회(Telecommunications Technology Association, TTA)의 &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;를 중심으로 살펴보도록 한다.\n\n2.3.1 EC의 신뢰할 수 있는 인공지능 논의\n2019년 4월 EC의 &lt;신뢰할 수 있는 인공지능 윤리 가이드라인&gt;은 TAI의 논의가 본격화된 시발점이라 할 수 있다. 이 가이드라인에서는 TAI를 1) 합법적이고(lawful), 2) 윤리적이며(ethical), 3) 강건한(robust) AI로 규정한다. 합법성은 모든 법률과 규정을 따르는 것이다. 윤리성은 윤리 원칙과 가치를 존중하는 것이다. 강건함은 기술적인 측면과 사회적 측면을 모두 고려한다. 흔히 강건함은 AI가 학습데이터가 아닌 다양한 실제 상황(in the wild)에서도 객체 탐지나 행동 인식과 같은 목표 과업을 높은 성능으로 수행할 수 있음을 의미한다. 그러나 TAI에서 강건함은 창발적 해악까지 예방한다는 의미로 해석할 수 있다.\nTAI를 실현하기 위한 핵심 요구사항(key requirements)으로는 1) 인간 기관의 관리 감독(human agency and oversight), 2) 기술적 견고성 및 안전성(Technical Robustness and safety), 3) 개인 정보 보호 및 데이터 거버넌스(privacy and data governance), 4) 투명성(transparency), 5) 다양성, 비차별성 및 공정성(diversity, non-discrimination and fairness), 6) 사회적, 환경적 복지(societal and environmental well-being), 7) 책무성(accountability) 등 일곱 가지를 제시했다.\n1)은 AI 시스템이 루프 속 인간을 통해 인간의 감독을 받아 인간 기본권 증진과 의사 결정에 도움을 주도록 작동해야 한다는 것을 의미한다. 2)는 AI가 문제 발생시 항상 대응할 수 있도록 유연하게 설계되어 의도치 않은 해악까지도 방지할 수 있어야 한다는 것을 뜻한다. 3)은 개인정보 보호는 물론 데이터에 대한 합법적 접근과 관리를 위한 데이터 거버넌스를 구축해야 한다는 것을 말한다. 4)는 설명가능한 AI에 대한 요구사항이다. AI와 관련된 데이터, 시스템, 비즈니스 모델이 투명하고 추적 가능해야 한다. 또한 이해 당사자에게 AI의 기능과 한계를 적절하게 설명해야 한다. 5)는 취약 계층에 대한 부정적 영향을 제거하는 것을 의미한다. 또한 장애와 무관하게 AI에 접근가능해야(accessible) 한다. 6)은 지속가능성(sustainability) 측면의 복지 증진과 관련된다. 즉 AI가 현 인류는 물론 미래 세대에게 혜택을 줄 수 있어야 한다. 또한 다른 생명체를 포함한 환경의 영향을 고려해야 한다. 7)은 AI 시스템의 알고리듬, 데이터, 설계, 그리고 그 결과에 대한 감사가능성(auditability), 책임 요구에 대한 응답가능성(responsibility), 그리고 적절한 보정 가능성이 보장되어야 한다는 것을 의미한다.\nEC는 가이드라인을 바탕으로 2020년 7월 &lt;신뢰할 수 있는 인공지능 자체 평가 목록&gt;(Assessment List for Trustworthy Artificial Intelligence, ALTAI)를 내놓았다(EC, 2020). 이어 이러한 노력은 세계 최초의 AI 규제 법안 인공지능법(Artificial intelligence Act)으로 결실을 맺는다. 인공지능법의 초안은 2021년 4월 발의됐다(손영화, 2021). 이후 EU는 수정을 거쳐 2023년 12월 법안에 합의했다(김진희, 2023.12.10). 해당 법안은 테러나 범죄 예방, 법 집행, 국가 안보 등 일부 분야를 제외한 안면인식과 대규모 언어 모델(large language model, LLM) 사용을 엄격히 규제하고 있다. 또한 자율 주행이나 의료 등 고위험 기술에 대해서는 데이터 공개와 엄격한 테스트를 강제했다. 이를 위반할 경우 최대 3500만 유로, 또는 전 세계 매출의 7%에 해당하는 벌금을 부과하는 강제 수단도 포함됐다.\n\n\n2.3.2 OECD의 신뢰할 수 있는 인공지능 논의\nEU의 적극적인 움직임은 국제적으로 TAI 논의 확산을 가속화했다. EC 가이드라인이 나온지 한 달 뒤인 2019년 5월 경제협력개발기구(Organisation for Economic Co-operation and Development, OECD)는 AI 권고안(Recommendation of the Council on Artificial Intelligence)을 발표했다(OECD, 2019). 권고안에 따르면 TAI란 AI 시스템의 기획, 개발, 구축, 운영 등 전 단계에서 신뢰 가능한 AI의 원칙들이 실현된 AI 시스템을 의미한다. 이는 AI 생태계 전반에 걸쳐 작동해야 한다. 이를 위해 OECD는 TAI를 위한 5개 원칙을 제시했다.\n첫째, 포용 성장, 지속가능 발전과 복지 증진(inclusive growth, sustainable development and well-being)이다. AI는 인간의 능력, 창의력, 소수집단에 대한 포용력을 증진시키고, 사회적 불평등을 해소하며, 환경을 보호하는데 사용되도록 노력해야 한다.\n둘째, 인간 중심적 가치와 공정성(human-centered values and fairness)이다. AI는 인권, 자유, 민주적 가치, 인간의 존엄성, 자율성, 노동권, 평등, 다양성, 공정성, 사회 정의, 개인정보 보호 등을 개선하는데 사용되어야 한다는 것을 의미한다.\n셋째, 투명성과 설명가능성(transparency and explainability)이다. EC 가이드라인과 마찬가지로, AI 행위자(actors)는 사용자나 고객 등 AI 이해관계자에게 AI 시스템의 개발, 운영 등에 대해 정보를 투명하게 제공해야 하며, 그 의사결정과 핵심 내용을 이해하기 쉽게 설명해야 한다는 것을 뜻한다.\n넷째, 강건함, 보안, 안전(robustness, security and safety)이다. AI 시스템이 이러한 요건을 갖추기 위해 AI의 전체적인 사용 주기 전반에서 위험이 지속적으로 모니터링되고 추적 가능해야 한다. 위험이 발견됐을 때는 이를 분석하고 대응할 수 있도록 만들어져야 한다.\n다섯째, 책임성(accountability)이다. AI 행위자는 AI 시스템의 신뢰성을 구현할 수 있도록 행동 강령이나 안내서를 명시하고, 관련 문서를 공개하며, 필요한 감사를 받음으로써 책임성을 증명할 수 있어야 한다.\n이 밖에도 권고안에는 TAI에 대한 국가 정책 및 국제 협력에 대한 제안도 포함됐다. 여기에는 TAI 연구에 대한 공공 투자와 민간 투자 장려, 규제 프레임워크와 평가 메커니즘의 개발, 노동 시장 변화에 대응하기 위한 사회적 협의와 교육 지원, 개발도상국에 대한 지원과 국제 표준 마련을 포함한 국제 공조 등을 제안한다. 이에 따라 미국, 일본, 한국, 싱가포르, 호주 등 각국은 TAI를 위한 정책을 발표했다(The White House Office of Science and Technology Policy, 2020). 국제표준화기구(International Organization for Standardization)와 국제전기기술위위원회(International Electrotechnical Commission, IEC), 즉 ISO/IEC의 AI 위원회 JTC1/SC42 산하 그룹 중 신뢰성 작업 그룹(Working Group 3, WG3) 등은 기술적 관점에서 TAI 표준화 작업을 진행하고 있다(곽준호, 2022).\n\n\n2.3.3 한국의 신뢰할 수 있는 인공지능 논의\n한국은 2020년 11월 과학기술정보통신부가 &lt;국가 인공지능 윤리 기준&gt;을 발표했다(과학기술정보통신부, 2020). 우선 3대 기본 원칙으로 1) 인간 존엄성 원칙, 2) 사회의 공공선 원칙, 3) 기술의 합목적성 원칙을 꼽았다. 1)은 인간이 AI와 교환 불가능한 가치를 가지므로 인간에 해가 되지 않도록 개발되어야 한다는 것을 의미한다. 2)는 AI가 사회적 약자와 취약 계층의 접근성을 보장하고, 인류 보편적 복지를 향상하도록 개발되어야 한다는 것을 뜻한다. 3)은 AI가 인류의 삶과 번영에 필요한 도구라는 목적에 맞게 윤리적으로 개발되어야 한다는 것을 말한다.\n3대 원칙을 실행하는 10대 요건으로는 1) 인권보장, 2) 프라이버시 보호, 3) 다양성 존중, 4) 침해 금지, 5) 공공성, 6) 연대성, 7) 데이터 관리, 8) 책임성, 9) 안전성, 10) 투명성이 포함됐다. 각각의 내용은 앞서 설명한 EC의 가이드라인이나 OECD 권고안과 일맥상통한다.\n2021년에는 4차산업혁명위원회가 &lt;사람이 중심이 되는 AI를 위한 신뢰할 수 있는 인공지능 실현 전략(안)&gt;을 내놓았다(4차산업혁명위원회, 2021). 그 내용으로는 1) 신뢰 구현을 위한 법‧제도, 윤리적, 기술적 요구사항을 종합한 AI 개발 가이드북 제작과 보급, 2) AI 제품이나 서비스에 대한 민간 자율의 인증제 도입 및 지원, AI 일괄 지원 플랫폼 운영, 3) 설명 가능성‧공정성‧견고성 등을 향상시키기 위한 신뢰성 원천기술 개발 추진, 4) 학습용 데이터의 신뢰성 확보를 위한 표준 기준 제시 및 데이터 개방, 5) 고위험 AI에 대한 국민 안전‧신뢰성 향상 방안 연구, 6) AI 윤리 기준 실천을 위한 윤리 교육 강화 및 개발자‧이용자용 체크리스트 보급, 윤리‧신뢰성 향상을 위한 공론의 윤리 정책 플랫폼 운영 등이 있다.\n이어 2023년 7월 TTA는 EC의 ALTAI와 유사한 &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;를 내놓았다(한국정보통신기술협회, 2023). 기존 가이드라인에 비해, TTA의 안내서는 기술적으로 구체화된 TAI 개발 및 검증 방법론을 제시한다. 안내서는 전 분야에 해당하는 일반 분야와 함께 공공·사회 분야, 의료 분야, 자율주행 분야 등 세부 분야별 안내서를 제시했다.\n안내서에는 TAI의 설계 요소를 AI 신뢰성 프레임워크로 제시한다. 해당 프레임워크는 크게 세 가지로 구성된다. 첫째, 신뢰성 확보 대상이다. AI 데이터, AI 모델과 알고리즘, AI 시스템, 사람-AI 인터페이스가 해당된다. 둘째, 생애주기별 요구사항 분류이다. AI 생애주기란 AI 시스템을 구현하고 AI 서비스를 운영하는 과정을 뜻한다. AI 생애주기는 계획 및 설계, 데이터 수집 및 처리, AI 모델 개발, 시스템 구현, 운영 및 모니터링 등 5단계로 나뉜다. 셋째, AI 윤리 기준 준용이다.\n신뢰성 요건으로 AI 윤리 기준의 10대 요건 중 기술적 적용 가능한 4개 요건을 선별했다. 4개 요건은 다양성 존중, 책임성, 안전성, 투명성이다. AI 신뢰성 프레임워크를 정리하면 &lt;그림 1&gt;과 같다.\n\n\n\n그림 1. 인공지능 신뢰성 프레임워크(출처: 한국정보통신기술협회, 2023)\n\n\n또한 신뢰성 요건별 세부 속성 및 키워드는 &lt;표 1&gt;과 같다(한국정보통신기술협회, 2023). 그리고 신뢰성 요건을 충족시키기 위한 요구사항을 생애주기별로 15개 제시했다. AI 생애주기별 요구사항과 적용되는 신뢰성 요건 및 각 요구사항은 &lt;표 2&gt;와 같이 정리할 수 있다(한국정보통신기술협회, 2023).\n\n표 1. 신뢰성 요건별 정의와 관련 속성 및 관련 키워드(출처: 한국정보통신기술협회, 2023 일부 수정).\n\n\n\n\n\n\n\n\n신뢰성 요건\n정의\n관련 속성\n관련 키워드\n\n\n\n\n다양성 존중\nAI가 특정 개인이나 그룹에 대한 차별적이고 편향된 관행을 학습하거나 결과를 출력하지 않으며, 인종･성별･연령 등과 같은 특성과 관계없이 모든 사람이 평등하게 AI 기술의 혜택을 받을 수 있는 것\n공정성(fairness), 정당성(justice)\n편향(bias), 차별(discrimination), 편견(prejudice), 다양성(diversity), 평등(equality)\n\n\n책임성\nAI가 생명주기 전반에 걸쳐 추론 결과에 대한 책임을 보장하기 위한 메커니즘이 마련되어 있는 것\n책임성(responsibility), 감사가능성(auditability), 답변가능성(answerability)\n책임(liability)\n\n\n안전성\nAI가 인간의 생명･건강･재산 또는 환경을 해치지 않으며, 공격 및 보안 위협 등 다양한 위험에 대한 관리 대책이 마련되어 있는 것\n보안성(security), 견고성(robustness), 성능보장성(reliability), 통제가능성(controllability)\n적대적 공격 (adversarial attack), 복원력(resilience), 프라이버시(privacy)\n\n\n투명성\nAI가 추론한 결과를 인간이 이해하고 추적할 수 있으며, AI가 추론한 결과임을 알 수 있는 것\n설명가능성(explainability), 이해가능성(understandability), 추적가능성(traceability), 해석가능성(interpretability)\nXAI(eXplainable AI), 이해도(comprehensibility)\n\n\n\n표 2. 신뢰성 요건 충족을 위한 AI 생애주기별 요구사항 생애주기 요구사항 신뢰성 요건 다양성 책임성 안전성 투명성 1. 계획 및 설계 1. AI 시스템에 대한 위험관리 계획 및 수행 　 O 　 O 2. AI 거버넌스 체계 구성 O O O O 3. AI 시스템의 신뢰성 테스트 계획 수립 　 　 O O 2. 데이터 수집 및 처리 3. 데이터의 활용을 위한 상세 정보 제공 　 O 　 O 4. 데이터 강건성 확보를 위한 이상(Abnormal) 데이터 점검 　 　 O 　 5. 수집 및 가공된 학습 데이터의 편향 제거 O O 　 O 3. AI 모델 개발 6. 오픈소스 라이브러리의 보안성 및 호환성 확보 　 O O 　 7. AI 모델의 편향 제거 O 　 　 　 8. AI 모델 공격에 대한 방어 대책 수립 　 　 O 　 9. AI 모델 명세 및 출력 결과에 대한 설명 제공 　 O 　 O 4. 시스템 구현 11. AI 시스템 구현 시 발생 가능한 편향 제거 O 　 　 　 12. AI 시스템의 안전 모드 구현 　 O O O 13. AI 시스템의 설명에 대한 사용자의 이해도 제고 　 　 　 O 5. 운영 및 모니터링 14. AI 시스템의 추적가능성 확보 　 　 O O 15. 서비스 제공 범위 및 상호작용 대상에 대한 설명 제공 　 O 　 O 출처: 한국정보통신기술협회(2023), 일부 수정.\n요구사항은 다시 대분류 34개, 소분류 67개의 2단계의 체크리스트로 세분화했다. &lt;표 3&gt;은 생애주기 중 AI 모델 개발 단계의 요구사항 및 대분류 수준의 체크리스트의 예이다.\n소분류 수준의 체크리스트는 매우 구체적인 기술적 방안을 담고 있다. 예컨대 요구사항 9의 ’AI 모델 공격에 대한 방어 대책 수립’과 같은 내용은 일종의 AI 시스템의 모델을 훔쳐가는 모델 추출 공격(model extraction attack)에 대한 질의(query) 횟수 제한과 같은 기술적 방어 기법이나, AI 시스템에 적용된 모델을 속이는 모델 회피 공격(model evasion attack)에 대한 적대적 훈련(adversarial training)과 같은 방어 기법의 적용 여부를 따진다.\n표 3 신뢰할 수 있는 인공지능 모델 개발 단계의 요구사항과 체크리스트\n출처: 한국정보통신기술협회(2023)"
  },
  {
    "objectID": "tai.html#저널리즘-분야에서-신뢰할-수-있는-인공지능의-함의",
    "href": "tai.html#저널리즘-분야에서-신뢰할-수-있는-인공지능의-함의",
    "title": "2  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "2.4 저널리즘 분야에서 신뢰할 수 있는 인공지능의 함의",
    "text": "2.4 저널리즘 분야에서 신뢰할 수 있는 인공지능의 함의\n사회 전반의 AI 전환(AIX) 추세에 따라 저널리즘 분야에서도 AI 활용 가능성이 높아지는 추세다(한국언론진흥재단, 2020; 이현우ˑ이성민ˑ이상규, 2023; Beckett, 2019; Diakopoulos, 2019; Jones et al., 2022). 저널리즘에 활용되는 AI를 저널리즘 AI라고 부를 수 있다(박대민, 2023; Beckett, 2019). 이 절에서는 저널리즘 AI에서 TAI의 함의를 살펴본다.\n사실 TAI 관련 논의에서 저널리즘은 물론 미디어에 대한 관심은 높지 않다. EU의 AI 정책 문서에서 미디어에 대한 언급은 많지 않고 저널리즘에 대한 언급은 더욱 적다. 언론사 역시 AI 활용에 대한 높은 관심에도 불구하고 자사의 윤리 강령에 AI 활용 가이드라인을 포함시키는 사례는 많지 않다(Porlezza, 2023). 그러나 탈진실 사회(post truth society)로도 불리는 현재에, 딥페이크, 가짜 뉴스, 편향, 반향실 효과 등 AI가 촉발할 것으로 예상되는 미디어와 저널리즘 분야의 신뢰성 위기는 심각한 수준이다(박대민, 2023; 박대민, 2022; Edelman, 2023; Newman et al., 2023).\n저널리즘 AI의 신뢰성은 크게 세 가지 측면에서 논의될 필요가 있다. 첫째, 저널리즘 AI는 AI 기술이므로 TAI로 설계되어야 한다. 둘째, 저널리즘 AI의 신뢰성은 AI를 사용하는 인간과 조직의 신뢰성과도 연계된다. 즉 언론사를 비롯한 미디어의 신뢰성이 저널리즘 AI의 신뢰성에 영향을 준다. 셋째, 저널리즘 AI의 신뢰성은 결국 사용자의 신뢰도를 높이는 방향으로 사용되어야 한다.\n사실 TAI의 논의에서는 첫번째 사항만 고려되는 경향이 강하다. TTA의 안내서를 저널리즘과 같이 특정 영역에 구체적으로 적용할 때 크게 두 가지 측면에서 한계가 보인다(박대민, 2023). 첫째, 해당 안내서가 AI 생애주기 중심으로 작성되어, 저널리즘 분야의 기사 생애주기와 맞지 않다. 즉 저널리즘 AI에서 TAI를 구현하려면, AI 생애주기를 기사 생애주기 관점에서 통합해야 한다. 둘째, TTA의 신뢰성을 구현에서 언론인과 같은 도메인 전문가의 역할이 과소평가되어 있다. 이는 비록 TTA가 AI 생애주기 전체를 고려한 요구사항을 만들었지만, 실제로는 기획과 개발 중심이고 운영을 과소평가했기 때문이다. 즉 기획과 개발에 생애주기 5단계 중 4단계를, 15개 요구사항 중 13개를 할당하고 서비스 운영에는 생애주기 1단계와 요구사항 2개만 할당한 것에서도 나타난다. 그러나 AI를 활용하는 각 영역 입장, 저널리즘 AI를 활용하는 미디어의 입장에서는 운영이 거의 전부나 다름없다. 물론 저널리즘 AI에 TAI를 적용할 때 운영만 고려하라는 것은 아니다. 핵심은 저널리즘 AI를 TAI로 구현할 때 언론인과 같은 영역 전문가의 역할이 훨씬 더 강화되어야 한다는 것이다. 기획, 개발, 운영 전반에 루프 속 인간으로서 언론인이 개입할 수 있도록 해야 한다(박대민, 2023; Broussard et al., 2019; Gutierrez-Lopez et al. 2019; Wu et al., 2022).\n한편 저널리즘 AI의 신뢰성이 언론사의 신뢰성의 영향을 받는다면, 저널리즘 AI의 신뢰성은 저널리즘의 신뢰성을 제고하고 사용자의 신뢰도를 높이는 방식으로 활용되어야 한다. 다행히 저널리즘의 가치 지향과 TAI의 가치 지향은 어느 정도 통약 가능성(commensurability)을 갖고 있다. TAI 투명성과 안전성은 저널리즘 사실성의, TAI의 다양성과 책임성은 저널리즘 공정성의 전제 조건이다. 즉 탈진실 사회에서 TAI 기반 저널리즘 AI를 활용함으로써 저널리즘 AI의 투명성과 안전성을 높여서 저널리즘의 사실성을 개선할 수 있다. 또한 TAI 기반 저널리즘 AI를 통해 저널리즘 AI의 다양성과 책임성을 높임으로써 저널리즘의 공정성을 향상시킬 수 있다(박대민, 2023).\n이것은 AI를 쓰면 저널리즘의 사실성과 공정성이 제고된다는 기술결정론적인 입장이 아니다. 반대로 AI라는 기술을 사실성과 공정성 제고를 위해 사용할 수 있도록 구성해야 한다는 구성주의적 입장에 가깝다. 그리고 그 구성 전략으로서 TAI의 논의를 참고할 수 있다는 것이다. 뿐만 아니라 JAI를 TAI로 구성하는 과정에서 TAI의 방법론을 더욱 정교화하고 합목적적으로 사용할 수 있다는 것을 의미한다.\n정리하면 TAI 기반 JAI는 다음과 같은 세 조건을 충족해야 한다. 첫째, 저널리즘 분야에서 AIX가 진행되어야 한다. 즉 저널리즘과 AI의 실천적 결합이 전면적으로 이뤄져야 한다. 둘째, JAI에 TAI를 적용되어야 한다. 저널리즘의 AI가 TAI 관점에서 신뢰성을 확보해야 한다. 셋째, JAI가 언론 신뢰 개선에 기여해야 한다(박대민, 2023).\n이를 TAI를 다른 사회 영역에 적용할 때로 일반화할 수도 있다. 첫 번째는 AIX 조건이다. AI를 도메인에 적용하는 것을 의미한다. 둘째, TAI 조건이다. AIX에 TAI를 적용하는 것이다. 셋째, 도메인 조건이다. AI가 해당 도메인의 신뢰를 제고할 수 있어야 한다(박대민, 2023)."
  },
  {
    "objectID": "tai.html#참고문헌",
    "href": "tai.html#참고문헌",
    "title": "2  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "2.5 참고문헌",
    "text": "2.5 참고문헌\n고학수·김용대·윤성로·김정훈·이선구·박도현·김시원 (2021). &lt;인공지능 원론&gt;. 서울: 박영사.\n과학기술정보통신부 (2021). &lt;신뢰할 수 있는 인공지능 실현 전략&gt;. 과학기술정보통신부.\n곽준호 (2022). 데이터의 품질과 인공 지능 시스템의 신뢰성. . 201권. 59-63.\n김길수 (2020). 인공지능의 신뢰에 관한 연구. &lt;한국자치행정학보&gt;, 34권 3호, 21-41.\n김진희(2023.12.10). EU, 37시간 진통 끝에 세계 최초 ‘AI 규제법’ 합의…주요 내용은? &lt;헬로T&gt;. Retrieved from https://www.hellot.net/news/article.html?no=84839\n박대민 (2023). 통과하면 사실로 인정되는 AI를 만들 수 있는가: 설명가능한 AI를 통한 사실성 제도로서 언론의 재구성. &lt;언론과사회&gt;. 31권 2호. 139-181.\n박대민 (2022). 미디어 인공지능: 컴퓨터 비전 분야 딥러닝 모델의 미디어 동영상 적용 가능성에 관한 연구. &lt;커뮤니케이션이론&gt;. 18권 1호, 111-154.\n박도현. (2021). &lt;인공지능과 해악&gt;. 서울대학교 법학대학원 박사학위논문. 4차산업혁명위원회 (2021). &lt;사람이 중심이 되는 AI를 위한 신뢰할 수 있는 인공지능 실현 전략(안)&gt;. 4차산업혁명위원회.\n손영화 (2023). AI 공정성에 관한 연구: 차별 없는 AI 사회의 실현. &lt;한양법학&gt;, 34권 3호. 275-304.\n손영화 (2021). EU AI 규칙안에 대한 일고찰. &lt;IP & Data 法&gt;, 1권 2호. 27-52.\n신예진 (2022). 신뢰할 수 있는 인공지능 개발 안내서. . 201권. 21-28.\n오로라(2024.1.4.). 美 최예진 교수 “안중근을 ’테러리스트’라는 AI, 韓 피해 상상 힘들어”. &lt;조선일보&gt;. Retrieved from https://www.chosun.com/economy/tech_it/2024/01/02/734OD6WEIZERFI3OARG55LQZHI/\n이현우·이성민·이상규 (2023). &lt;언론산업 인공지능(AI) 활용방안 연구&gt;. 서울: 한국언론진흥재단.\n한국언론진흥재단 (2020). &lt;2020 뉴스미디어의 신뢰·혁신·소통&gt;. 서울: 한국언론진흥재단.\n한국정보통신기술협회 (2023). &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;. 성남: 한국정보통신기술협회.\n한상기 (2021). &lt;신뢰할 수 있는 인공지능&gt;. 서울: 클라우드나인.\nBeckett, C. (2019). New powers, new responsibilities: A global survey of journalism and artificial intelligence. London School of Economics & Political Science.\nBeutel, G., Geerits, E., & Kielstein, J. T. (2023). Artificial hallucination: GPT on LSD?. Critical Care, 27(1), 148.\nBroussard, M., Diakopoulos, N., Guzman, A. L., Abebe, R., Dupagne, M., & Chuan, C. H. (2019). Artificial intelligence and journalism. Journalism & Mass Communication Quarterly, 96(3), 673–695.\nColeman, J. S. (1990). Foundations of social theory. Cambridge, MA: Belknap Press.\nDiakopoulos, N. (2019). Automating the news: How algorithms are rewriting the media. Harvard University Press.\nDhiman, D. B. (2023). Does Artificial Intelligence help Journalists: A Boon or Bane?. Available at SSRN 4401194.\nEdelman (2023). 2023 Edelman trust barometer (Global Report). Retrieved from https://www.edelman.com/trust/2023/trust-barometer\nEuropean Commission (May, 2019). Ethics guidelines for trustworthy AI. Retrieved from https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nEuropean Commission (July, 2020). Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment. Retrieved from https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment\nFukuyama, F. (1995). Trust: The social virtues and the creation of prosperity. New York: Free Press. Future of Life (2023.3) Pause Giant AI Experiments: An Open Letter. Retrieved from https://futureoflife.org/open-letter/pause-giant-ai-experiments/\nGambetta, D. (1988). Trust: Making and breaking cooperative relations. New York: Basil Blackwell.\nGutierrez-Lopez, M., Missaoui, S., Makri, S., Porlezza, C., Cooper, G., & MacFarlane, A. (2019, February). Journalists as design partners for AI. In Workshop for accurate, impartial and transparent journalism: challenges and solutions. CHI 2019.\nGuzman, A. L. (2018). What is human-machine communication, anyway? In Guzman A. L. (Eds.), Human-machine communication: Rethinking communication, technology, and ourselves (pp. 1-28). New York: Peter Lang.\nHanitzsch, T., Van Dalen, A., & Steindl, N. (2018). Caught in the nexus: A comparative and longitudinal analysis of public trust in the press. The International Journal of Press/politics, 23(1), 3-23.\nJarvenpaa, S. L., & Leidner, D. E. (1999). Communication and trust in global virtual teams. Organization Science, 10(6), 791-815.\nJones, B., Jones, R., & Luger, E. (2022). AI ‘Everywhere and Nowhere’: Addressing the AI Intelligibility Problem in Public Service Journalism. Digital Journalism, 10(10), 1731-1755.\nKrishnan, A. (2016). Killer robots: legality and ethicality of autonomous weapons. Routledge.\nLuhmann, N. (1988). Familiarity, confidence, trust: Problems and alternatives. In D. Gambetta (Ed.), Trust: Making and breaking cooperative relations (pp. 94-107). New York, NY: Blackwell Publishing.\nMayer, R. C., James H., Davis and F. David Schoorman. (1995). An Integrative Model of Organizational Trust. Academy of Management Review, 20(3). 709-734.\nMcknight, D. H., Carter, M., Thatcher, J. B., & Clay, P. F. (2011). Trust in a specific technology: An investigation of its components and measures. ACM Transactions on management information systems (TMIS), 2(2), 1-25.\nMcKnight, D. H., Choudhury, V., & Kacmar, C. (2002). Developing and validating trust measures for e-commerce: An integrative typology. Information systems research, 13(3), 334-359.\nMeyerson, D., Weick, K. E., & Kramer, R. M. (1996). Swift trust and temporary groups. In R. M. Kramer & T. R. Tyler (Eds.), Trust in organizations: Frontiers of theory and research: 166-195. Thousand Oaks, CA: Sage.\nMiles, R. E., & Creed, W. E. D. (1995). Organizational forms and managerial philosophies: A descriptive and analytical review. In B. M. Staw & L. L. Cummings (Eds.), Research in organizational behavior, vol. 17: 333-372. Greenwich, CT: JAI Press.\nNewman, N., Fletcher, R., Kirsten, E., Robertson, C. T., & Nielsen, R. K. (2023). Reuters Institute digital news report 2023. Reuters Institute for the study of Journalism. Retrieved from https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2023\nOECD (May 2019). Recommendation of the Council on OECD Legal Instruments Artificial Intelligence. Retrieved from https://legalinstruments.oecd.org/en/instruments/oecd-legal-0449\nOpdahl, A. L., Tessem, B., Dang-Nguyen, D. T., Motta, E., Setty, V., Throndsen, E., Tverberg, A., & Trattner, C. (2023). Trustworthy journalism through AI. Data & Knowledge Engineering, 146, 102182.\nPorlezza, C. (2023). Promoting responsible AI: A European perspective on the governance of artificial intelligence in media and journalism. Communications, 48(3), 370-394.\nRousseau, D. M., Sitkin S. B., Burt R. S, Camerer C., Not so Different after a Cross-discipline View of Trust. Academy of Management Review, 23(1) 393-404.\nShapiro, D., Sheppard, B. H., & Cheraskin, L. (1992). Business on a handshake. Negotiation Journal, 8: 365-377.\nThe White House Office of Science and Technology Policy (February 2020). American Artificial Intelligence Initiative: Year One Annual Report. Retrieved from https://www.nitrd.gov/nitrdgroups/images/c/c1/American-AI-Initiative-One-Year-Annual-Report.pdf\nWu, X., Xiao, L., Sun, Y., Zhang, J., Ma, T., & He, L. (2022). A survey of human-in-the-loop for machine learning. Future Generation Computer Systems, 135, 364-381.   ## 더 읽을 거리\n\nAI 신뢰성 문제는 다양하게 제기되고 있다. 특히 AI 윤리와 관련된 최근 이슈를 알고 싶다면 다음 뉴스레터를 참조할 수 있다. AI 윤리 레터: https://ai-ethics.stibee.com/\n2023년 12월, 유럽 연합이 세계 최초로 TAI을 핵심으는 하는 인공지능법에 합의했다. 2024년 1월 5일 현재, 구체적인 내용은 아직 공개되지 않았다. 관련 내용은 추후 아래 링크에서 업데이트될 것으로 보인다. 유럽 연합의 인공지능법(AI act): https://artificialintelligenceact.eu/\nTAI 논의는 거대 담론에서 기술 구현을 논의하는 단계로 구체화되는 추세다. 그 예로는 아래의 보고서를 참고할 수 있다. 한국정보통신기술협회 (2023). &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;. 성남: 한국정보통신기술협회.\n저널리즘 분야의 인공지능에서 신뢰할 수 있는 인공지능의 적용 방향성을 모색한 연구로는 다음 논문을 참고할 수 있다. 박대민 (2023). 신뢰할 수 있는 인공지능 기반의 저널리즘 인공지능: 언론 신뢰와 인공지능 신뢰성 간 통약가능성을 바탕으로. &lt;언론과 사회&gt;, 31권 4호, 5-47."
  },
  {
    "objectID": "tai.html#생각해볼-문제",
    "href": "tai.html#생각해볼-문제",
    "title": "2  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "2.6 생각해볼 문제",
    "text": "2.6 생각해볼 문제\n\nAI의 신뢰성 문제를 야기하는 구체적인 사례는 어떤 것이 있는가?\n유럽연합의 인공지능법에서 TAI 관련 주요 쟁점 및 해법은 무엇이었는가? 해당 법안 이후 TAI를 각국 정부와 국내외 기업에서는 어떻게 수용하고 있는가? 그 한계와 대안은 무엇인가?\nTAI를 기술적으로 구현하려는 시도로는 어떤 것이 있는가? 이를 미디어 영역과 같이 구체적인 사회 영역에 적용할 수 있는가? 어떤 사회적, 기술적 기획이 필요한가?"
  },
  {
    "objectID": "fml.html#compas-알고리즘",
    "href": "fml.html#compas-알고리즘",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "3.1 COMPAS 알고리즘",
    "text": "3.1 COMPAS 알고리즘\n수학적으로 정의된 공정성 개념에 쉽게 접근하기 위해서 COMPAS라는 범죄자 프로파일링 소프트웨어의 유명한 사례에 대해 먼저 논의해 보자. COMPAS (Correctional Offender Management Profiling for Alternative Sanction)는 Northpointe(현 Equivant)라는 기업에 의해 개발된 프로그램으로 범죄자의 신상 정보를 이용하여 재범 가능성을 예측하는 기능을 가지고 있다. 이는 미국 뉴욕, 위스콘신, 캘리포니아 등의 지방 법원에서 보석 결정 등을 내리는데 보조 도구로 사용되었다. 미국의 탐사보도언론 프로퍼블리카(Propublica)는 COMPAS에 의해 내려진 결정을 분석한 결과, COMPAS에 의한 자동화된 결정이 인종에 따라 편향되었다고 폭로하였다.\n아래 테이블은 &lt;알고리즘이 지배한다는 착각Outnumbered&gt;이라는 책에서 저자 데이티드 섬프터가 원 데이터를 재구성한 알기 쉬운 예이다.1\n\n\n\n흑인\n고위험\n저위험\n합계\n\n\n\n\n재범○\n1,369\n532\n1,901\n\n\n재범✕\n805\n990\n1,714\n\n\n합계\n2,174\n1,522\n3,615\n\n\n\n\n\n\n백인\n고위험\n저위험\n합계\n\n\n\n\n재범○\n505\n461\n966\n\n\n재범✕\n349\n1,139\n1,488\n\n\n합계\n854\n1,600\n2,454\n\n\n\n위의 표에서 ‘고위험’이 의미하는 바는 COMPAS가 해당 범죄자에 대해 재범 가능성이 높다고 판정한 경우를, ’저위험’은 그 반대의 경우이다. 즉, 각 열(column)은 자동 분류 알고리즘의 예측 결과에 대응한다. 반면, 각 행(row)에 표현된 ’재범○’, ’재범✕’는 실제로 이후에 데이터에 포함된 범죄자들이 재범을 저질렀는지 여부를 의미한다. 즉, 이 테이블은 COMPAS 알고리즘의 재범 가능성에 대한 예측과 사후적으로 밝혀진 실제 재범 여부를 모두 제공하기에, 이를 이용해 알고리즘 예측의 정확성을 평가해볼 수 있는 지도학습 모형에 해당한다.\n프로퍼블리카는 COMPAS의 편향된 예측에 대한 우려를 표명하면서, 몇 가지 통계치를 제시하였다. 위의 데이터에서 가장 먼저 비판할 수 있는 바는, 흑인이 고위험군으로 판정될 확률은 60%(=2174/3615)인 반면, 백인이 고위험군으로 판정될 확률은 34.8%(=854/2454)에 불과하다는 것이다. 물론, 이는 실제 두 인종 간의 재범률 차이에서 기인한 것일 수도 있다. 하지만, 프로퍼블리카는 알고리즘의 정확도가 두 인종 간에 다르게 나타난다는 점에서 또 다른 우려를 표명한다. 예컨대, 흑인의 경우 데이터에 포함된 1,714명이 실제 재범을 저지르지 않았는데도 불구하고, 그 중 805명은 COMPAS가 고위험군으로 분류하여 46.9%%(=805/1714)의 위양성률(false positive rate)을 보인 반면, 백인은 23.5%(=349/1488)의 위양성률을 가지고 있다는 것이다. 즉, 위의 데이터에 따르면, COMPAS 알고리즘은 백인 범죄자에 대해서는 그들에게 불리한 오류를 발생시킬 확률이 더 적다.\n이에 대해 COMPAS를 개발한 Northpointe는 이와 다른 정확도 개념을 이용해 프로퍼블리카의 비판을 반박하였다. 예컨대, 고위험군으로 예측된 2,174명의 흑인 중 1,369명이 실제 다시 범죄를 저질러, 63%(=1,369/2,174)의 정확도를 보였으며, 고위험군으로 예측된 854명의 백인 중 505명이 재범을 저질러, 59%(=505/854)의 정확도를 보였기에, COMPAS알고리즘은 두 인종 집단 사이에서 정확도의 큰 차이를 보이지 않았다는 것이다.\n이와같이 혼란스러운 논란의 바탕에는 사실 공정성이라고 부를만한 다양한 기준이 있으며, 이들이 서로 모순될 가능성이 크다는 매우 근본적인 문제가 존재한다. 이를 좀 더 분명하게 이해하기 위해 이제 약간의 수학적 기호와 정의를 도입해보도록 하자."
  },
  {
    "objectID": "fml.html#공정성의-수학적-정의",
    "href": "fml.html#공정성의-수학적-정의",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "3.2 공정성의 수학적 정의",
    "text": "3.2 공정성의 수학적 정의\nCOMPAS 사례를 통해 살펴본 공정성 개념들을 수학적으로 일반화하기 위해 우리는 몇 가지 확률변수(random variable)2들을 정의할 것이다. 먼저, 우리가 예측하고 싶은 ’진실’을 \\(Y\\)라고 부르자. COMPAS의 예에서 \\(Y\\)는 실제 재범 여부에 해당한다. \\(Y=1\\)라는 확률변수 \\(Y\\)의 실현값은 재범을 실제 저질렀음을, \\(Y=0\\)은 반대의 경우를 의미한다고 하자. COMPAS가 예측을 할 때는 물론 재범 여부를 알 수 없지만, 위의 경우와 같이 사후적으로는 해당 데이터가 수집되어 \\(Y\\)를 알게되는 경우가 있고, 이를 통해 인공지능의 예측 결과의 정확성을 사후적으로 평가해볼 수 있다. \\(Y\\)와 달리, \\(R\\)은 알고리즘의 예측을 표현하는 확률변수라고 하자. 즉, 위의 예에서 COMPAS가 해당 범죄자를 재범 ’고위험군’이라고 평가한 경우에는 \\(R=1\\), ’저위험군’이라고 평가한 경우에는 \\(R=0\\)이라는 확률변수의 실현값으로 표현할 할 수 있을 것이다.\n이제 개개인이 가지고 있는 특성 역시 확률변수로 표현하자. 수학적 공정성의 정의는 먼저 ‘차별’이 발생할 수 있는 ’민감한 개인의 특성’, \\(A\\)와 그렇지 않은 일반적인 개인의 특성 \\(X\\)가 구분된다고 가정한다. COMPAS의 예에서 \\(A\\)는 인종에 해당할 것이다. 기호의 간결함을 위해 \\(A=a\\)는 개인이 흑인인 경우를, \\(A=b\\)는 개인이 백인인 경우를 의미한다고 가정하자. \\(X\\)는 민감속성 이외의 모든 다른 특성이 될 것이다. 지금까지의 정의를 모두 종합하면 아래와 같다.\n\n\\(Y \\in \\{0,1\\}\\): 예측하고자 하는 진실 (e.g. 실제 재범 여부)\n\\(R \\in \\{0,1\\}\\): 모형의 예측 (e.g. 고위험군/저위험군)\n\\(A \\in \\{a,b\\}\\): 민감한 개인의 특성 (e.g. 인종)\n\\(X\\): 민감하지 않은 개인의 특성 (e.g. 거주지역, 직업)\n\n물론 \\(Y\\), \\(R\\), \\(A\\)는 위와 같이 0/1 또는 a/b처럼 이분법적으로 구분될 필요는 없으며, 더 많은 카테고리가 존재하는 경우, 또는 해당 변수들이 연속적으로 변화하는 숫자일 경우로 확장할 수도 있다. 여기서는 설명의 간결함을 위해 위와 같이 단순하게 표현할 수 있는 경우에만 논의를 한정할 것이다.\n위에서 프로퍼블리카 제기했던 비판 중 첫 번째 비판-즉, 흑인을 고위험군으로 예측하는 경우가 백인을 고위험군으로 예측하는 경우보다 더 많다는 지적을 확률로 표현하면 다음과 같다.\n\\[P(R=1|A=a) &gt; P(R=1|A=b)\\]\n여기서 \\(P()\\)는 (괄호 안에 표시할) 어떤 사건의 확률을 타나내는 수학 기호이다. 예컨대 \\(P(R=1)\\)은 (인종을 막론하고) COMPAS가 고위험군이라고 예측할 확률을 의미한다. \\(P( | )\\)라는 조금 더 복잡한 기호는 \\(|\\) 기호 뒤에 표시된 어떤 조건 하에서의 확률, 즉, ’조건부 확률(Conditional Probability)’을 의미한다. 즉, \\(P(R=1|A=a)\\)은 ’인종이 흑인인 조건 하에서(\\(A=a\\)) 고위험군으로 예측(\\(R=1\\))될 확률’을, \\(P(R=1|A=b)\\)는 ’인종이 백인인 조건 하에서(\\(A=b\\)) 고위험군으로 예측(\\(R=1\\))될 확률’을 의미한다. 프로퍼블리카의 비판은 이 두 조건부 확률이 동등해야 ’공정한 알고리즘’이라는 생각을 전재한다. 이를 ’통계적 동등성(statistical parity)’로서의 공정성이라고 한다.\n프로퍼블리카의 COMPAS 알고리즘에 대한 두 번째 비판은 알고리즘의 ‘정확도’가 두 인종 사이에 달랐다는 점이었다. 구체적으로, ’위양성률(False Positive Rate)’, 즉, 실제 재범을 저지르지 않은 흑인들을 고위험군으로 잘못 분류하는 경우가 백인에게 동일한 오류가 발생하는 경우에 비해 지나치게 잦았다는 것이다. 이러한 비판을 조건부 확률의 기호로 표현하자면 다음과 같다.\n\\[P(R=1|Y=0, A=a) &gt; P(R=1|Y=0, A=b)\\] 여기서 \\(P(R=1|Y=0,A=a)\\)는 ‘실재 재범을 일으키지 않았고(\\(Y=0\\)), 흑인이라는 조건(\\(A=a\\)) 하에서’ 고위험군으로 예측(\\(R=1\\))될 확률이, ‘실재 재범을 일으키지 않았고(\\(Y=0\\)), 백인이라는 조건(\\(A=b\\))’ 하에서 고위험군으로 예측(\\(R=1\\))될 확률보다 높다는 것이다.\n반면, Northpointe의 재반박, 즉, 고위험군으로 분류된 사람 중에 실재 재범을 일으킬 확률은 흑은과 백인 사이에 거의 같다는 주장은 어떻게 표현할 수 있을까? 다음의 수식을 살펴보자.\n\\[P(Y=1|R=1, A=a) = P(Y=1|R=1, A=b)\\]\n여기서 \\(P(Y=1|R=1,A=a)\\) 라는 표현과 앞서 프로퍼블리카의 비판에서 보았던 \\(P(R=1|Y=0,A=a)\\)라는 표현을 비교해보자. \\(Y\\)에 해당하는 값이 0이냐, 1이냐, 즉, 재범을 일으키지 않은 경우에 주목하느냐, 재점을 일으킨 경우에 주목하느냐 라는 차이도 있지만, 더 중요한 차이는 \\(P(Y|R,A)\\)라는 표현은 \\(Y\\)가 조건문 앞에, \\(P(R|Y,A)\\)라는 표현은 \\(R\\)이 조건문 앞에 있어, 확률을 표현하고자 하는 사건과 조건 사이의 위치가 뒤바뀌었다는 것이다. 즉, 프로퍼블리카와 COMPAS는 두 개의 자른 조건부 확률에 주목하고 있는 것이다. 프로퍼블리카는 재범으 저지르지 않았는데 고위험군으로 분류되는 ’억울한 일이 발생하는 오류’에 주목했다면, COMPAS는 고위험군이라는 분류가 얼마나 잘 재범자를 찾아낼 수 있는지를 말해주는 ’성능’에 주목했다고 볼 수 있다. 이렇게 다른 조건부 확률에 주목하게 되면 공정성에 대해 다른 평가를 하게 된다. 아니, 더 정확하게는 이 사태가 보여주는 것은 우리가 일반적으로 ’공정성’이라고 부르는 개념 속에는 여러 가지 다른 개념이 포함되어 있다는 것이다.\n앞서 우리는 적어도 3개의 공정성 개념을 본 셈이다. 이제, 수학 기호를 이용해 이를 조금 더 일반화해 보자. 이러한 일반화 작업을 통해 사실은 무수히 많은 공정성 개념이 존재할 수 있다는 사실을 알 수 있다. 더 나아가, 안타깝게도 대부분의 경우 이러한 다수의 공정성 개념들은 동시에 성립 가능하지 않는 경우가 많다."
  },
  {
    "objectID": "fml.html#수학적-정의",
    "href": "fml.html#수학적-정의",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "3.3 수학적 정의",
    "text": "3.3 수학적 정의\n다음의 공정성 정의들은 위의 프로퍼블리카와 COMPAS 간의 논쟁에서 등장한 수학적 공정성 개념을 조건부 확률을 통해 일반화한 것이다.\n\n3.3.1 비인지(Unawareness)\n\\[P(R=1|X,A)=P(R=1|X)\\]\n이 수학적 정의에서 중요한 것은 등호 왼편에는 조건부 확률의 조건분에 \\(A\\)가 포함되어 있는 한편, 오른쪽에는 그렇지 않다는 것이다. 이것이 의미하는 바는, \\(A\\)라는 민감한 속성이 포함되어 있든 포함되어있지 않든, 알고리즘의 예측 결과는 같아야 한다는 것이다. 다시 말해, 알고리즘이 민감 속성을 ‘모른채(unaware)’ 예측을 생산해야 한다는 것이다.\n이러한 공정성 요건은 매우 직관적이다. 특정한 민감 속성을 고려한 차별적 예측이 문제를 발생시키는 것이라면, 해당 민감 속성으 고려하지 않으면 되는 것이 아닐까? 이러한 직관성으로 인해, 비인지 조건은 기존 공정성과 관련된 여러 법률들과도 잘 어울린다. Barocas와 Selbst가 매우 영향력 있는 2016년 논문에서 지적한 바와 같이, 자동화된 알고리즘 이전부터 내려온 공정성과 관련된 법률들은 예측에서의 불평등한 대우(disparate treatment) 또는 예측의 불평등한 효과(disparate impact)를 규제한다. 여기서 전자는 예측 또는 판단 ’과정’에서 여러 집단을 불평등하게 다루어서는 안 된다는 것을 의미하기에, 실질적으로는 예측 대상이 특정 민감 속성을 가졌는지를 예측 과정에 포함시켰을 경우 이를 불평등 대우로 여기고 처벌하는 방식의 법 논리를 가지게 된다. 즉, 민감속성의 비인지를 요구하는 것이다. 이는 과거 법을 인공지능을 위한 예측에 그대로 활용할 수 있다는 대단히 큰 장점이 있다.\n그러나, 이러한 공정성의 요구에는 치명적인 단점이 있다. 바로 민감 속성과 통계적으로 상관관계를 갖는 대리변수들이 존재할 수 있다는 것이다. 예컨대, 특정 지역에 특정 인종이 집중적으로 거주한다면 (에미넴이 출현한 영화 &lt;8마일&gt;에 나오는 디트로이트를 떠올려보자), 우편번호라는 전혀 민감해보이지 않는 속성은 인종이라는 민감한 속성을 ‘높은 확률로’ 알아낼 수 있는 ‘대리변수(proxy)’가 된다. 특히, 과거에 비해 훨씬 더 많은 변수를 예측에 한꺼번에 사용할 수 있는 ’빅데이터’를 통한 예측 환경에서는 매우 많은 대리변수가 존재해서 민감 속성으 거의 정확하게 추론할 수 있는 경우가 많다. 따라서, 단순히 민감속성을 예측에 사용해서는 안 된다는 비인지 조건은 알고리즘의 공정성을 달성하기에는 ’지나치게 약한’ 조건일 가능성이 크다. 그 때문에, 많은 현대의 공정성 조건들을 이러한 대리변수의 존재를 전제한 경우가 많다.\n이는 어떤 의미에서는 절차적 공정성을 의미한다.\n\n\n3.3.2 통계적 동등성(Statistical Parity)\n\\[P(R=1|A=a)=P(R=1|A=b)\\]\n통계적 동등성은 문헌에 따라서, 독립성(Independence), 또는 인구적 동등성(demographic parity), 집단 공정성(group fairness)라고 부르기도 한다.\n이 정의는 프로퍼블리카의 COMPAS에 대한 첫번째 비판에서 이미 본 것으로, 집단 사이에 예측의 차이가 없어야 한다는 것을 의미한다. COMPAS와 조금 다른 예를 들자면, 자동화된 알고리즘이 대출 승인을 해 주는 경우, 대출 확률이 남성과 여성 간에 차이가 없어야 한다는 것이다. 이는 앞서 Barocas와 Selbst의 구분에서 두 번째 종류의 공정성 법률, disparate impact 요건과 관련된다. 즉, 예측 과정이야 어떻든, ‘결과적으로’ 집단 간에 차이가 없어야 한다는 주장이다.\n많은 경우, 이러한 요건은 글자 그대로 적용된다기 보다는, 근사식 형태로 요구된다. 즉, 두 집단 사이의 예측치 차이가 ’지나치게 커서는 안 된다’는 식이다. 이러한 방식은 기존 법안의 공정성 법률에서 종종 관찰되는데, 미국 법률의 four-fifth rule에 해당한다. 80%의 법칙이라고 옮겨도 좋을텐데, 어떤 집단도 가장 선정 확률이 높은 집단에 비해 선정확률이 80%이하로 내려가서는 안 된다는 것이다. 즉, 대출 승인률이 남성에게 50%라면 여성에게 적어도 40%는 되어야 한다는 것이다.\n어떤 과정을 통해서 예측을 하던간에 집단 간 같은 정도의 예측 결과를 요구한다는 점에서 통계적 동등성은 매우 강한 공정성 요건인 것처럼 보인다. 하지만, 해당 요구사항에는 몇가지 문제점이 있다. 가장 쉽게 생각할 수 있는 문제는, 실제 예측하고자 하는 진실 \\(Y\\)와 민감한 속성 \\(A\\) 사이에 상관관계가 있을 수 있다는 점을 고려하지 않는다는 것이다. 예컨대, 흑인과 백인 사이의 ‘평균적인’ 대출상환능력에는 실제 20% 이상의 차이가 있을 수도 있다. 이에 대해 미리 20%의 상한을 결정하는 것은 은행으로 하여금 손해를 감수할 것을 요구하는 셈이다. 물론, 이는 장기적인 사회적 변화를 위해서 기업이 감수해야 할 몫이라고 합의할 수 있는 부분이라고 할 수 있다.\n하지만, 더 중요한 문제는 이른바 ‘laziness’라는 문제이다. 즉, 평균적으로 예측치의 비율만 집단간 유사하게 맞추면 되기 때문에 예측이 얼마나 중요한지는 중요하지 않다는 것이다. 예를들어, 통계적 동등성에 따라 앞서의 예와 같이 40%의 대출 승인 비율을 준수해야 하는 집단이 있다면, 이들에게 얼마나 상환 능력에 따라 대출 승인을 해야 하는지는 통계적 동등성과 아무런 관련이 없다. 극단적으로 40%의 확률로 윗면이 나오는 동전을 던져서 랜덤하게 대출승인을 해 준다고 해도 통계적 동등성은 달성할 수가 있는 것이다. 이러한 문제점을 해결하기 위해서는 ’정확성’ 개념이 공정성 안으로 들어올 필요가 있는데, ’정확성’이 예측 결과(\\(R\\))가 실제(\\(Y\\))를 얼마나 잘 맞추는가에 대한 측정치인 이상, 공정성 정의 안에 \\(Y\\)가 포함되어야 한다는 것으 의미한다.\n또 한 가지 근본적인 문제점은 세상에는 실제 불평등이 존재한다는 것이다.\n앞으로 이야기할 공정성 개념은 주로 이 두 가지 문제를 해결하기 위한 개념들이라고 볼 수 있다.\n이러한 두 가지 문제점은 사실 여기서 핵심은 두 가지 문제이다. - Y를 도입해서 정확성에 대한 고려가 필요하다는 것. - 실제 세상에는 불평등이 존재한다는 것. 이 두 가지 문제는 앞으로도 계속 문제가 되고 회피하기 어렵다.\n\n\n3.3.3 분리성(Seperation)\n\n정확성이 왜 중요한가 - 고릴라 사태를 생각해봐라….\n\nequalized odd, 정확도 동등성(accuracy parity)\n참양성 동등 \\[P(R=1|Y=1, A=a)=P(R=1|Y=1, A=b)\\]\n아래는 위양성동등 \\[P(R=1|Y=0, A=a)=P(R=1|Y=0, A=b)\\]\n참음성 동등. \\[P(R=0|Y=0, A=a)=P(R=0|Y=0, A=b)\\]\n아래는 위음성동등 \\[P(R=0|Y=1, A=a)=P(R=0|Y=1, A=b)\\]\n이제 조건문에 \\(Y\\)가 등장했다. 이는 특정 정확도, 즉 진실에 대비해서 얼마나 예측치가 정확하게 결과를 알아냈는지에 대한 비율을 표현하는 네가지 정확도, 참양성률(True Positive Rate; TPR), 위양성률(False Positive Rate; FPR), 참음성률(True Negative Rate; TNR), 위음성률(False Negative Rate; FNR) 등이 집단간에 동등할 것을 요구하는 공정성의 정의이다.\n예컨대 참양성 동등의 경우, 민주주의에서 강조하는 특정한 공정성 개념, ’기회의 평등’과 직접적인 연관을 갖는다. 왜냐하면, 실재로 맞춤한 능력을 갖는 사람(\\(Y=1\\))이 알고리즘에 의한 예측에 의해 기회를 얻을(\\(R=1\\))확률이 집단 간에 동일(\\(P(R=1|Y=1,A=a)=P(R=1|Y=1,A=b)\\))해야 한다는 것이 바로 기회의 평등의 개념이기 때문이다.\n문제는 잘 기계학습에서이 네 가지도 하나가 성립한다고 해서 다른 정확도가 함께 올라가리라는 보장이 없을 뿐더러, 어떤 경우에는 상호 충돌하여 한 가지 정확도가 개선되면 다른 정확도는 하락하는 트레이드오프(trade-off) 관계가 성립하는 경우가 많다는 것이다. 따라서, 실질적으로는 여러 정확도 개념 중에서 예측의 대상이 되는 사람들에게 ’억울한 오류’를 막기 위한 정확도, 또는 오류율 개념에 주목하는 경우가 많다. 예컨대, 재범 위험성이 큰 사람은 실재 재범을 저지르지 않음(\\(Y=0\\))에도 ’양성(positive; \\(R=1\\))’으로 예측되는 경우 ’억울한 피해’가 발생할 수 있으므로, 잘못된 양성 판정, 즉, 위양성률에 주목하여, 집단 간에 위양성률이 동등하도록 하는 것을 최우선으로 한다. 반면, 대출 상환 능력을 예측하여 이를 바탕으로 대출 승인을 하는 경우, 실재로 대출을 갚을 수 있는 능력이 있음에도(\\(Y=1\\)), 대출을 승인을 허용하지 않는 음성 판정(\\(R=0\\))을 하는 경우, 이는 대출 신청자 입장에서는 억울한 판정이므로, 이렇게 잘못 음성 판정을 내릴 확률, 즉, 위음성률을 집단간에 동등하게 하는 것이 공정한 알고리즘의 우선적인 선택이 된다.\n여기서 ’우선적 선택’이라는 표현을 이해하는 것이 중요하다. 알고리즘의 공정성을 확보하고자 하는 조직은 결국 여러 공정성 개념 중에서 더 중요한 개념과 덜 중요한 개념 사이에서 조직의 목표에 따라 균형을 맞추는 선택을 해야 한다는 것이다. 이는 파래토 원리에 따른 균형이라는 마지막 테마와 직접적으로 연결된다.\n기본적으로 이 관점은 차별적 현실을 인정하자는 관점. 있는 능력 차이를 인정하고 그 안에서 정확도만 동일하면 된다…\n‘최우선’ -&gt; 결국 이것은 선택, 그리고 파레토….\n여기서는 비지도 학습은 뺄 것.\n\n\n3.3.4 충분성(Sufficiency)\n프로퍼블리카의 비판에 대한 COMPAS의 반비판, 즉, 재범 위험이 크다고 예측한 사람 중에서 실재 재범을 일으킨 확률이 인종 집단 간에 크게 차이나지 않는다는 주장은 분리성과는 상이한 정확도 개념을 이용하여 공정성을 정의하는 관점에 바탕을 두고 있다. 즉, 알고리즘이 양성 또는 음성이라고 예측했을 때 그것이 얼마나 실재와 일치하는지, 일종의 ’예측의 성능/성과’에 초점을 맞추는 관점이다. 충분성은 문헌에 따라 예측률 동등(Predictive Rate Parity)라고 부른다. 이를 수식으로 표현하면 다음과 같다.\n양성예측동등(Positive Predictive Parity) \\[P(Y=1|R=1,A=a)=P(Y=1|R=a,A=b)\\] 음성예측동등(Negative Predictive Parity) \\[P(Y=0|R=0,A=a)=P(Y=0|R=0,A=b)\\]\nRecall과 Precision 차이에 해당. -&gt; 머신러닝에서 이 두개는 전혀 동등하지 않으며, 대부분 동시에 성취 불가능한다.\n단점은 분리성과 유사하게 기존의 차별을 용인하는 문제.\n사실은 이러한 공정성 개념이 전부는 아니다. 위의 공정성 개념들을 일부 수정하고자 한 것. 예컨대 마이크로소프트의 Cynthia Dwork(윤리적 알고리즘 분야에서 최고의 권위를 가지고 있는 미국의 여성 컴퓨터공학자)와 동료들이 제시한 개인적 공정성(Individual Fairness)은 비슷한 특성을 가지고 있는 개인이 다른 예측 결과를 경험해서는 안 된다는 의미에서의 공정성 개념으로, 우리가 앞서 살펴본 공정성 개념들이 집단간 비교를 통해 정의되는 것에 비해서, 각 개인에게 공정한 경험을 보장하려고 하는 시도라는 점에서 의의가 있다. 또한, 영국의 앨런 튜링 연구소의 Chris Russell과 동료들이 제안한 인과적 공정성(Causal Fairness)은 민감한 속성이 다른 비민감 속성의 ’원인’으로 영향을 미쳐 최종적으로 예측 결과에 영향을 미치는 메커니즘을 고려하고자 하는 공정성 개념이다. 이러한 분야는 여전히 많은 발전을 이루고 있고, 논의되고 있는 측면이 커 여기서는 생략하도록 한다."
  },
  {
    "objectID": "fml.html#불가능성-정리-impossibility-theorem",
    "href": "fml.html#불가능성-정리-impossibility-theorem",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "3.4 불가능성 정리 (Impossibility Theorem)",
    "text": "3.4 불가능성 정리 (Impossibility Theorem)\n이렇게나 많은 공정성 개념이 존재한다는 것은 그 자체로 문제이지만, 코넬의 정보과학 대가인 Jon Kleignberg등이 명명한 바, 이른바 ’불가능성 정리’라고 부르는 수학적 결과는 더욱 더 큰 난점을 제기한다. 이 불가능성 정리에 따르면, 우리가 위에서 살펴본 공정성 정의들은 수학적으로 동시에 성립할 수 없다는 것이다. 이를 다시 표현하자면, 우리가 말하는 공정성이라는 용어 속에는 기실 다양한 차원의 공정성 개념들이 숨어 있는데, 이들은 본질적으로 서로 모순된다는 것이다. 따라서, 프로퍼블리카가 ’분리성’에 근거하여 제기했던 비판과 ’충분성’에 근거해서 Northpointe가 제시한 반박은 애초부터 엇갈릴 수밖에 없고, 그 입장 차이는 해소할 방법이 없다는 것이다!\n그렇다고 해서 이러한 결론이 공정성은 우리가 성취할 수 없는 목표라는 것을 의미하는 것일까? 그렇지 않다. 사실, 이렇게 서로 모순되는 공정성 개념이 우리의 ’공정성’이라는 용어의 일상적 용법에 숨어있었다는 것을 알아내는 것은 후퇴라기 보다는 한발 전진이라고 할 수 있다. 문제 해결의 전제는 우리가 마주한 문제가 무엇인지를 아는 것이기 때문이다. 이렇게 사회과학적 영역에서 수학적 조작화를 통해 인간 언어의 모호성을 극복하고 다양한 차원들이 존재함을 드러내서 더 나아가 일종의 불가능성 정리에까지 도달하는 것은 처음이 아니고, 오히려 사회과학의 진전에서 중요한 역할을 하였다.\n예컨대, 경제학, 정치경제학…. (내 영어논문에서…)"
  },
  {
    "objectID": "fml.html#파레토-경계를-통한-균형-찾기",
    "href": "fml.html#파레토-경계를-통한-균형-찾기",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "3.5 파레토 경계를 통한 균형 찾기",
    "text": "3.5 파레토 경계를 통한 균형 찾기\n문제가 양립할 수 없는 다수의 목표들이라면, 결론은 결국 목표들 간의 취사선택 또는 타협이 될 것이다. 그리고 수학저 조작은 ‘체계적 타협’을 위한 도구까지 마련해준다(Kearns). 이러한 타협을 달성하는데 핵심적으로 사용하는 개념적 도구는 파레토 효율성(Pareto Efficiency)라는 개념이다. 이 개념은 19세기와 20세기초에 주로 활동한 빌프레도 파레토(Vilfredo Pareto)라는 이탈리아의 사회학자이자 경제학자의 이름을 딴 것으로, 파레토 효율성 말고도 파레토의 이름이 붙은 다른 유명한 개념들도 많이 있습니다. 파레토 효율성은 사회적 최적상태(optimality)를 규정하는 하나의 방식으로, 대략적으로 “어느 누군가의 희생 없이는 다른 사람들의 상태에 대한 개선이 불가능한 상태”를 의미한다고 볼 수 있습니다. 이를 조금 더 일반화하자면, “하나의 목표에 대한 양보 없이 다른 목표들의 개선이 불가능한 상태”라고 생각할 수 있습니다. 그런데, 이러한 개념에 대한 설명이 그다지 ’최적상태’ 또는 ‘효율성’ 등의 표현과 어울리지 않는 것처럼 보이는 것도 사실이다. 사실 파레토 효율이라고 부를 수 있는 상태는 무수히 많이 존재한다. 오히려 파레토 효율성의 의의는 파레토 효율성 관점에서 ‘열등한’ 사회적 선택을 찾아내는 기준이 된다는 점에 있다고 볼 수 있다. 이를 이해하기 위해서 다음 그래프를 살펴보자.\n아래 그림은 다시 그리는 것이 나을 수 있겠다…\n Ho, D. E. & Xiang, A. (2020). Affirmative algorithms: The legal grounds for fairness as awareness,\nhttps://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/\n위의 그림은 자동화된 알고리즘을 이용하는 가상의 조직이 정확도(중 하나의 지표)와 공정성(중 하나의 지표)를 자동화된 판단을 통해 다렁하고자 하는 두 가지 목표라는 것을 가정하고 있다. 그래프 상의 각 점들은 그 조직이 선택할 수 있는 예측 알고리즘들이 성취할 수 있는 정확도와 공정성의 조합을 의미한다. 이제 가장 바깥에 있는 점들을 연결한 선을 살펴보자. 이 선 위에 존재하는 알고리즘은 ‘파레토 효율적’이다! 왜 그런가 예컨대 해당 선 위에 있는 알고리즘 A를 생각해보자. 이 알고리즘보다 공정성이 높은(즉, A보다 오른쪽에 위치한) 알고리즘을 선택하려면 어떻게 해야 할까? 일단, A가 위치한 경계선 오른쪽 바깥에 있는 알고리즘은 기술적인 한계로 인해 그 조직이 선택할 수 없는 알고리즘이다. 따라서, 즉, A에서 공정성과 정확성을 ’동시에’ 개선할 방법은 없다! 따라서, 공정성을 개선하기 위해서는 정확도를 희생하면서, 경계선 상 혹은 안쪽에 있는 다른 알고리즘을 선택하는 수밖에 없다. 같은 논리에 따라, 반대로 A알고리즘에서 정확성을 개선한 알고리즘을 선택하고 싶다면, 공정성을 다소 희생하는 수밖에 없다. 이제 “하나의 목표에 대한 양보 없이 다른 목표들의 개선이 불가능한 상태” 라는 파레토 효율성의 정의를 생각해보면, A는 정확히 해당 정의에 부합하는 알고리즘이라는 것을 알 수 있다. 사실, 이러한 논리는 경계선 상에 있는 모든 알고리즘에 적용된다. 따라서, 해당 경계선 상에 존재하는 알고리즘은 모두 ’파레토 효율적’인 알고리즘이다! 이러한 중요성 때문에 해당 경계선을 특별하게 ’파레토 경계(Pareto Frontier)’라고 부릅니다.\n이제 파레토 경계 안 쪽에 있는 알고리즘들, 예컨대 B와 같은 알고리즘에 대해 생각해보자. B와 같은 알고리즘은 정확도와 공정성을 ‘동시에’ 개선할 수 있다. 왜냐하면 B에서 오른쪽 상단 방향으로 파레토 경계 안 쪽에 더 좋은 알고리즘이 존재하기 때문이다. 이렇게 두 가지 목표를 동시에 개선하느 것을 ‘파래토 개선(Pareto Improvement)’라고 부른다. 이렇게 파레토 개선이 가능한 알고리즘은 어떠한 희생도 없이 개선이 가능하므로, 개선된 알고리즘에 비해 확실히 ’열등한’ 알고리즘이라고 볼 수 있다. 따라서, 파레토 개선이 가능한 알고리즘을 ’파레토 열등(Pareto Dominated)’하다고 한다. 이제, 파레토 효율적인 알고리즘(=파레토 경계상의 알고리즘)은 파레토 개선이 불가능한 알고리즘이라는 것을 알 수 있고, 파레토 경계 안 쪽에 존재하는 알고리즘은 확실히 파레토 열등한 알고리즘이라는 것을 알 수 있다. 따라서, 이러한 파레토적 사고 방식을 통해 단 하나의 최적 알고리즘을 알 수 없다고 하더라도, 적어도 파레토 열등한 알고리즘이 아니라, 파레토 곡선 상의 파레토 효율적 알고리즘 중에 선택이 이루어져야 한다는 것을 알 수 있다!\n그렇다면 파레토 경계상의 많은 알고리즘 중에는 어떠한 알고리즘을 선택해야 하는가? 여기서부터는 조직이 여러 목표들 사이의 경중을 어떻게 평가할 것인가에 대한 합의된 기준이 필요하다. 즉, 다수의 목표를 동시에 가장 최적 수준에서 달성할 수는 없으므로, 목표들 간의 ‘타협’이 이루어져야 하며, 이는 조직의 형태, 목적, 구성원의 생각, 제도 등에 따라 다를 수 있다는 것이다. 이러한 타협 역시 파레토 경계 위에서 단순히 ’감으로’ 이루어질 필요는 없다. 별도로 자세하게 다루어야 하는 내용이므로, 여기서 자세하게 다루기는 어렵지만, 해당 조직이 여러 목표 간에 어떻게 타협하는 것을 최선이라고 생각하는가, 혹은 그렇게 합의했는가를 일종의 조직의 선호를 나타내는 수학적 ’함수’로 표현할 수 있다. 그리고 그 함수는 다음과 같이 그래프로 표현 가능하다.\n\n\n\n무차별 곡선을 이용한 알고리즘 선택\n\n\n위의 그림에서 조직의 선호를 나타내는 곡선은 ‘무차별 곡선(indifference curve)’이라고 하는데, 해당 무차별 곡선의 형태는 정확성과 공정성 간의 타협에 대한 조직의 선호에 따라 달라진다. 이제 ’주어진 선호/합의’ 하에서 최적 알고리즘은 두 그래프가 만나는 점에서 결정된다. (경제학원론을 배운 학생이라면 이러한 사고 방식에 익숙할 것이다. 보지 못한 경우라도, 경제학원론 교과서의 소비자효용, 후생경제학 부분을 훑어보길 바란다. 이러한 선호 표현 방식과 그를 통한 의사 결정의 문제를 다루는 분야를 의사결정과학Decision Science이라고 부른다). 만약, 더 많은 정확성과 더 많은 공정성 개념을 고려하여 타협을 하고자 한다면, 위의 파레토 경계와 파레토 곡선은 더 이상 2차원의 그래프로 표현할 수는 없을 것이다. 그러나 시각화하기 어려운 고차원의 선택 문제라고 하더라도 동일한 논리를 통한 선택은 여전히 가능하다.\n지금까지 훑어본 다양한 공정성의 개념과 양립불가능성 정리, 그리고 파레토 효율성을 통한 알고리즘 선택 과정은 알고리즘 공정성에 대한 과학적 논의가 던져주는 부정적 전망과 부정적 전망을 동시에 보여준다. 모두 합리적으로 들리는 여러 차원의 공정성을 동시에 달성할 수 없다는 것, 그리고 공정성과 예측의 정확성 역시 자주 충돌한다는 것은 인간사회가 인공지능의 자동화된 예측과 결정에 의존하는데 위험성이 수반된다는 것을 의미한다. 하지만, 이러한 양립불가능성은 자동화된 결정시스템에 고유한 것이 아니다. 인간에 의해서 동일한 결정이 이루어진다고 하더라도, 동일한 문제는 발생한다. 다만, 그 전에는 이렇게 엄밀한 수학적 접근을 시도하지 않았기에 우리가 가지고 있는 딜레마를 정확히 보고 있지 못했을 뿐이다. 오히려 공정성의 다양한 정의를 밝히는 작업은 조직의 합의된 목표와 그 안에 포함된 타협을 체계적으로 알고리즘 선택에 반영할 수 있는 도구를 제공한다는 점에서 진보라고 할 수 있다. 우리에게 이러한 도구들이 있는 한, 선택은 늘 가능하다. 중요한 것은 사람과 조직이 좋은 목표를, 좋은 타협안을 만들어내는 것이다. 사람의 일을 기계가 대신 해 줄수는 없는 일이라는 교훈은 또 다시 반복된다.\n전체 워크플로우를 보여주는 그림"
  },
  {
    "objectID": "fml.html#남은-문제들",
    "href": "fml.html#남은-문제들",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "3.6 남은 문제들",
    "text": "3.6 남은 문제들\n우리가 위에서 살펴본 공정성 개념들은 ‘사후적으로라도’ 정답을 알 수 있는 기계학습, 즉, 지도학습(Supervised Machine Learning)을 사용하는 경우 예측 결과의 공정성을 개선하기 위해 사용할 수 있는 공정성 개념의 수학적 조작화라는 주제에 국한된 것이었다. 이외에도 인공지능 공정성을 다루는 다른 연구 영역들이 여전히 존재한다. 여기서는 교재의 목적상 이 모든 논의를 담지는 못하고, 그러한 논의들이 발전되고 있다는 것 정도를 지적하도록 한다. 관심있느 독자들은 챕터 말미의 ’더 읽을거리’와 ’참고문헌’을 참조하기를 바란다.\n\n3.6.1 비지도학습의 공정성\n가장 먼저 생각해볼 수 있는 것은 비지도학습(Unsupervised Machine Learning)에서의 공정성이다. 비지도학습은 우리가 위의 논의 속에서 사후적으로라도 알게 되는 것으로 가정했던 예측하고자 하는 진실, \\(Y\\)가 존재하지 않는 경우의 기계학습을 의미한다. 예컨대 온라인 소비자의 행동 데이터로부터 충성도가 높은 소비자를 찾아내는 상황을 생각해보자. ’충성도 높은 소비자’라는 것은 어차피 마케터의 관점에서 존재하는 관념일 뿐, 이를 명확하게 정의하기 어렵기 때문에, 사후적으로라도 ’충성도 높은 소비자’가 누구였는지를 알아내는 것은 어렵다. 이러한 경우에는 비슷한 행동을 보이는 소비자들끼릴 묶어내는 ’군집화(Clustering)’라는 비지도학습 기술을 사용한다.\n또 최근 주목받는 예로 인공지능에 의한 기계번역을 하는 경우를 떠올려보자. 언어의 요소(예컨대 단어)를 다차원의 수치로 표현한다음, 이 수치가 가까운 다른 언어의 요소로 대체하는 방식을 이용한다. 예컨대, ’개’와 ’dog’는 다른 언어에 속한 단어이지만, 숫자로 표현된 두 단어의 거리는 가깝기에 이를 이용한 컴퓨터는 번역을 할 수 있는 것이다. 이렇게 언어를 다차원의 수치로 표현한 것을 ’임베딩(Embedding)’이라고 하는데, 단어의 임베딩을 찾아내는 작업 역시 비지도학습에 해당한다. 사실 임베딩은 자동 번역뿐 아니라, 언어를 음악으로, 그림으로, 동영상으로 ’번역’하는 기술에도 다양하게 사용된다.\n임베딩 그림\n정확성에 기반한 공정성 개념이였던 분리성, 정확성의 수식에 \\(Y\\)가 포함되었었다는 것을 상기해보면, 그러한 공정성 개념은 \\(Y\\)가 존재하지 않는 비지도학습에는 적용되기 어려울 것이다. 그럼에도 불구하고, 비지도학습에는 다양한 공정성 문제가 발생한다. 예컨대. ’여성’이라는 단어의 임베딩은 ’간호사’의 임베딩에 더 가깝고, ’남성’이라는 단어의 임베딩은 ’의사’의 임베딩에 더 가깝게 도출된다면, 이는 임베딩에 편향이 반영되어있다는 것을 의미한다. 그리고 이는 인공지능이 이용하는 학습데이터, 즉, 인간이 만들어낸 데이터가 편향된 이상 인공지능을 자동으로 해결해주지 않는다. 따라서, 최근에는 이를 해결하기 위한 다양한 공학적 시도들이 있다. 관심있는 독자들은 키언스의 책이나, Bolukbasi 등 (2016) 등의 논문을 살펴보기 바란다.\n편행된 임베딩 그림\n\n\n3.6.2 데이터의 편향성\n또 한가지 중요한 주제는 데이터 그 자체가 가지는 편향이다. 앞서 언급한 바와 같이, 인공지능은 인간의 오류와 편견을 그대로 반복하거나, 더 나쁘게는 확대재생산한다. 이렇게 인간의 편향이 인공지능의 편향에 스며드는 가장 영향력 있는 통로는 알고리즘의 복잡한 디자인이라기 보다는 데이터 그 자체이다. 2016년, Barocas와 Selbst는 California Law Review에 데이터에 편향이 숨어드는 경로에 대한 매우 영향력 있는 논문을 게재하였다. 그러한 경로는 매우 다양해서, ‘좋은 근로자’, ’높은 금융 신용도’와 같이 알고리즘을 통해 예측하고자 하는 목표 그 자체를 정의하는데 있어 편향이 포함될 수 있다. 예컨대 ’좋은 근로자’를 정의하면서 흔히 남성성과 결부되는 속성을 포함시키는 경우가 그러하다. ’좋은 근로자’가 중립적으로 잘 정의되었다고 하더라도, 기존 사원 기록에서 특정 근로자를 좋은 근로자로, 특정 근로자를 부족한 근로자로 인간이 ’레이블링’하는 인간의 판단에도 편향이 포함될 수 있다. 예컨대, 과거 인사평가 기록을 인공지능의 학습데이터로 이용했는데, 남성 인사 평가자가 남성 사원에 대해 유리한 판단을 해 왔다면, 이러한 학습데이터를 이용한 인공지능은 편향적인 결정을 생산할 것이다. 또한, 데이터 수집 과정에서 특정 그룹이 과도하게 많이 데이터에 포함되거나, 적게 포함된 경우에도 데이터가 편향을 가지게 된다. 예컨대, 특정인종의 거주지역에 집중적으로 순찰을 수행하면서 발생한 기록을 인공지능의 학습 데이터로 이용했을 때(과대표집), 해당 인종의 범죄 위험을 높게 평가한다든지, 소수인종의 인터넷 이용 데이터가 다수인종에 비해 상대적으로 적어(과소표집) 소수 인종에 대한 예측이 지나치게 부정확한 예가 그렇다. 또, 직접적을 차별을 하지 않더라도, 특정 집단에게 유리한 개인 특성(예컨대, 출신대학, 출신지역 등)을 학습에 과도하게 많이 포함시키는 경우에도 편향이 발생할 수 있다.\n\n\n3.6.3 개선 방법 (Fair Algorihtm)\n마지막으로, 인공지능의 편향 위험이 관측되었을 때, 이를 수정하기 위한 방법에 대한 논의 역시 활발하다. 실질적으로 어떻게 인공지능을 어떻게 더 공정하게 만들 수 있을 것인지에 관한 논의이므로, 최근 컴퓨터공학자들의 노력은 이 부분에 집중되고 있다. 가장 기초적으로는 데이터에 대한 사전작업을 통해서 데이터에 숨은 편향성을 미리 교정한 후(전처리; preprocessing) 인공지능을 학습시키는 방법 (대표적으로는 Zemel et al.의 2013년 ICML 페이퍼), 인공지능 학습시, 위의 논의에서 정의한 공정성 요구조건을 일종의 학습의 제약으로 포함시키는 방법 (대표적으로는 Zafal et al.의 AISTAT 2017년 페이퍼), 인공지능의 학습이 종료된 후에 공정성 정의를 이용하여 학습 내용을 수정하는 방법 (이른바 후처리;postprocessing)으로 나눌 수 있다. 이 분야는 기술적인 지식을 요구하므로, 본 챕터에서 자세하게 다룰 수는 없지만, 기계하습의 원리에 대한 다소간의 이해가 있다면, 위에서 논의한 공정성의 정의들이 이러한 ‘공정한 알고리즘’ 개발에 직접적으로 사용되고 있다는 것을 알 수 있다.\n위의 정의들이 이러한 개선방법을 만들어내는데 도구로 포함된다는 것이 중요하다.\n요건 블로그 보면서 해결…\n해법: Preprocessing 등등… -&gt; 이 역시 ’그 외 토픽’에 포함시키는 것이…\n\n데이터셑의 공정성: Barocas & Selbst"
  },
  {
    "objectID": "fml.html#결론",
    "href": "fml.html#결론",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "3.7 결론",
    "text": "3.7 결론\n이 장에서는 활발하게 공정한 인공지능(FML) 논의에서 ’지도학습 맥락에서 공정성의 수학적 정의’를 주로 살펴보았다. 이러한 공정성 개념에 대한 수학접 접근을 통해 알게된 결론은, 우리가 일상적으로 사용하는 ’공정’의 개념에는 서로 모순되는 다른 차원의 공정성 개념이 포함되어 있다는 것이다. 이는 일견 우리가 일반적으로 옳다고 여기는 가치를 동시에 성취 불가능하다는 부정적 결론으로 보일 수 있지만, 꼭 그렇지는 않다. 오히려 수학적 접근을 통해 우리가 이미 가지고 있던 개념의 모호성을 더욱 분명하게 함으로써 논의를 한걸음 진전시키는 것이라고 볼 수 있다. 구체적으로, 공정한 인공지능 논의를 통해 공정성은 더 발달한 인공지능이 기술적으로 해결할 수 있는 것이 아니라, 모순되는 목표들 간에 인간이 도출해내야 하는 타협과 합의를 통해서 성취가능한 것이라는 것을 알게 되었으며, 그러한 타협과 합의가 존재한다면 공정성의 수학적 정의는 이를 컴퓨터가 이해할 수 있는 언어로 번역함으로써 알고리즘을 인간의 결정에 부합하도록 체계적으로 개선할 수 있는 통로를 마련해주며, 대안으로 개발되고 있는 공정한 인공지능 알고리즘들은 대부분 이러한 수학적 정의에 의존하고 있다.\n지금까지 본 바와 같이 인공지능의 편향은 인간 세계에 존재하는 편향이 반영된 것이며, 이를 해결하기 위한 방법 역시 인간의 합의에 달려있다. 수학적/공학적 논의는 인간 세계가 생각하는 공정을 기계가 이해할 수 있도록 하는 통로를 마련하는 것에 불과하다. 단, 이렇게 ㄷ 인공지능의 공정성을 인간의 불공정성의 단순한 반영으로 생각하는 일종의 수동적 접근을 전재한다. 이러한 수동적 접근은 분리성, 충분성의 정의에서 본 것과 같이 이미 세상에 존재하는 불평등은 주어진 것으로 받아들이는 경향이 있다. 그러나, 인간 세계의 불평등을 그대로 받아들이고 있는 인공지능이 인간의 개입없이 판별하는 영역이 늘어나면 늘어날 수록 그러한 불평등은 확대재생한 될 공산이 크다. 즉, 읟적인 교정을 위한 장치가 존재하지 않는 자동화된 결정은 현상 유지를 넘어서 사태를 악화시킬 가능성이 크다는 것이다. 따라서, 최근에는 affirmative action을 취하는 인공지능에 대한 논의가 발전하고 있으며, 여기에는 더욱 더 인간과 사회의 결정이 중요해진다. 개선은 적어도 인공지능이 혼자 할 수 없는 것. 인간이, 또는 인간과 인공지능의 ’연합’이 할 수 있는 것!!! 그것을 기계에게 떠넘기지 말고, 인간은 할 일을 피하지 말아야 할 것이다!"
  },
  {
    "objectID": "fml.html#더-읽을거리",
    "href": "fml.html#더-읽을거리",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "3.8 더 읽을거리",
    "text": "3.8 더 읽을거리\n\nBarocas & Selbst\n임베딩 교정\nBarocas 최신 교과서 등등….\n키언스\n알고리즘이 지배한다는 환상."
  },
  {
    "objectID": "fml.html#참고문헌",
    "href": "fml.html#참고문헌",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "3.9 참고문헌",
    "text": "3.9 참고문헌\n홍찬숙 (2021) 청년의 무엇이 ‘성평등 프레임에서 젠더갈등과 공정성 프레임으로’ 변화한 것인가? 키언스\nBolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.\nChouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2), 153-163.\nKleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807.\nDwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012, January). Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference (pp. 214-226).\nRussell, C., Kusner, M. J., Loftus, J., & Silva, R. (2017). When worlds collide: integrating different counterfactual assumptions in fairness. Advances in neural information processing systems, 30.\nhttps://afraenkel.github.io/fairness-book/content/05-parity-measures.html\nhttps://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb\nBarocas, S., & Selbst, A. D. (2016). Big data’s disparate impact. California law review, 671-732.\n공정한 알고리즘은\n샌델이 비판한 바와 같이 롤즈는 공정성으로 정의를 대체하기 위해서 얄팍하게 만든다..하지만, 공정하는 말을 둘러싼 최근의 논쟁을 보면… 이것은 공정한가? 저것은 공정한가? 예컨대, 할당제에 느끼는 청년 남성들의 볼공정함…이것은 무엇인가…이런 문제들은 알고리즘의 공정성의 문제에도 여전히 적용된다. 하지만, 이들은 거기서 멈출 수 없다. 수학으로 직접적으로 연결시켜야 함. - 따라서 수학적 정의를 추구 - 이를 통해 여러 공정성 개념들을 밝혀냄 -&gt; 동시 성립 불가능함을 밝혀냄. - 결국 제도의 문제.\nBarocas & Selbst 논문/교과서 롤스\n\n\n\n\nBarocas, Solon, and Andrew D Selbst. 2016. “Big Data’s Disparate Impact.” California Law Review, 671–732.\n\n\nDwork, Cynthia, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. “Fairness Through Awareness.” In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, 214–26."
  },
  {
    "objectID": "fml.html#footnotes",
    "href": "fml.html#footnotes",
    "title": "3  공정한 인공지능을 위한 기술적 해법",
    "section": "",
    "text": "프로퍼블리카는 기사와 함께 그들이 정보공개청구를 통해 취득하여 보도에 사용한 데이터를 모두 공개하고 있다. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing 참조.↩︎\n특정 확률에 따라 그 값이 결정되는 변수를 확률변수라고 한다.↩︎"
  },
  {
    "objectID": "xai.html#설명이-쉬운-알고리즘.",
    "href": "xai.html#설명이-쉬운-알고리즘.",
    "title": "4  설명 가능한 알고리즘",
    "section": "4.1 설명이 쉬운 알고리즘.",
    "text": "4.1 설명이 쉬운 알고리즘.\n먼저, 인공지능의 근간이 되는 기계학습, 그 중에서도 지도학습(supervised learning) 방법은 분류(classification) 문제에 대해 복습해보도록 하자. 분류의 문제는 어떠한 특성(feature)을 이용해 타겟(target)을 예측하는 문제를 의미한다. 기계학습을 처음 배울 때 하는 이미지 특성을 이용해 개와 고양이를 자동으로 나누는 알고리즘, 손글씨 이미지를 이용해 숫자를 파악하는 알고리즘 같은 것들이 분류 알고리즘이다. 설명으 편의를 위해, 여기서는 그 보다 더 간단한 알고리즘을 생각해보자. 즉, 두 가지의 특성을 이용해 0 또는 1의 판단을 하는 알고리즘이다. 첫번째 특성을 \\(X_1\\), 두번째 특성을 \\(X_2\\)라고 하자. 그리고 예측하고자 하는 \\(Y\\)는 0 또는 1의 값을 갖는데, 이를 예측하고자 하는 것이다. 비근한 예로, 소득(\\(X_1\\))과 신용점수(\\(X_2\\))를 이용해 대출을 상환할 수 있는지(\\(Y=1\\)), 그렇지 않은지(\\(Y=0\\))를 판단하는 알고리즘을 만드는 상황 그런 경우이다.\n이러한 경우에 적용할 수 있는 가장 간단한 분류 알고리즘 중 하나로 이야기되는 것 중 하나는 ’로지스틱 회귀’라는 방식이다. 이는 선형성을 갖는다.\n로지스틱 회귀 그림\n이를 수식으로 표현하면 다음과 같다. 그림으로 봐도, 수식으로 봐도, 즉시 설명이 가능하다. 그림에서 기울기에 해당하니까..\n그런데, 이런 간단한 분류 방법이 늘 현실적이지는 않을 것이다. 가장 먼저, 저 경계선이 곡선인 경우도 허용하고 싶을 것이다. 다음과 같이… 그림\n또, 다음과 같이 1로 예측되는 영역이 분리되어 있는 경우도 있을 수 있다.\n그림\n이는 로지스틱 회귀 분석과 같은 방법으로는 불가능하고, 이를 조금 더 복잡한 함수에 집어넣는다.\n수식\n이러한 방식 중 하나가 바로 우리가 잘 알고 있는 ’인공신경망(Artificial Neural Networks; ANN)이다. 이제 저 안에 들어가 있는 모수는 기울기라는 단순한 해석을 가지지 않는다. 정확히 말하면, 저 모수를 설명할 방법이 없다! 이를 blackboxedness라고 한다.\n결론은 더 나은 예측을 위한 모형의 ’유연성(flexibility)’를 얻기 위해 모형은 더 복잡해져야 하며, 그 댓가로 설명가능성이 떨어진다는 것이다. 즉, 모형의 예측력과 설명가능성 사이에는 트레이드오프 관계가 성리하는 것이다!\n분류 알고리즘 중에는 로지스틱 회귀 말고도 본질적으로 설명하기 쉬운 모형들이 있다. 예컨대 의사결정 나무 같은 것이 그렇다.\nDecision Tree 그림\n의사결정 나무 같은 것은 에전 인공지능 시대의 근간 이기도 했다. 그러나 현대적인 인공적인 인공지능에 사용하는 분류 알고리즘은 이를 믹스한 것. Random Forest같은 경우… 따라서 다음과 같은 트레이드 오프를 보일 수 있다.\n일본 트레이드 오프 그림"
  },
  {
    "objectID": "xai.html#설명-가는성을-높이는-기술-xai",
    "href": "xai.html#설명-가는성을-높이는-기술-xai",
    "title": "4  설명 가능한 알고리즘",
    "section": "4.2 설명 가는성을 높이는 기술: XAI",
    "text": "4.2 설명 가는성을 높이는 기술: XAI\n그렇다면, 예측력을 위해 우리는 설명을 포기해야 하는가? 그렇지 않다. 많은 인공지능 연구자들은 예측력과 복잡도를 보존하면서도 여전히 설명하기 위한 기술들을 개발하고 있다. 여기서 핵심적인 생각은, 에측을 위한 모형을 그대로 둔 채로, ’설명을 위한 장치’를 개발하는 것이다. ’설명가능한 인공지능(eXplainable AI; XAI)’라고 부르는 영역에서 활발하게 논의되고 개발되고 있는 모형들은 그 자체로 설명이 쉬운 기계학습 모형보다는, 예측모형에 더할 수 있는 설명 장치들이라고 할 수 있다.\n이러한 설명 장치들도 몇 가지 유형으로 구분할 수 있다. 그 첫번째는 모형의존형(model-specific) 기술과 모형불문형(model-agnostic) 기술의 구분이다. 모형의존형 기술은 인공지능에 사용되는 예측 모형이 특수한 기술을 따를 때만 작동할 수 있는 모형 설명 기술이다. 예컨대, 영상/이미지 관련 인공지능에 자주 사용되는 CNN (Convolutional Neural Network)에서 작동하는 GRAD-CAM, Score CAM, Grad-CAM++ 등의 기술이 그것이다. 모형의존형 기술은 주예측모형의 작동 방식을 응용하여 만들어진 것이므로, 모형-맞춤형 설명이 가능하다는 장점이 있다. 그러나 이러한 모형들은 예측모형에 바로 응용할 수 없을 뿐만 아니라, 주모형과 직접 연동되어 작동하는 경우가 많아, 주모형의 속도를 느리게 하는 단점을 갖는 경우가 많다. 반면, 모형불문형 기술은 주모형과 독립적으로 설계되고 작동하기 때문에 범용성이 높고, 주모형의 퍼포먼스를 하락시키지 않는 경우가 많다. 이 때문에 최근이 개발 경향을 범용성 높은 모형불문형 기술을 개발하면서 설명력을 가능한 모형의존형 기술에 근접시키도록 하는 방향으로 발전하고 있다. 따라서, 이 장에서는 모형불문형 기술에 초점을 맞추어 기술할 것이다.\n또 다른 구분으로는 전역적(global) 설명과 국소적(local) 설명 기술의 구분이 있다. 전역적 설명은 이용자가 복잡한 모형 전체의 구조를 이해할 수 있도록 단순화하는 방식을 의미하는 반면, 국소적 설명은 인공지능에 의해 내려진 특정한 판단의 이유를 제공하는 방식을 의미한다. 기게학습 모형을 이용하면서 이를 유지, 개선하기 위한 운용 목적이나, 보안상의 문제를 발생시킬 수 있는 외부 공격에 대해 강건한 모형을 만들기 위한 모니커링 목적으로는 전역적 설명이 자주 사용된다. 그에 반해, 인공지능이 내린 판단에 영향을 받는 일반 이용자들이, 예컨대 자동화된 대출 결정을 받거나, 국경에서 보안 검색의 대상이 되었을 때, 그러한 결정에 대한 설명을 요구하는 경우라면 모형 전체에 대한 설명은 일반인들에게 그다지 유용하지도 않을 것이며, 더 나아가 주어진 설명이 ’이해가능(interpretable)’하지 않을 수도 있다. 이러한 경우에는 특정 결정에 대한 국소적 설명이 더욱 유용할 수 있을 것이다."
  },
  {
    "objectID": "xai.html#대표적인-xai-기술",
    "href": "xai.html#대표적인-xai-기술",
    "title": "4  설명 가능한 알고리즘",
    "section": "4.3 대표적인 XAI 기술",
    "text": "4.3 대표적인 XAI 기술\nXAI는 빠르게 발전하고 있는 분야이기 때문에, 지금도 새로운 기술이 계속해서 개발되고 있지만, 여기서는 대표적으로 알려진 몇 가지 기술을 중심으로 전반적인 개념을 이해하고자 한다. 최근에도 자주 인용되는 XAI 기술을 전역적, 국소적 설명의 구분 방식에 따라 일별해 보면 다음과 같다.\n\n\n\n분류\nXAI 기술\n특징\n\n\n\n\n전역설명\nPartial Dependence Plot (PDP)\n특성의 변화가 기계학습 모델의 예측 결과에 반영되는 영향을 그래프로 표시\n\n\nAccumulated Local Effect (ALE)\n특성의 변화가 ‘평균적으로’ 예측 결과에 반영되는 영향을 표현한 것으로 PDP와 유사하지만, 조금 더 빠르게 계산 가능\n\n\nPermutation Importance\n특성을 무작위로 재정렬해 모델 예측 오차가 증가하는 것을 관찰하는 방식으로 특성과 결과의 관계를 밝힘\n\n\nGlobal Surrogate\nAI모형의 예측 방식을 유사하게 학습한 해석가능한 모형으로 대리 설명. 의사결정 트리가 자주 사용됨\n\n\n국소설명\nLocal Surrogate (LIME)\n이미지나 텍스트를 포함한 다양한 데이터에 대해 임의의 판별 AI모형의 예측을 선형 근사로 설명.\n\n\nShapley Additive exPlanations (SHAP)\n각종 데이터에 대응하는 AI모형의 예측에 대해 특성의 공헌도를 게임이론적 지표를 이용해 설명.\n\n\n반사실적 설명\n현재의 판단 결과를 바꿀 수 있는 가장 작은 특성값의 변화를 이용해 현재 판단 결과의 ‘원인’을 설명.\n\n\n\n\n4.3.1 전역적 설명\n여기서는 Permutation Importance 방식과 Global Surrogate에 대해서 이야기 해 보도록 하겠다. Permutation Importance는 많은 전역적 설명 방식과 유사하게, 예측 결과를 도출하기 위한 근거가 되는 특성(feature)들의 상대적 중요도(importance)를 계산하는 것을 목표로 한다. 설명 방식은 다음과 같다. 가장 먼저, 중요도를 측정하기 위한 특성을 먼저 선택한다. 그 다음 다른 특성의 값들은 주어진 데이터 그대로 둔 채, 선택된 특성값만을 무작위로 재배열한다. 이를 그림으로 표현하면 다음과 같다.\n 모든 다른 특징량은 1, 2, 3 등 원데이터에 주어진대로 배열되어 있지만, 중요도를 측정하고자 하는 특징량 E는 무작위 배열되어 10, 94, 48와 같이 규칙을 갖지 않는 순서로 재배열되어 있다. 이제 이렇게 (특징량 E만) 변형된 데이터를 이용해 설명하고자 하는 모형을 추정한다. 모든 모형 추정은 ‘오류율’을 생산하는데, 이렇게 변형된 데이터를 바탕으로 한 모형의 오류율과, 변형하지 않은 원 데이터를 바탕으로 추정한 같은 모형의 오류율을 비교해보면 전자의 오류율이 클 것이다. 이는 위의 예에서 특징량E에 포함된 예측에 사용되어야 할 유용한 정보를 무작위 배열을 통해 삭제한 것이나 다름 없기 때문에, 더 적은 정보(변형된 데이터)를 가지고 추정한 모형이 더 많은 정보(원데이터)를 이용해 추정한 모형보다 정확하지 않을 것이라는 것을 생각해보면 납득이 갈 것이다. 이제 변형 데이터로부터 도출된 오류율과 원데이터로부터 도출된 오류율의 비율, 즉, 특정 특징량을 무작위 배열함으로써 ’증간한 오류의 양이 얼마인가’를 측정하면, 반대로 그것은 해당 특성이 얼마나 예측에서 중요한 역할을 하고있었는가를 나타내는 지표가 된다. 이러한 과정을 하나의 특성(위의 예에서는 ’특징량 E’) 뿐만 아니라, 모든 특성에 대해 반복하면, 각 특성의 중요도를 파악할 수 있게 되는 것이다.\n이러한 과정을 통해, 다음과 같이 각 특성의 중요도를 시각화할 수 있다.\n\n\n\n특성별 중요도 표현\n\n\n위와 같은 시각화는 각각의 특성이 해당 모형의 예측에서 얼마만큼 중요한 역할을 하는지를 표현해주기에, 예측 모형이 아무리 복잡하더라도 그 모형이 작동하는 방식을 대체로 이해하는데 도움을 준다. 또한, 위의 중요도 게산 방식은 어떠한 예측 모형을 사용하는가와 전혀 관계없이 수행할 수 있다는 점에서, 모형불문형 기술이라고 할 수 있다. 더 나아가, 중요도 계산에 이용하는 오류율은 사용하는 모형, 데이터와 관계 없이 같은 단위(unit)를 가지기 때문에, 예측 모형 간의 비교 분석을 수행하는데에도 유용하다.\n물론 이러한 전역적 설명 방식은 고유한 한계를 갖는데, 다수의 다른 특성들 사이의 예측에 있어서의 의존관계를 파악하는데 도움을 주지 못하며, 각 특성이 특정 영역에서는 큰 중요도를 가지다가 다른 영역에서는 중요도가 떨어지는 등 ‘비선형적’ 중요도를 갖는 경우에도 이를 파악하는데 도움을 주지 못한다. 전역적 설명 방식은 본질적으로 복잡한 모형 전체를 단순화하여 표현하는 접근방식이기에 피하기 어려운 단점이다.\n또 다른 전역적 설명방식으로 Global Surrogate을 들 수 있다. 여기서 Surrogate이라고 함은 대리 모형, 즉, 복잡한 모형에 대한 조금 더 단순한 근사 모형을 의미한다. Global Surrogate이 사용하는 근사모형은 특정한 모형으로 미리 정해져 있지 않고, 근사모형의 이용자가 쉽게 이해할 수 있다고 믿는 한, 어떤 모형이라도 가능하다. Global Surrogate의 개략적인 아이디어를 그림으로 표현하면 다음과 같다.\n\n\n\nGlobal Surrogate의 간단한 예\n\n\n즉, 실제 모형은 예측의 정확도를 높이기 위해 0과 1을 구분하는 구불구불한 곡선이라면, 이를 이해하는 것이 쉽지 않으므로, 가능한 비슷한 예측결과를 만들어 낼 수 있는 근사모형은 선형 모형을 추정하여, 대략적인 설명을 한다는 아이디어이다. 물론, 선형 모형일 필요는 없고, 의사결정 트리와 같은 다른 유형의 이해가능성이 높은 모형을 이용할 수 있다.\n이 역시 주 모형의 작동 방식을 알지 못하더라도 구축 가능하다는 점에서 큰 장점을 갖지만, 근사 모형이 주 모형으로부터 대단히 멀 수 있다는 단점이 있다는 것을 즉각적으로 알 수 있다. 가깝게 하기 위해서는 근사 모형 역시 유연하게 만들어야 할 것이고, 그렇게 되면, 결국에는 근사 모형 역시 설명이 어려워진다는 원래의 함정을 빠져들게 된다.\n전역적 설명은 알고리즘 자체의 사전적 설명과 투명성 관점에서는 더 의미가 있는 방식일 것.\n\n\n4.3.2 국소적 설명\n앞서 설명한 바와 같이 국소적 설명은 예측 모형 전체를 일반 사용자에게 이해시키기 위한 목적이라기 보다는, 인공지능이 해당 사용자에게 부여한 특정 예측, 또는 결정이 이루어진 이유를 제공하기 위한 목적을 가지고 있다. 이미 논의한 바와 같이, 일반 이용자가 쉽게 이해할 수 있을 정도로 전역적 설명 기술을 적용하기 위해서는 복잡한 모형을 지나치게 단순화해야 한다는 문제점이 있으므로, 해당 이용자가 관심을 갖는 특수한 사안으로 설명의 범위를 국한하는 국소적 설명 방식이 최근 각광받고 있다. 또한, 국소적 설명 방식은 기업의 특정 결정에 대해 설명을 요구할 권리를 보장하는 기존의 소비자 보호 법안의 취지에 잘 부합한다는 장점이 있다. 이 장에서는 최근 각광받는 국소적 설명 기술인 LIME (Local Interpretable Model-agnostic Explanations)와 반사실적(Counterfactual) 설명 방식에 대해 간단히 논의해 보도록 할 것이다.\nLIME은 Global Surrogate 방식과 유사하게 대리/근사 모형(Surrogat)을 이용한 설명 기술이다. 그러나 Global Surrogate과의 결정적인 차이는, 근사 모형을 특정 예측치 근방(locality 또는 neighborhood)에서 생성한다는 것이다. 이 점을 더 쉽게 이해하기 위해서 다음과 같은 분류 문제와 근사 모형을 생각해 보자.\n\n\n\n국소적 근사모형\n\n\n여기서 회색과 붉은색으로 표시된 영역은 예측을 위한 기계학습 모형(주모형)이 -1(회색) 또는 +1(붉은색)으로 예측하는 구간을 의미한다. 두 영역의 경계를 보면 알 수 있듯이 그 경계는 매우 비선형적이다. 이러한 비선형적인 모형이 만들어낸 예측결과는 ’특징량 1과 특징량 2의 수치가 어떠어떠하기 때문에 +1 또는 -1로 예측하였다’와 같은 방식으로 설명이 어렵다. 이 때문에, 이 불규칙해 보이는 경계 전체를 선형적인 모형으로 근사한다면(즉, Global Surrogate을 도입한다면), 지나친 단순화로 인해 좋은 근사모형을 만들어낼 가능성이 낮다.\n때문에, LIME은 전체 모형에 대한 근사 모형을 포기하고, 특정 위치에서의 근사 모형만을 생성한다. 예컨대 위의 그림에서 별표에 해당하는 어떤 예측치가 있었다고 하자. 이 예측치는 회색 영역에 위치해 있으므로, -1이라는 값을 가질 것이다. 좀 더 쉽게 이해하기 위해서 구체적인 시나리오를 도입하자면, 소득(특징량1)과 신용점수(특징량2)의 특정한 조합(별표) 때문에 해당 소비자는 신용카드 발급이 자동적으로 거절(-1)된 경우라고 생각해보자. 이 때, 해당 소비자는 본인에게 불리한 판정(더 정확하게는 신용 위험이 높다는 ‘예측’)이 이루어졌으므로, 그 이유를 알려달라고 요구할 수 있을 것이다. 이 때, LIME은 근사 모형을 형성하되, 해당 소비자가 위치한 그 근방에서 예측 모형을 생성하자는 접근이다. 이 근사 모형은 위의 그림에서 점선으로 표현되어 있다. 해당 점선은 전체적인 불규칙한 예측 경계에 대해서는 (즉, Global Surrogate으로서는) 나쁜 근사모형이지만, 별표로 표시된 특정 예측치 ‘근방에서만큼은’ 상당히 좋은 근사모형이라는 것을 알 수 있다. LIME을 수행하는 구체적인 알고리즘은 다음과 같다.\n\n첫째, 설명 대상 데이터(별표)에 무작위 오류를 더하여, 설명 대상 데이터 근방의 가상 데이터를 만들어낸다 (이는 위의 그림에서 점들의 ’위치’로 표현되어 있다). 이를 기술적으로는 설명 대상 데이터를 섭동(perturbation)한다고 한다.\n둘째, 가상 데이터를 ’주모형’에 삽입하여 가상 데이터에 대한 AI 모형의 예측 결과를 얻는다 (위의 그림에서 예측 결과는 가상 데이터의 색깔에 해당한다).\n셋째, 가상 데이터와 그에 대한 예측결과를 이용하여, 비교적 단순한 근사모형을 만든다.\n\n이러한 방식을 이미지에 적용하면, 다음과 같은 재미있는 결과를 얻을 수 있다.\n 이는 LIME을 처음 제시한 Ribeiro 등의 논문(2016)에 수록된 것으로, 특정 오류에 대해서 근사모형을 만들어 보니, 허스키와 늑대를 구분하는 경계가 배경에 해당하는 영역이었다는 것을 알 수 있었다는 예시이다. 즉, 대상의 형태에 따라 허스키와 늑대를 오해한 것이 아니라, 배경의 눈 때문에 늑대라고 생각했다는 것이다.\n중요한 점은, LIME의 이러한 설명이 모형 전체가 그러한 오류를 가지고 있다는 것을 의미하지는 않는다는 것이다. 특정 오류에 대한 설명일 뿐이다.\nRibeiro, M. T., Singh, S., & Guestrin, C. (2016, August). ” Why should I trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144).\nLIME 역시 surrogate을 이용. 반사실은 요즘 핫한 주제!"
  },
  {
    "objectID": "xai.html#설명가능성과-법안과의-연관성",
    "href": "xai.html#설명가능성과-법안과의-연관성",
    "title": "4  설명 가능한 알고리즘",
    "section": "4.4 설명가능성과 법안과의 연관성",
    "text": "4.4 설명가능성과 법안과의 연관성\n아직 논란이 되고 있는 부분이라 (법학자들 사이에서) 자세히 논의하기는 어렵다. 하지만… EU - 설명가능성이 권리로 포함되어 있는가? (Edwards & Veale, 2018) US - 인공지능에게 요구하지 않은 기존의 법을 확장할 수 있는가.\n알고리즘 등록제 - ex ante -&gt; 이건"
  },
  {
    "objectID": "xai.html#더-읽을거리",
    "href": "xai.html#더-읽을거리",
    "title": "4  설명 가능한 알고리즘",
    "section": "4.5 더 읽을거리",
    "text": "4.5 더 읽을거리\n\n인터넷 교재\n인공지능 개론인가? (그 검은 책…)\n\n인공지능의 실패 -&gt; 예외상태 (슈미트, 발리바르, 등등등) -&gt; 예외상태가 주체성을 결정하는 것이 아닐까. 노동.\n책임의 분배.\n인공지능의 책임성…도덕적으로나, 피해 보상으로나, 책임성을 따지는 것은 매우 어렵다. 하지만, 그 전에 전제되어야 하는 것이 있다. 어떠한 판단을 했는가 하는것. 그것이 책임있는 인공지능, 혹은 책임있는 인공지능과 인간의 네트워크의 충분조건은 될 수 없을 지언정, 필요조건인 것은 사실.\n책임성 있는 인공지능…\n결정을 인공지능에 맡길 수록…. 원래 결정은 모두 인간이 하지 않았다 - 사실은 이에 대해서 많은 법들이 있어왔다. 인간이 하든, 하지 않든… 하지만, 문제는 더욱 복잡해졌다는 것.\n또, 인간과의 협업을 위해서…\n설명 가능한 인공지능 v. 설명을 위한 인공지능."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barocas, Solon, and Andrew D Selbst. 2016. “Big Data’s Disparate\nImpact.” California Law Review, 671–732.\n\n\nDwork, Cynthia, Moritz Hardt, Toniann Pitassi, Omer Reingold, and\nRichard Zemel. 2012. “Fairness Through Awareness.” In\nProceedings of the 3rd Innovations in Theoretical Computer Science\nConference, 214–26."
  }
]