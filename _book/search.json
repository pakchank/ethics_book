[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "인공지능 시대의 윤리 또는 윤리적 인공지능 - 기술, 정책, 철학",
    "section": "",
    "text": "서문\n팬데믹의 공포가 사그라들던 2022년 말, ChatGPT는 신종 바이러스가 등장하던 때처럼 소리소문 없이 우리 곁에 와 있었다. 처음에는 먼 나라 이야기처럼, 기술자들만 관심을 갖는 재미난 이벤트처럼 여겨지던 OpenAI의 대화형 인공지능 서비스는 어느 순간엔가 우리의 대화를, 업무환경을, 그리고 삶을 바꾸어나가기 시작했다. 물론, 이전에도 유사한 기술을 사용하는 서비스는 이미 곳곳에서 작동하고 있었지만, 꽤나 고차원의 대화가 가능한 것으로 보이는 인공지능 인터페이스를 통해, 우리는 그간 이미 인공지능과 공존하는 삶으로 한걸음, 한걸음 나아가고 있었다는 사실을 다소 뒤늦게 깨달은 것 같기도 하다.\n인공지능과 함께 살아야 한다는 불편한 사실을 자각한 지금, 우리는 인공지능의 윤리와 관련된 많은 소식들을 언론을 통해 접하게 되었다. 영화에 등장하는 빌런을 꼭 닮은 밴처 사업가들과 이들의 폭주에 대한 두려움이 소셜 미디어와 증권 시장을 떠들썩하게 만들기도 했고, 안전한 인공지능의 비전에 대한 의견 차이로 보이는 갈등이 인공지능 산업을 선도하는 기업의 명운을 흔드는 것처럼 보이는 사건도 있었다. 세계적 올드 미디어 기업과 인공지능 산업 진영 간에 지식의 창조를 어떻게 이해할 것인가에 대한 해석을 둘러싸고 한 판 법정 싸움도 예고되어 있다. 지난 반세기가 넘도록 세계에서 가장 매력적인 컨텐츠를 만들던 산업의 창작자들은 인공지능의 자동적 컨텐츠 생산에 맞서, 창조적 노동의 권리를 지켜내기 위한 대규모 투쟁을 시도하였다. 그 사이 정보 기술에 관한 규제를 선도하고 있는 유럽연합에서는 인공지능법 도입에 거의 합의하였고, 이는 이른바 `브뤼셀 효과’를 타고 미국의 IT 산업, 그리고 전세계의 인공지능 생태계의 진화에 큰 영향을 미칠 것이라고 여겨지고 있다.\n이 모든 소동들은 인공지능 윤리에 대한 논의가 빠르게 변화하는 세상을 탄식하는 유희적 담론이 아니라, 우리가 살아내야 하는 산업 생태계의 규칙, 따라서 우리의 일자리, 삶의 가치를 재정의 하는 시급하고도 심대한 영향력을 갖는 문제를 다루고 있다는 것을 보여준다. 그럼에도, 인공지능 윤리와 관련된 논란들은 여전히 한국에서는 수입된 이야기들처럼 들리는 것도 사실이다. 이름있는 공학자들과 사업가들이 인공지능의 윤리적 도전에 대해 어떻게 한 마디 보태었는지, 미국, 중국의 IT기업의 혁신에 대해 유럽연합이 어떤 규제로 대응하려 하는지에 대한 정보들은 여전히 짧은 단신이나 요약 비디오를 통해 유통되거나, 아니면 전문가들 사이에 보고서 형태로 오고 가는 것이 현실이다. 이는 어느 때보다 인공지능 윤리 문제에 대한 사회적 이해 증진이 시급한 시기임에도, 아직 관련 지식에 대한 대중적 교육이 어떻게 가능할지에 대한 합의가 없기 때문일 것이다.\n이러한 교육 공백에는 여러 이유가 있겠지만, 무엇보다 인공지능시대의 윤리 문제를 제대로 이해하기 위해서는 폭넓은 학제적 사고가 필수적이라는 사안의 복잡성이 큰 몫을 차지한다. 기계와 인간 사이의 새로운 관계정립을 논하고 있는 최근의 많은 철학적 발전은 정보과학의 한 분야라고 할 수 있는 사이버네틱스(cybernetics) 그리고 인지과학의 성취에 대한 재해석을 바탕으로 한다. 법학자들은 인공지능이 제기하는 창작, 지식, 가치에 대한 다양한 문제들을 법적 언어로 해석하여 규제안으로 만들어내고 있고, 이는 다시 해당 규제에 합치되는 ’윤리적 인공지능’의 공학적 개발 노력으로 이어지기도 한다. 진정 융합적인 이 과정은 너무나도 빠른 기술변화에 종속되어 있기에, 이를 아우르는 교육 커리큘럼을 만든다는 것은 더욱 어렵다.\n그럼에도, 이 교재는 시급한 인공지능 윤리 교육의 공백을 메꾸기 위한 시도로 만들어졌다. 이 교재 집필에 참여한 필진들은 현재 누구보다 기술, 철학, 사회과학의 교차점에서 연구와 교육을 위해 열심히 활동하고 있는 학자들이라고 할 수 있다. 그 중 1부는 인공지능이 만들어내고 있는 철학적 문제들을 기술철학, 인문학 관점에서 일별하고 있다. 또 2부는 그러한 가치의 혼동으로 일어난 사회적 결과들과 그에 대한 제도적 대응을 다루고 있다. 마지막으로, 3부는 윤리적 문제에 대응하기 위해 공학자들이 다시 고민하여 내어놓은 기술적 솔루션들을 다루고 있다.\n교재 개발에 할애된 시간의 제약으로 인해, 여전히 다루지 못한 중요한 영역들이 많이 남아있다. 또한 너무나도 빠른 혁신의 속도로 인해, 한 번 교재로 정리해둔 지식이 언제까지 유효할지에 대한 두려움이 큰 것도 사실이다. 이러한 문제들에 대한 부분적인 대안으로, 빠르게 업데이트할 수 있고, 재공유하며, 협업 개발이 가능한 온라인 오픈 택스트북의 형식을 채택하였다. 이 교재의 소스는 오픈소스 소프트웨어처럼 공유 플랫폼을 통해 공개되어 있으며, 언제든지 기존 내용의 업데이트, 새로운 저자의 참여가 가능하도록 하였다. 이렇게 재빠른(agile) 지식 공유형태를 취함으로써 우리가 겪고 있는 담론, 지식생산, 교육의 공백을 빠르게 메꿀 수 있게 되기를 희망한다. 교재 내용에 대한 수정을 제안하거나, 새로운 챕터를 저술하는 방식으로 참여를 원하는 독자는 언제든 편자의 이메일(chankyungpak@knu.ac.kr)로 연락을 주기를 바란다.\n본 교재의 개발은 대구경북지역혁신플랫폼 전자정보기기사업단의 지원으로 이루어졌다. 큰 지원을 해주신 사업단과 교재 개발 프로젝트를 기획한 경북대학교 미디어공공성센터장 이강형 교수께 감사를 전한다.\n2024.1.15.\n저자들을 대신하여,\n편자 박찬경",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "basic.html",
    "href": "basic.html",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "",
    "text": "1.1 인간-기술의 관계를 바라보는 관점\n인공지능이 우리 미디어 환경의 근본적인 변화를 가져올 것이라는 전망은 거의 확실해 보인다. 실제 우리의 삶에서 인공지능이라는 미디어가 전면에 등장한 것은 알파고를 통해 처음 시작되었다면, 이제 일상적 삶의 일부를 구성하는 요소가 되었음을 부정하기는 힘들 정도로 인공지능은 모든 미디어와 결합 되고 있다. 그런데 인공지능과 관련된 새로운 상황에 대면할 때마다 우리는 과거 기술 발전에 대해 열광하던 모습들과는 달리 인간의 지위와 존재 조건에 대한 심각한 우려를 동시에 제기하고 있다. 새 기술을 바라보는 인간의 시선은 항상 기대와 우려가 교차했지만, 지금 던지는 질문은 그 기술로부터 파생될 각종 사회경제적 문제만이 아니라 ’인간이라는 존재는 무엇인가’라는 보다 근본적인 존재 물음이라고 할 수 있다.\n그런 의문과 질문 속에는 오랜 기술과 미디어의 변화과정에서 제기되었던 기술과 인간의 관계에 관한 오래된 형이상학적 구도가 여전히 개입하고 있음을 알 수 있다. 가장 대표적인 것으로 ’주인으로서의 인간과 노예 혹은 도구로서의 기술’이라는 아주 오래된 도식이 그것이다. 최근 주인을 누르고 해방된 노예 혹은 주인을 압도하는 노예처럼 비춰진 ’알파고’라는 한 기술적 대상을 바라보는 시선도 이런 구도의 연장선에 있으며, 각종 추천 알고리즘으로 무장한 각종 미디어들이 우리의 선택을 도와준다는 외형을 가장하며 우리의 취향에까지 개입하고 있는 양상은 이런 경향성의 현재적 모습이다.\n여기에 더해 검토해야 할 또 다른 철학적 쟁점은 기술적 대상 혹은 미디어를 하나의 대상으로 파악하여 인간과 대결하는 구도로 지각하는 방식이다. 즉 기술 혹은 미디어를 독립적으로 존재하는 실체적 대상으로 간주하고, 그 독립된 실체와 인간의 관계를 상정하고 진행하는 생각의 방식이다. 이런 구도는 기술적 대상들 특히 지금 논의의 대상인 인공지능을 하나의 실체적이고 독립적인 대상으로 파악함으로써 인간과 대립시키는 방식으로 파악하는 방향으로 나아가게 한다. 이는 기술에 대한 단순한 의인화를 넘어 서양 철학의 오래된 실체론적 인식구조가 밑바탕에 깔려있기 때문이다.\n인간과 기술의 관계에 대한 인식변화는 변화하는 기술 환경 속에서 인간의 위치, 즉 인간-기술의 관계에 대한 재검토는 물론, ’인간’과 ’기술’에 대한 존재론적인 차원의 근본적인 문제를 제기하고 있다. ’포스트휴먼(post-human)’이라는 인간에 대한 새로운 개념을 주장하는 사람들에 따르면, 당연하게 우리에게 주어진 실체처럼 보이는 ’인간’이 사실은 하나의 역사적 인식들과 실천들 속에서 구성된 것이며, 우리가 지금까지 지녀온 인간이라는 개념이 현재와 같은 기술 환경에서는 더 이상 인간을 설명하는데 적절하지 않다고 주장한다(Hayles, 1999. 참조). 지금 우리에게 일반적으로 받아들여지는 ’인간’이라는 개념, 즉 알파고와 이세돌처럼 기술의 대척점에 놓인 바로 그 인간이라는 개념이 사실은 역사적, 사회적으로 구성된 개념이기 때문에 새로운 상황에서는 새로운 인간 개념이 생겨날 수 있다는 것이다. 물론 이들 사이에서도 미묘한 차이가 존재한다. 그러나 보다 정확하게 말하자면 인간의 개념규정이나 그 개념이 포섭하는 범위의 문제가 아니라 인간과 기술 혹은 인간과 그를 둘러싼 환경의 관계에서 인간이 파악되는 방식이 변화하는 것이라고 보는 것이 더 타당하다. 왜냐하면 인간은 어떤 방식으로든지 주변 환경과 관계를 맺어 왔고, 그 속에서 인간의 본질 혹은 존재론적 지위를 규정 받아왔기 때문이다. 인간을 중심으로 생각하면 인간을 어떻게 파악하느냐에 따라 그를 둘러싼 환경이 정의될 수 있겠지만, 인간은 환경과 더불어, 그 관계 속에서 환경을 정의하는 존재이자 환경에 의해서 정의 당하는 존재이기도 하다는 점을 받아들인다면 이야기는 달라진다.\n새로운 기술 환경에 대한 또 다른 시선으로 포스트휴머니즘과는 약간 결을 달리하는 트랜스휴머니스트(trans-humanist)를 주창하는 이들이 있다. 이들은 현재 인간을 넘어선 포스트휴먼으로 이행하는 특이점을 향해가는 인간향상기술의 발전을 긍정적으로 파악하며, 인간의 사이보그화로 정의되는, 즉 새로운 형태를 지닌 인간종(種)을 종적인 진화로 간주한다. 그들은 이처럼 변화된 인간을 자율적인 주체로서의 인간이 역량을 확장시켜나가는 근대적 계몽의 일환으로 간주한다. 근대 이후 인간이 추구해온 인간 해방의 연장선에서 포스트휴먼 현상을 바라보는 것이다. 이런 생각을 대표하는 이들로, 보스트롬(Bostrom), 커즈와일(Kurzweil, 2005), 모라벡(Moravec, 1999) 등을 들 수 있는데, 이들은 자연적으로 태어난 인간의 육체적 한계를 현대 기술력으로 극복할 수 있고, 이것은 이전의 인간과는 다른 포스트휴먼을 약속한다고 믿는다. 이들의 시선은 앞서 말한 서구의 형이상학적 입장이 극단화된 것으로 볼 수 있는데, 특히 인간의 육체적 변환이 인간의 본질과 인간과 그 주변 환경과의 관계를 변화시킬 것이라는 점은 고려하지 않는 듯 보인다.\n이들이 말하는 새로운 인간종은 인간의 육체를 철저히 영혼에 종속적인 것으로 보는 테카르트적 인간의 극단적 형태라고 볼 수 있다. 이런 점에 대해 비판적인 시선을 보내고 있는 캐서린 헤일스(Hayles, 2013), 배드밍턴(Badmington, 2000), 캐리 울프(Wolfe, 2010), 스테판 헤어브레히터(Herbrechter, 2013) 등과 같은 비판적 포스트휴머니스트라고 불리는 사람들은 트랜스휴머니즘의 주창자들의 사고가 인간에 대한 다소간의 몰이해에 기초해 있다고 비판한다. 이들은 기술문화의 급진적 변화에 대해 인정하고 그것을 받아들이면서도 트랜스휴머니즘이 근본적으로 전제하고 있는 데카르트적 인간중심주의 또는 자유주의적 휴머니즘에 대해서는 비판적 태도를 취한다. 이들은 인간과 비인간(포스트휴먼)의 경계 자체가 지닌 불확실성과 서로의 존재에 깊숙이 스며든 상대의 영향에 주목하며, 포스트휴머니즘 안에서 여전히 작동하고 있는 서구의 오래된 형이상학적 편향인 인간중심주의의 유령을 끄집어내어 해체하고자 한다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>인공지능 윤리의 모색을 위한 철학적, 기술학적 기초</span>"
    ]
  },
  {
    "objectID": "basic.html#인간-기술인공지능의-관계-문제-인간중심주의anthropocentrism",
    "href": "basic.html#인간-기술인공지능의-관계-문제-인간중심주의anthropocentrism",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "1.2 인간-기술(인공지능)의 관계 문제 : 인간중심주의(anthropocentrism)",
    "text": "1.2 인간-기술(인공지능)의 관계 문제 : 인간중심주의(anthropocentrism)\n우리는 인간이라는 종적인 틀 속에서 지각하고 판단하며, 그런 구속성 속에서 무엇인 옳고 바람직한 것인가를 판단할 수 밖에 없다. 우리가 다른 존재가 아니기 때문에 우리가 생각하는 모든 것들은 우리 인간 중심적인 판단이 될 수 밖에 없다고 생각할 수 있을 것이다. 그렇기 때문에 우리 인간을 중심에 두고 기술과 자연을 파악하는 방식은 지극히 당연한 것처럼 보인다. 그러나 사실 이런 인간중심적 사고는 근대를 관통하며 만들어진 구도이며, 서양 형이상학이 끝없이 이런 인식에 동력을 공급한 결과이지, 인간의 존재 일반의 당연한 인식으로 받아들일 수는 없다. 특히 기술에 관한한 이런 인간중심주의적 경향은 더 강력한데, 기술은 인간이 사용하기 위해 고안했고 만들었으며, 따라서 그 기술은 주인인 인간을 위해 사용되는 도구이며, 인간의 목적에 부합하는 중립적인 대상으로 파악하는 것이다.\n기술에 대한 인간중심주의가 보여온 인식을 가장 잘 드러내 주는 것이 ’기술의 도구성’에 대한 인식과 더불어 근대 이후 과학기술의 본질을 둘러싸고 지속적으로 주장되어온 가치중립성이라는 개념이다. 이는 자연적 사실을 탐구하고 발견하며, 이를 체계적으로 설명하는 활동인 과학과 그에 기반한 기술은 그 자체로는 ’가치중립적’이라는 주장이다. 이런 주장들은 기술이란 수단일 뿐 그 자체는 선도 아니고 악도 아니며, 인간이 그것에서 무엇을 만들어 내는지, 기술이 인간에게 어떻게 기여하는지, 인간이 기술을 어떤 조건 아래 두는지가 중요하다고 본다. 이들이 보기에는 기술의 가치중립성은 과학기술자에게도 객관적이며 중립적인 태도를 보장하는데, 그들은 대상을 있는 그 자체로 파악하며 오로지 객관성과 보편성의 원리만을 추구한다는 입장을 가능하게 한다. 이 같은 이념에는 근대가 표방하는, 과학기술이야말로 가장 합리적이고 이성적 사유의 표준이라는 신념과 신뢰가 전제되어 있다.\n기술 발전의 자율성 또한 근대인의 사유를 부단히 사로잡아온 개념이다. 그것에 따르면 과학기술의 발전은 자가발전적 메커니즘처럼 그 속에 내재된 자율적 원리에 따라 움직인다. 곧, 기술 발전이란 인간의 의지나 사회적 관계에 의해 결정되는 것이 아니라 기술의 자체 발전적 메커니즘을 따른다는 것으로, 이는 기술 선택의 자동성, 기술의 자율적 증식, 기술들 사이의 필연적 연결로 이어질 수 있다. 이 같은 기술발전의 자율성과 그것을 정당화하는 객관성의 이념은 기술 발전이 사회변동의 일차적 요소로 정치, 경제, 사회, 문화를 주도한다는 기술결정론으로 이어진다.\n그런데 우리가 기술을 중립적이라고 여길 때, 우리는 최악의 경우에 처해질 수 있다. 왜냐하면 오늘날 사람들이 인간을 중심에 두고, 기계의 주인으로서 인간을 설정하기 위해 도입한 기술의 도구성과 가치중립성은 우리를 기술의 본질에 대해 완전히 맹목적이게 할 수 있기 때문이다. 가치중립적인 기술은 오로지 사람들에 의해 선하게도 악하게도 사용될 수 있다는 양면적 입장은 언뜻 보기에는 타당해 보이지만, 정작 기술이 미치는 영향력을 논의하기에는 지나치게 단순한 감이 있다. 이는 선한 기술 혹은 악한 기술이란 구분과는 별개로 기술은 자신을 둘러싼 모든 환경에 이미 영향을 미치고, 삶의 맥락 전체를 바꾸어 버릴 수도 있기 때문이다.\n하이데거는 기술중립주의가 궁극적으로는 기술결정론으로 환원되어 기술 개발과 그것의 산업적 이용을 무제한적으로 허용함으로써 초래할 수 있는 엄청난 위험의 가능성을 경고하고 있다. 그는 서구의 인식론적 틀을 구성해온 기술의 가치중립적 본질과는 거리를 두고 있는 것인데, 그의 기술철학적 질문은 현대 기술의 위험에 대한 지적으로 이어진다. 그는 기술이 도전적이고 총체적방식으로 기술에 내재된 고유한 방식으로 인간을 비롯한 모든 존재들을 닦달하고 몰아세우고(ge-stell) 있다고 비판한다. 기술은 스스로의 본질을 드러내지 않고 위장한다. 인간은 스스로 기술을 통제하고 있다고 착각하지만, 기술의 힘은 이미 인간의 통제를 벗어나 있다는 것이다. 이것은 모든 사물을 대상화, 부품화할 뿐 아니라 심지어 인간마저도 대상화, 부품화한다. 지금 거의 모든 회사에 설치되어 있는 인사부의 영어 명칭은 ’Department of Human Resources’다. 즉 인간도 자원이며, 필요한 곳에 사용되는 부품이요 대상이라는 말이다. 그럼에도 사람들은 여전히 기술을 도구처럼 장악한다고 믿고 자신의 욕망대로 다루려 하는데, 하이데거는 기술이 인간의 의지를 벗어날 가능성이 커질수록 기술을 지배하려는 사람들의 욕구 또한 절박해질 것으로 보았다.\n인간의 활동은 마침내 기술 발전과 짝하여 존재하는 모든 것들을 착취하는 행위에 이른다. 하지만 모순적이게도 기술의 위장술은 끝내는 인간 스스로를 착취의 대상으로 전락시킨다. 즉 인간중심주의에서 발현한 인식틀, 즉 기술을 인간을 위한 도구로 파악하는 것과 인간의 목적에 부응하는 중립적인 대상으로 파악한 것과 같은 기술과의 이상적인 관계맺음은 본질적으로 가능하지 않은 것이라고 비판하는 것이다. 당연하게도 기술에 대한 우리의 이해 방식은 최종 국면에서는 항상 윤리적인 질문으로 끝나게 되는 이유가 바로 이것이며, 인간-기술의 관계에 대한 철학적이고 근본적인 접근방식에 대한 이해가 중요한 이유도 여기에 있다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>인공지능 윤리의 모색을 위한 철학적, 기술학적 기초</span>"
    ]
  },
  {
    "objectID": "basic.html#인간과-인공지능에-대한-관계론적-접근",
    "href": "basic.html#인간과-인공지능에-대한-관계론적-접근",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "1.3 인간과 인공지능에 대한 관계론적 접근",
    "text": "1.3 인간과 인공지능에 대한 관계론적 접근\n트랜스휴먼으로 대표되는 기술이 초래하는 미래에 대한 낙관론은 인간과 기술에 대한 특정한 관점에 기초하고 있다. 즉 이들은 기술과 인간의 관계를 파악하는데 있어서 ‘실체론’적이라고 불릴 수 있는 관점을 드러내고 있다. 실체론은 관계를 맺는 두 실체가 이미 존재하고 있고 그것들이 만들어내는 것이 관계라고 생각하는 입장이다. 비록 그것이 역사적 과정 속에서 변화를 거치면서 만들어진 구성적인 요소라고 하더라도, 이들의 입장은 ‘인간’과 ‘기계’ 혹은 ’인간’과 ’미디어’라는 두 실체를 먼저 상정하고 논의를 진행한다.\n한편 이와 다른 시각도 존재하는데, 이 실체라는 것들은 관계가 만들어내는 전체 앙상블(ensemble)을 통해서만 그 현실적인 존재의 특성이나 모습을 지닐 수 있다고 생각하는 방향도 있다. 이를 관계론적인 접근이라고 할 수 있다. 이 관점에 따르면, 실제 혹은 존재는 생성되는 것이지 이미 주어진 것이 아니라는 것이며, 생성된 존재 역시 외부와 단절적으로 존재하는 것이 아니라 끊임없이 외부 환경(milieu)과 소통하며 변환되어 가는 존재라고 할 수 있다. 이런 존재에는 생명체만이 아니라 기술적 대상들도 포함된다고 할 수 있다.1 지금 우리가 관심을 가지고 있는 인공 지능이라는 기술적 대상 역시 하나의 실체적인 존재로서 바라보기 보다 전체 미디어 환경 속에서 이 기술이 다른 존재들과 맺고 있는 관계의 양상 속에서 기술적 대상으로서 인공지능은 존재하게 된다는 것이다.\n이 점에 대해 기술철학자 시몽동(Simondon)은 분명하게 언급하고 있는데, 그는 “기술적 대상은 인공적인 존재로 고찰되어서는 안 된다.”고 말한다(Simondon, 1989/2011, 383쪽). 왜냐하면 그는 우리가 흔히 생각하는 방식으로 자연적인 것과 인공적인 것으로 구분하지 않기 때문이다. 시몽동은 “인공성은 자연 생산물의 자발성에 대립하는 것으로서 기술적 대상의 제작된 기원을 가리키는 특성이 아니라, 인간의 인공화하는 행동에 내재하는 것”이라고 주장하는데, “(인공화하는) 이 행동은 자연적인 대상에 대해서든 완전히 제작된 대상에 대해서든 다 개입한다.”고 본다. 그는 예를 들어 온실 속의 꽃은 자연적인 대상처럼 보이지만 원래 자연적인 대상에 대한 조절들이었던 것이 온실 속의 인공적인 조절들이 되었다고 본다. 인간의 개입에 의해 인간과 관계맺음의 관계 양상으로서의 인공화가 진행되는 것이지, 그것이 애초에 인간에 의해 제작되었기 때문에 인공적인 것은 아니라는 것이다. 즉 기술적 대상들은 만들어졌다는 이유로 인간에게 단순히 도구적 대상으로 사용되는 존재가 아니며, 생명체로서의 현실적 모습을 띠고 있다고 해서 자연적인 것만은 아니라는 것이다. 여기서 중요한 지점은 인간의 개입에 의해 탄생하는 존재론적인 차원 즉 관계맺음의 문제이다.2\n기술적 대상과 인간의 관계 양상이 기술에 대한 본질을 파악할 수 있는 근본적인 것이라는 점은 “기술의 본질은 전혀 기술적인 것이 아니다”라고 말한 하이데거에서도 찾아 볼 수 있듯이, 기술적 대상에 대한 이해는 기술 그 자체에 대한 탐색으로는 불가능하다. 시몽동이 지적하고 있는 점 역시 이 부분이라고 할 수 있다. 기술적 대상에 대한 탐구는 그것이 발생하기 이전과 그 개체가 발생되는 과정의 여러 요소들 그리고 개체 외부의 요소들과 맺게 되는 앙상블들 속에서 계속 다른 존재로 변환을 진행하는 전체 과정을 함께 고려해야 기술적 개체 혹은 대상(여기에서는 인공지능)의 의미를, 특히 인간과 더불어 지니는 의미를 파악할 수 있다.\n그러나 기술적 대상들을 파악하는데 있어 이런 시각을 확보하는 것은 ‘기술적인 것’ 또는 ’인공적인 산물’에 대한 문화적 편견으로 말미암아 쉬워 보이지 않는다. 이를 시몽동은 다음과 같이 비판적으로 지적하고 있는데, “예술을 예술의 대상들로 환원시키는 것, 인간성을 단지 성격적 특징들을 지니고 있을 뿐인 일련의 개인들로 환원시키는 것, 이와 같은 것이 바로 기술적 실재를 기계들의 집합으로 환원시킬 때 취하는 태도다. 전자는 세련되지 못한 것으로 쉽게 판명되는 대신에 후자의 경우에는 마찬가지로 파괴적인 환원이 실행되고 있음에도 불구하고 문화의 가치들에 부합하는 것으로 간주된다.”는 것이다(Simondon, 1989/2011, 211쪽).\n우리가 인공지능이 무엇이고 그것이 우리에게 던지는 질문이 무엇인가를 파악하기 위해서는, 흔히 진행되는 방식인 기술적 대상 혹은 미디어로서의 인공지능을 인간과 분리하여 그 존재의 특성과 구성 요소 그리고 작동방식을 인공지능의 가장 줌심적 문제로 사고하는 것이 지닌 문제점을 살펴보고, 이런 파악방식만으로는 기술적 대상이나 미디어로서의 인공지능이 지닌 의미를 알 수가 없다는 점을 잘 알고 있어야 한다. 인공지능이라는 미디어의 존재론적 성격 역시 결국 생성적이며, 다른 미디어들과의 앙상블을 통해 계속 변환되는 과정에 놓여있다는 것을 인식한다는 점은 이 미디어에 대한 우리의 윤리적 접근에도 중요한 영향을 미치는 문제이다. 왜냐하면 개별적 미디어로서의 인공지능에 대한 선호나 부정 혹은 환호나 두려움이라는 것이 사실은 윤리적 문제를 고려할 때 아무런 도움을 주지 못하는 전제에 입각해서 전개되는 논의이기 때문이다.\n미디어 철학자 매클루언은 미디어와 인간이 맺는 관계를 미디어와 인간이라는 두 실체 사이의 일로 보지 않는다. 또한 어떤 하나의 미디어도 개별적으로 존재하는 독립된 실체로 여기지 않으며, 이전의 미디어들이 새롭게 등장하는 미디어들에 의해 변환된 구조에 참여한다는 점을 지적한다. 이런 그의 기본적인 입장은 미디어의 발생과 존재 방식을 입체적이고 순환적인 구조로 파악하는 것에 근거하고 있다. 시몽동과 매클루언의 주장의 유사성은 단순한 우연이 아니라, 그들이 기술적 대상을 바라보는 관점 자체가 발생론적 존재론이며, 인간과 기술의 관계를 파악하는 방식도 거의 유사한 지점을 확보하고 있기 때문이다.\n시몽동의 이른바 ’관계의 존재론’은 서양철학의 전통에서는 현저하게 예외적인 개념적 구도를 보여준다. 그는 존재냐 생성이냐의 이분법을 거부할 뿐만 아니라 전통적인 사상과는 다르게 생성으로부터 존재를 구성하려는 시도를 하고 있다(황수영 2015, 118쪽). 또한 매클루언이 생각하는 미디어와 인간의 관계는 미디어의 측면에서 보면 미디어는 ’인간의 확장된 몸’이며, 인간의 측면에서 미디어를 보면 인간의 몸이란 ’스며든 기술’로 파악될 수 있다(김상호, 2009). 다시 말하자면, 매클루언과 시몽동이 공유하는 기본적인 생각은 생성과정을 중심에 두고 그 생성된 대상으로서 기술적 대상-미디어를 파악하고, 인간과 기술적 대상들은 상호 협력적 관계 속에서 그 존재론적 의미를 획득한다는 것이다. 따라서 이 두 사상가의 사고 속에서 주인으로서의 인간의 지위와 노예(혹은 도구)로서의 기술적 대상-미디어의 지위는 더 이상 존재할 수가 없다. 매클루언이 그의 책 속에서 보여준 기술적 대상이나 미디어의 이와 같은 지위의 상승 혹은 존재론적 재설정은 기존의 형이상학적 입장이나 인간중심주의적 입장에서 보았을 때, 그의 입장을 기술결정론적이라고 비판할 수밖에 없을 것이다.\n기술에 대한 인간중심주의적 접근을 벗어나 새로운 관계 설정을 주문하는 철학자들의 주장은 보기에 따라 사람들에게 충격적으로 다가갈 수 있다. 그러나 이들의 이런 주장을 살펴보면 새로운 미디어 환경에서 벌어지는 존재들의 앙상블에서는 새로운 인식이 요청되고, 여기에는 결과적으로 새로운 윤리적 질문과 요청이 생겨날 수 밖에 없다. 다음 주장들은 인간-기술 간 새로운 관계 설정의 요청 사항들이다.\n\n“인간이 기술적 대상들보다 열등하거나 우월하지 않아야 한다.”(Simondon, 1989/2011, 129-130쪽).\n“기술(또는 다양한 방식으로 확장된 신체)을 정상적으로 사용하는 사람은 그 기술에 의해 끊임없이 변형되고, 다시 그의 기술을 새롭게 변형시키는 방법들을 찾아내게 된다. 마치 벌이 식물의 생식기이듯이 인간은 말하자면 기계 세계의 생식기로서 언제나 새로운 형태들을 수태시키고 진화시키는 것이다.”(McLuhan, 1964/2011, 107쪽).\n“인간의 진정한 본성은 연장들의 운반자, 그래서 기계의 경쟁자가 아니라, 기술적 대상들의 발명가이며 앙상블 안에 있는 기계들 사이의 양립가능성의 문제를 해결할 수 있는 생명체다. 기계들의 수준에서, 기계들 사이에서, 인간은 그 기계들을 조정하고 그것들의 상호 관계를 조직화한다. 인간은 기계들을 다스리기보다는 양립가능하게 만들며, 정보를 수용할 수 있는 열린 기계의 작동이 내포되어 있는 비결정성의 여지에 개입하여 기계로부터 기계로 정보를 번역해주고 전달해 주는 자다. 인간은 기계들 사이의 정보 교환이 갖는 의미작용을 구축한다. 인간이 기술적 대상에 대해 갖는 적합한 관계 맺음은 생명체와 비생명체 사이의 접속으로 파악되어야만 한다.”(Simondon, 1989/2011, 385쪽).\n\n기술적 대상들/미디어는 각기 떨어져서 존재하는 요소가 아니며, 다른 수준에서 다른 형태로 존재하는데, 그것에 개입하는 중요한 요인이 바로 인간이라는 사실이 시몽동과 매클루언의 사상의 공통점이다. 시몽동에 따르면 기술적 대상들은 요소-개체-앙상블의 세 수준들에서 존재하며, 기술성은 이 세 수준들을 따라 변환하며 역사적으로 발전해 나가고, 이 기술성의 발달 정도에 따라 기술적 대상들과 인간 사이의 관계 양상도 달라진다는 것이다.\n이런 발생론적 혹은 생성의 존재론과는 달리 기술에 대해 그리고 미디어에 대해 전통적인 접근들이 지니고 있는 공통적인 전제는 기술이나 미디어의 존재 양태를 ’형상-질료’라는 서양 형이상학의 전통적인 이분법에 근거하여 파악하고 있는 것이다. 기술이나 미디어를 인간의 의지나 필요에 따라 사용 혹은 처분 가능한 대상이라고 보는 관념은 주인으로서의 인간과 노예로서의 기술이라는 관념이 여전히 투사, 적용되고 있다고 볼 수 있다.3 이는 단순히 기술/미디어와 인간의 관계를 파악하는 것에 국한된 것이 아니라 어떤 의미에서는 서양 철학사 전반에 걸쳐 형성된 일반적인 사고의 방식과 관련이 있다. 흔히 ’생성의 철학(혹은 관계론적 접근)’과 ’존재의 철학(개체성의 철학)’으로 대비되는 이 긴장은 매클루언과 시몽동의 발생론적 존재론을 파악하는 시작 지점이라고 할 수 있을 것이고, 인공지능을 파악함에 있어 철학적으로 대비되는 두 접근방식을 점검해볼 수 있는 기본적인 지점이 될 수 있을 것이다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>인공지능 윤리의 모색을 위한 철학적, 기술학적 기초</span>"
    ]
  },
  {
    "objectID": "basic.html#더-생각해볼-내용",
    "href": "basic.html#더-생각해볼-내용",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "3.1 더 생각해볼 내용",
    "text": "3.1 더 생각해볼 내용\n과거에 이루어진 의도적 불공정, 예컨대 유색인종, 여성, 장애인, 성소수자 등에 대한 차별들은 데이터 학습 과정에서 배제되지 못함으로써 보호 속성을 지닌 인구 집단에 대해 차별로 이어질 가능성이 존재한다. 단적인 예가 2020년 한국의 모 스타트업 기업에서 개발한 AI 챗봇 ’이루다’이다. 이루다는 20대 여성을 컨셉으로 하고 있었는데, 이용자들이 성소수자에 대한 질문을 했을 때 혐오성 발언을 내뱉을 뿐만 아니라 이루다에 대한 성희롱성 발언에 적극적으로 동조하고 그것을 수용하는 대답을 보냈다. 결국 이루다는 차별과 혐오를 증폭시킬 수 있다는 우려와 함께 이용이 중단되었다.\n\n새로운 기술에 대한 윤리적인 질문과 답변은 기술이 가진 기술적인 특성이나 사용과 관련된 구체적인 특성에 국한되지 않고, 기술의 전체적인 앙상블을 고려해야 한다. 위의 사례를 보면 인공지능 기술이 맺고 있는 관계맺음의 양상은 어떻게 고찰될 수 있고 또 그 속에서는 윤리적 질문은 어떻게 제기될 수 있는가?",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>인공지능 윤리의 모색을 위한 철학적, 기술학적 기초</span>"
    ]
  },
  {
    "objectID": "basic.html#더-읽으면-좋은-자료",
    "href": "basic.html#더-읽으면-좋은-자료",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "3.2 더 읽으면 좋은 자료",
    "text": "3.2 더 읽으면 좋은 자료\nMcLuhan, M. (1964). Understanding Media: The Extension of Man. Gingko Press. 김상호 (역) (2011), 《미디어의 이해》. 커뮤니케이션북스.\nSimondon, Gilbert (1989). Du mode d’existence des objects techniques. Editions Aubier. 김재희 (역) (2011). 《기술적 대상들의 존재양식에 대하여》, 그린비.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>인공지능 윤리의 모색을 위한 철학적, 기술학적 기초</span>"
    ]
  },
  {
    "objectID": "basic.html#참고문헌",
    "href": "basic.html#참고문헌",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "3.3 참고문헌",
    "text": "3.3 참고문헌\n김상호(2002). 기술논의의 전개과정에 관한 비판적 고찰, 《언론과 사회》 10권(4호), 58-89쪽\n김상호 & 이호규(2003). 해롤드 이니스의 커뮤니케이션 사상: 편향을 중심으로, 《언론과 사회》 11권(3/4호), 78-107쪽.\n김상호(2004). 엔텔레키를 중심으로 해석한 맥루한의 미디어 개념, 《언론과 사회》 12권 4호, 79-116쪽.\n김상호(2008). 맥루한 매체이론에서 인간의 위치: 기술우선성을 중심으로, 《언론과학연구》 8권 2호, 84-121쪽.\n김상호(2009). 확장된 몸, 스며든 기술: 맥루한 명제에 관한 현상학적 해석, 《언론과학연구》 9권 2호, 167-206쪽.\n김상호(2011). 한국 텔레비전 테크놀로지의 사회적 수용-매클루언의 접근방식을 중심으로 《방송문화연구》 제23권 1호, 73-107쪽.\n김재희(2008). 베르그손에서 잠재성과 물질의 관계, 《시대와 철학》 제19권 2호, 9-44쪽.\n김재희(2011). 물질과 생성 : 질베르 시몽동의 개체화론을 중심으로. 《철학연구》 제93집, 231-260쪽.\n김재희(2014). 포스트휴먼 사회를 사유하기 위한 하나의 청사진-질베르 시몽동의 기술-정치 학. 《범한철학》 제 72권, 387-414쪽.\n김화자 (2011). 질베르 시몽동의 기술철학에 나타난 ’기술성의 의미: 현대 정보기술문화 이해 를 위한 소고. 《철학과 현상학연구》 제 51집, 35-66쪽.\n브루노 라투르 외(2010). 《인간 • 사물 동맹》, 홍성욱 엮음. 이음.\n연효숙(2013). 들뢰즈의 기관 없는 신체와 개체성의 문제. 《헤겔연구》 제 34호, 259-280쪽.\n이정우(2004). 《개념-뿌리들》, 철학아카데미.\n이지훈(2002). 시몽동: 생명의 자연철학, 《과학과 철학》 제13집, 139-157쪽.\n황수영(2003). 《베르그손, 생명과 지속의 형이상학》, 이룸.\n황수영(2005). 《근현대 프랑스철학，데까르뜨에서 베르그손까지》, 철학과 현실.\n황수영(2009). 시몽동의 개체화 이론: 프랑스 생성철학의 맥락에서, 《동서철학연구》 제53호, 199-224쪽.\n황수영(2014). 《베르그손, 생성으로 생명을 사유하기》, 갈무리.\n황수영(2015). 시몽동의 생성의 존재론에서 물질과 생명의 연속성과 불연속성. 《철학연구》 제 111집. 93-121.\nAristotle (1957). Metaphysica. Oxford University Press. 김진성(역) (2007), 《형이상학》, 이제이북스\nBadmington, Neil. ed.(2000). Posthumanism : A Reader, London：Palgrave.\nBarthelemy, J. & Norman, Barnaby (2015). Life and Technology : An Inquiry into and beyond Simondon. Luneburg : Meson Press\nBergson, Henri(1907). (L’)evolution creatrice. 황수영(역) (2005), 《창조적 진화》, 아카넷.\nBradley, Arthur.(2011). Originary Technicity: The Theory of Technology from Marx to Derrida, London- Palgrave. Macmillan.\nDeleuze, Gilles(1968). Difference et Repetition. 김상환 (역) (2004), 《차이와 반복》, 민음사.\nDe Boever, A. Murray, J. Roffe & A. Woodward (2012). Gilbert Simondon : Being and Technology. Edinburgh University Press.\nGow, Gordon(2001). “Spatial Metaphor in the work of Marshall McLuhan”, Canadian Journal of Communication, 26. 63-80.\nKurzwil, Ray(2005). The Singularity Is Near. 김명남•장시형 (역)(2007), 《특이점이 온다》, 김영사.\nLatour, Bruno(1991). We Have Never Been Modern. 홍철기 역(2009), 《우리는 근대인이었던 적이 없다》, 갈무리.\nHall, Edward(1959). The Silent Language. 최효선(역) (2000), 《침묵의 언어》, 한길사.\nHerbrechter, Stefan (2013). Posthumanism： a Critical Analysis, London- Bloomsbury.\nInnis, H. A. (1950). Empire and Communications. Oxford: The Clarendon Press.\nInnis, H. A. (1951). The Bias of Communication. Toronto: University of Toronto Press.\nMcLuhan, Marshall (1987). Letters of Marshall McLuhan. Molinaro, Matie, Corinne McLuhan & William Toye (eds.) NY: Oxford University Press.\nMcLuhan, M. (1964). Understanding Media: The Extension of Man. Gingko Press. 김상호 (역) (2011), 《미디어의 이해》. 커뮤니케이션북스.\nMcLuhan, Marshall, (1962). Gutenberg Galaxy: The Making of Typographic Man. Toronto: University of Toronto Press.\nMcLuhan, M & Eric McLuhan (1988). Laws of Media: The New Science.\nToronto :University of Toronto Press.\nMoravec, Hans (1999). Robot: Mere Machine to Transcendent Mind, Oxford- Oxford University Press.\nSimondon, Gilbert (1989). Du mode d’existence des objects techniques. Editions Aubier. 김재희 (역) (2011). 《기술적 대상들의 존재양식에 대하여》, 그린비.\nSimondon, Gilbert (1993). ‘The Genesis of the Individual’, in J. Crary & S. Kwinter (Eds.) Incorporations, NY: Zone. pp. 297-317.\nStiegler, Bernard (2009). Technics and Time 2: Disorientation, Stephen Barker tr.. California: Stanford University Press. Wolfe, Cary (2010). What is Posthumanism?, Minneapolis: University of Minnesota",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>인공지능 윤리의 모색을 위한 철학적, 기술학적 기초</span>"
    ]
  },
  {
    "objectID": "basic.html#footnotes",
    "href": "basic.html#footnotes",
    "title": "1  인공지능 윤리의 모색을 위한 철학적, 기술학적 기초",
    "section": "",
    "text": "물론 시몽동은 기술적(물리적) 대상과 생명체의 존재론적 차이를 분명히 하고 있지만, 기술적인 대상이든 생명체든 그것이 개체라는 존재의 발생론적 관점에서 이들 모두를 포괄하고 있다. 그가 예를 드는 결정(結晶)의 생성과 같은 경우가 가장 대표적인 예가 될 것이다. 이를 시몽동은 “기술적 대상들은 단순히 사용도구로서 취급될 수 있는 것들이 아니라 구체화의 정도에서 차이가 나는 특수한 개별자들로서 자기 나름의 발생과 진화를 겪는 것들이다.”라고 지적하고 있다(Simondon, 1989/2011, 9쪽).↩︎\n“열린 기계들의 앙상블은 인간을 상설 조직자로 기계들을 서로 연결시켜주는 살아있는 통역자로 상정한다. 노예집단의 감시자이기는커녕, 인간은 마치 연주자들이 오케스트라의 지휘자를 필요로 하듯이 그를 필요로 하는 기술적 대상들 모임의 상설 조직자다.” (Simondon, 1989/2011, 13쪽) 또한 “지휘자는 모든 연주자들을 모두에게 서로서로 연결시켜주는 통역자인 것이다. 이와 같이 인간은 자기 주위에 있는 기계들의 상설 발명가이자 조정자로 존재하는 기능을 갖는다. 인간은 자신과 함께 작용하는 기계들 가운데 존재한다.” (Simondon, 1989/2011, 14쪽)↩︎\n이런 점에 대해 시몽동은 다음과 같이 지적한다. “힘을 얻기 위해 기계들을 활용하는 장소로서 기술적 앙상블을 취급하는 철학은 기술에 대한 독재적인 철학이라고 부를 수 있을 것이다. 여기서 기계는 단지 수단일 뿐이다. 목적은 자연의 정복, 즉 일차적인 예속화를 이용해서 자연의 힘들을 지배하는 것이다. 그러니까 기계는 다른 노예들을 만들어 내는데 쓰이는 노예인 것이다. 이와 같은 정복의 영감과 노예제 주창자는 인간을 위한 자유의 요청이라는 명분으로 서로 만날 수 있다. 그러나 노예를 다른 존재자들, 즉 인간들, 동물들, 또는 기계들에로 이전시키면서 자유로워지기란 어려운 것이다.” (Simondon, 1989/2011, 183-4쪽).↩︎\n시몽동이 파악하는 생성의 관점은 굳이 메를로-퐁티를 다시 상기시키지 않더라도, 현상학적 입장에서 사태를 파악하는 방식과 아주 유사하다. 특히 상호-구성적(co-constitutive)이라는 말은 현상학적 접근을 하는 사람들이 하나의 대상을 인식하는 과정에서 대상과 인식하는 주체 사이에는 어느 한쪽의 결정적인 중요성이나 근본성이 있다는 것이 아니라 양쪽의 존재 모두가 함께 만들어내는 과정이라는 점을 강조하기 위해서 사용하는 용어이다. 기술과 그것을 사용하는 사람들 사이의 관계를 파악하는 데에도 중요한 지점이다. 특히 앙상블을 통해서나 생성으로 나타나게 되는 존재의 특성을 구성적(constitutive)이라고 파악할 때, 이 말은 구조적 결정성을 강조하는 구성(construction)이라는 개념과는 다르다. 이 두 지점의 차이를 놓치면 매클루언의 논의를 기술결정론적인 것으로 파악할 여지가 있다.↩︎",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>인공지능 윤리의 모색을 위한 철학적, 기술학적 기초</span>"
    ]
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "",
    "text": "2.1 서론\n맥루한(M. Mcluhan)적 의미에서 인간 능력의 확장이라 할 미디어는 이제 인간 존재의 정수에 해당되는 지능적 기능까지 모사함으로써, 미디어를 활용하는 인간 주체의 지위 자체에 질문을 던지고 있다. 미디어 테크놀로지가 인간과의 관계에서 맺는 의미의 본질적, 전면적 변화의 가능성으로 우리 눈앞에 연출되고 있다. 인공지능의 등장으로 현대 정보사회의 성격 또한 새로운 변화의 기로에 서있다. 다니엘 벨(Daniel Bell)이 말했던 정보사회(information society)에서 미디어 테크놀로지는 기껏해야 인간을 보조하는 도구에 불과했지만 지금 등장하고 있는 인공지능 기술은 도구 이상의 의미와 효과로 인간과 관계를 맺어갈 가능성이 점쳐지고 있기 때문이다. 이로 인해 인류 앞에 펼쳐지고 있는 인공 지능 시대를 어떻게 바라볼 것인가와 관련하여 새롭게 등장하는 기술 그 자체와 기술의 활용 방식 및 부작용과 문제점 등을 정확히 알아야 할 필요와 더불어 그러한 것들을 이해하는 데 기존의 지식이 여전히 유효할 것인지도 다시 살펴볼 시점이 도래했다.\n전통 지식의 유효성을 다시금 재론해야 할 필요가 커지고 있는 영역 중 하나는 인공지능 윤리 영역이다. 인공지능이 대량의 수행 능력을 발휘하여 인간 삶에 개입하는 자동화 역량과 스스로 계산하여 판단하는 자율성 역량을 기반으로 개인의 삶과 사회 전반에 개입하면서 다양한 도덕적 윤리적 문제들이 발생하고 있고 이런 양상은 더욱 심해질 것으로 전망된다. 전세계적으로 인공지능 윤리 문제에 대한 학술적 논의와 제도적 해법 모색의 열기는 매우 광범위하게 전개되며 뜨겁게 펼쳐지고 있다. 흥미롭게도 정치권력과 기술자본이 늘 윤리를 과학기술의 진보를 발목잡고 혁신을 방해하는 장애물쯤으로 여긴 오랜 역사와 달리 인공지능 윤리 논쟁에 적극 참여하고 있다. 이 양상은 인공지능 기술이 인간과 기술, 기술산업과 윤리의 관계 상의 질적 변화를 초래하고 있음을 시사하는 징후로 해석된다(손화철, 2018; 허유선, 이연희, 심지원, 2020). 물론 이런 변화에 대한 주목은 부분적으로 인공지능 기술에 대한 과도한 환상에 기인하는 면이 없지 않고(이호영 외, 2020; 이희은, 2021), 그에 따라 인공지능 윤리 논쟁이 시기상조의 일들로 호들갑을 떠는 측면도 있다는 주장들(Leslie-Kaelbling, 2021, 10월)도 존재한다. 또는 인공지능의 윤리적 문제성을 기술적 문제로 몰고가는 것은 허상이며 실제로 인공지능은 인간의 힘과 역량을 배가시키는 쪽으로 작용한다는 더 큰 진실을 보았을 때 인공지능에 대한 과도한 의미 부여는 착각에 불과하다는 문제의식들도 있다(김진석, 2019, p.11).\n이런 다양한 입장과 관점들을 놓고 인공지능과 윤리의 관계를 이해하기 위해서는 일단 인공지능 기술이 가진 성격과 의미를 어떻게 규정할 것인지부터 중요한 쟁점이 된다. 인공지능을 인간을 대리하여 윤리적 의사결정을 수행하는 주체로 보아야 할지? 실제로 인공지능이 인간과 같이 자아나 의식을 가지고 윤리적 고민이나 선택을 하는 것인지? 이런 쟁점들과 관련하여 인공지능의 성격을 명확히 규정할 수 없다면, 인공지능과 윤리의 관계를 설명하는 것은 쉽지 않을 수 있다. 이 측면에서 인공지능이 실제로 인간과 동일한, 즉 윤리적 주체로 존재할 수 있는지와 관련한 동일 속성이나 형질을 갖는지는 대단히 중요한 문제가 된다. 이러한 문제의식을 도덕적 속성의 실재론적 접근이라고 부른다(Coeckelbergh, 2014). 이러한 실재론적 관점은 인공지능 기술이 실제로 윤리적 행위자가 될 수 있는지 그 실재적 측면에 관심을 거쳐 인공지능 같은 지능형 미디어와 윤리의 관계를 이해하는 접근법으로 나름의 타당성을 갖는다. 인공지능이 스스로 윤리적 사고를 할 수 없는 단순한 도구에 불과하다면 인공지능에 대한 윤리나 정보사회의 윤리 같은 문제는 기존의 지식 틀로 얼마든지 대처가 가능하기 때문이다.\n문제는 인공지능과 인간이 맺어가는 관계가 양적 질적으로 나날이 심화되는 상황에 주목할 때 그런 속성이 있는지 없는지 여부와 관계없이 인공지능과 윤리의 관계는 기존의 방식으로 풀어가는 데 한계가 점점 더 커질 것이란 우려에 있다. 예컨대 로봇 비서나 군사용 로봇이 실제로 생각하는 능력이 없다고 해서 단순한 도구로 취급한다면, 인공지능을 학대하거나 비윤리적 목적으로 활용하는 일도 얼마든지 정당화될 수 있다. 이로 인해서 인공지능의 속성에 주목하기보다는 인공지능과 인간이 맺어가는 관계의 확산에 따라 인공지능이라는 새로운 미디어 테크놀로지와 윤리의 관계를 이해하는 지평 자체가 달라지고 있다는 점에 주목해야 할 필요가 있다. 기술과 인간의 관계적 측면에 주목할 때 정보사회의 윤리를 정보를 사용하는 인간을 중심에 놓고 모색했던 기존의 윤리학적 사유의 틀을 넘어선 윤리학적 지평을 재구성할 수 있다.\n인공지능 윤리와 관련한 새로운 사유의 틀이 필요한 이유는 또 있다. 앞으로 어떻게 될지는 아직 섣불리 예측할 수 없지만, 미디어 테크놀로지와 인간 사이에 설정된 경계가 나날이 약화되고, 그 구분이 애매모호해지고 있다는 점이다(Mazlish, 1995). 물론 아직 먼 이야기는 하지만, 인류 역사 전체를 놓고 볼 때 인간과 기계, 자연과 인공 사이의 근본적 구분은 끊임없는 불확실성을 향해 달려 왔으며, 유전공학과 정보기술의 발달은 인간과 인간 이외의 존재들 사이에 놓여 있다고 믿겨진 근본적 불연속과 단절 그리고 그러한 불연속과 단절에 근거한 인간 주체의 고유성에 대한 믿음의 약화를 초래했다. 미디어와 인간 신체의 물리적 직접적 연결을 가능케 하는 기술이 점점 더 발전하면서 인간과 비인간 사이에 설정된 전통적인 경계나 위계가 지속될 것이란 믿음도 도전받고 있다. 이처럼 도덕이나 윤리적 쟁점이 형성되는 현실 자체가 기존의 이원론적(dichotomic) 틀에 맞지 않게 변모하고 있다면, 탈이원론적인 방향에서 윤리학적 패러다임의 전환이 고민할 필요가 있다. 현대 윤리학은 그동안 윤리학적 주체를 오직 인간으로만 상정하는 오랜 습속에서 벗어나, 동물, 자연(환경), 지구(가이아) 같은 비인간 범주를 적극적으로 포용하는 방향으로 논의의 지평을 재형성해 왔다. 오직 인간을 중심에 놓고 윤리를 사고하는 전망이 다차원적이고 복합적인 한계에 다다랐다는 문제의식 때문이었다. 이제는 그런 문제의식을 인공지능이라는 포스트 휴먼 시대의 기반 기술에 접목할 때가 도래했다.\n이 글에서는 이런 문제의식을 인공지능 시대의 정보철학으로 정립한 이탈리아 출신 정보철학자 루치아노 플로리디(Luciano Floridi)의 정보 철학을 소개하면서, 그의 정보철학에 기초해 인공지능 시대 윤리를 어떻게 이해해 가야 할지에 대해 설명한다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>인공지능과 정보철학/윤리</span>"
    ]
  },
  {
    "objectID": "info.html#정보적-세계관을-이해하기-왜-사회가-아니라-인포스피어인가",
    "href": "info.html#정보적-세계관을-이해하기-왜-사회가-아니라-인포스피어인가",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "2.2 정보적 세계관을 이해하기: 왜 ’사회’가 아니라 ’인포스피어’인가?",
    "text": "2.2 정보적 세계관을 이해하기: 왜 ’사회’가 아니라 ’인포스피어’인가?\n최근 기술 사회 담론은 현대사회를 4차산업혁명시대, 인공지능사회, 지능정보사회 등 다양한 표현으로 부르고 있지만 그 초점은 정보혁명이라 할 수 있다. 플로리디가 말하는 정보 혁명은 탈-, 후기-사업사회론에서부터 본격적으로 등장한 기존의 미디어-사회 담론과는 다른 의미를 갖고 있다. 기존의 정보혁명은 주로 새롭게 등장한 정보통신기술을 중심에 놓고 사회가 작동하는 방식을 설명하고자 했다면, 플로리디가 말하는 정보사회는 우리, 즉 인간이 누구인가에 관한 이해의 문제를 끌어들인다는 점에서 기존의 정보사회 이론들과는 구별된다. 첫째, 플로리디에 따르면, 기존의 정보혁명론들은 대부분 인터넷혁명, 디지털혁명이 정보에 관한 최초의 혁명인 것처럼 설명한다. 그러나 플로리디가 볼 때, 인간은 초기 문자를 발명했을 때부터 정보적 삶을 살아 왔기 때문에, 문자 정보를 도구로 삼기 시작한 기술적 조건의 출현 때부터 사회의 정보적인 차원은 존재했다는 것이다. 둘째, 플로리디의 관점에서 정보는 도구, 자원의 범주가 아니라 우리가 살아가는 환경, 인간의 행위를 가능케 하면서 동시에 제약하는 조건 그 자체라는 점이다. 셋째, 인류 역사는 정보를 전달, 저장하는 시대를 넘어서 정보 환경 그 자체를 존립 조건으로 삼는 단계로 넘어왔다는 역사의식이다.\n정보를 이해하는 플로리디의 논의에서 주목할 부분은 정보라는 것이 인류 역사에서 기존에는 존재하지 않았는데 기술 발전에 의해 출현한 것이라고 이해하기보다는 인류 역사 전체에 항상 존재했던 것으로 간주해야 한다는 관점에 있다. 이러한 관점에서 플로리디는 바이오 스피어(biosphere)와 인포 스피어(infosphere)를 구분한다. 플로리디에 따르면 인포스피어는 물질, 지구, 자연 환경 같은 바이오스피어에 그 존재 자체가 가려져 있었지만, 기술 발전에 의해 인식될 수 있었다고 본다. 인포스피어라는 개념은 왜 중요할까? 그것은 기존의 정보윤리나 정보철학이 정보를 사유하는 거리는 (인간이) 정보를 어떻게 다루어야 하는가?에 국한되었다면, 인포스피어 개념은 정보 자체를 중심에 놓고 세계를 인식하는 관점을 열어주기 때문이다.\n\n\n\n&lt;그림 1&gt; 정보권(inforsphere)\n\n\n출처: http://si410wiki.sites.uofmhosting.net/index.php/Infosphere\n이를테면 플로리디는 인포스피어는 인간이 자연, 물리적 조건에서 살아가는 시공간이 아니라 모든 정보적인 행위자들이 서로 상호작용하는 환경이라고 말한다(Floridi, 2014, pp.25-48). 얼핏 보면 인포스피어 개념은 온라인 공간이나, 사이버 세계, 디지털 공론장 같은 개념과 별 다를 바 없어 보인다. 그러나 플로리디의 인포스피어 개념은 온/오프라인을 모두 포괄하는 개념으로, 정보를 주고 받으며 상호작용하는 모든 존재들이 공존하는 장이다. 이런 공간적 발상이 특히 의미있는 부분은 인간과 비인간 존재들 사이의 관계성, 연결성에 특히 주목할 수 있게 하기 때문이다. 따라서 플로리디는 인간, 로봇, 인공지능, 사물인터넷 같은 기술적 존재들이 모두 관계를 맺는 부분을 이야기하기 위해서 기존의 ’사회’라는 개념 대신 정보적 장, 즉 정보권이라고 표현한다.\n플로리디가 사회라는 개념보다 인포스피어라는 표현으로 세계를 규정하는 또 다른 이유는 윤리적 문제가 이제는 인간과 도구, 인간과 인간 사이에서만 형성되지 않기 때문이다. 이는 플로리디의 3차 기술 개념(Floridi, 2014, pp.25-32)3 을 보면 잘 나타난다. 인간과 직접 상호작용하지 않고 컴퓨터나 거대 데이터 센터 내부에서 기술과 기술 사이에서만 서로 교류하는 커뮤니케이션 과정들을 생각해 보자. 이 기술들은 인간과 일상적으로 결코 접촉하지 않는다. 뿐만 아니라 이런 기술들은 인간의 직접적 통제와 조작 관리가 없어도 독립적 혹은 준-독립적으로 행위한다. 더구나 이 기술들은 데이터 학습을 통해서 시스템 내부에서 인간의 의도적 개입이나 통제가 없더라도 자신의 내적 상태가 변화될 수 있는 성격을 가진 존재이다. 문제는 내적 상태가 변화하면, 행위가 변할 수 있고, 그러면 그 행위의 윤리성도 변할 수 있다는 점이다. 즉, 여기서 인공지능은 윤리적 자아나 의식의 보유 여부와 무관하게 윤리적 존재성을 갖는다. 일반인공지능 출현 가능성의 관건 요소로 주목받는 마음(mind)이나 심리적 지향(orientation)이 없다 하더라도 자율적 존재로서의 의미를 부여할 수 있다는 것이다.\n\n\n\n&lt;그림 2&gt; 1차 기술, 2차 기술, 3차 기술\n\n\n출처: http://si410wiki.sites.uofmhosting.net/index.php/Infosphere\n물론 3차 기술들도 인간의 설계와 학습, 그리고 통제 범위 안에서 행위하기 때문에, 3차 기술의 의미를 지나치게 과장할 수 없다는 반론도 가능하다. 하지만, 인공지능과 로봇의 자율성과 인간의 자율성이 똑같은 발생학적(embryological) 조건을 공유한다는 가정이 없어도, 인공지능이나 로봇의 자율성은 실존한다.4 따라서 이러한 기술들이 서로 어떻게 상호작용하는지에 따라 다양한 윤리적, 도덕적 문제들이 형성되는 현실에서 윤리학의 영토는 더 이상 인간의 도덕적 행위에 국한된 문제의 지평으로 설정되기가 어렵다. 이런 지점들을 윤리학적으로 ’포착’하기 위해서는 세계 자체를 정보적으로 재규정할 필요가 있음을 시사한다. 이러한 측면에서 플로리디는 정보 윤리를 말하기 이전에 세계를 정보적으로 이해해야 한다고 주장하는 것이며, 정보적 관점에서 세계를 이해하려는 노력이 바로 플로리디가 말하는 정보철학인 셈이다.5",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>인공지능과 정보철학/윤리</span>"
    ]
  },
  {
    "objectID": "info.html#정보윤리학과-윤리학의-존재론적-확장",
    "href": "info.html#정보윤리학과-윤리학의-존재론적-확장",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "2.3 정보윤리학과 윤리학의 존재론적 확장",
    "text": "2.3 정보윤리학과 윤리학의 존재론적 확장\n\n2.3.1 왜 윤리학은 존재론적으로 확장되어야 하는가: 피동자 중심/존재 중심의 윤리학\n플로리디의 정보윤리학은 인간은 물론 인간 이외의 존재들을 정보라는 근본 범주의 관점에서 바라보는 존재론적 시각을 바탕으로 존재들 일반에 적용 가능한 일반 윤리 이론을 지향한다. 쉽게 말하면 윤리학을 인간 윤리학에서 끄집어 내 정보적 존재 모두의 윤리학으로 재편하자는 것이다. 자유주의든, 공화주의든, 공동체주의든, 공리주의든, 의무론이든 현대 윤리학의 모든 이론의 중심에는 인간이 놓여 있다. 현대사회가 도덕적 책임을 개별 인간이나 인간 집단 차원에서 논의하는 방식(Miller, 2001)은 따라서 동물, 자동차, 바위, 컴퓨터, 인공지능에게 윤리적인 책임을 물을 수 없다. 그들은 윤리적 행위자의 범주에 속하지 않기 때문이다.\n예컨대 로봇이 인격을 갖는지 여부를 기준으로 접근하면 로봇에 대한 학대나 비윤리적 활용은 아무런 문제가 되지 않는다. 로봇은 보통 인간을 돕기 위해 만들어지고 배치된다. 따라서 로봇과 인간 사이에는 서로를 윤리적으로 배려하고 대우해야 할 관계에 대한 도덕적 고려가 적용되어야 한다. 인간은 로봇을 돌보고, 로봇은 인간을 돌본다. 하지만 현 단계 로봇은 독립적 인격체가 아니기 때문에 인간이 로봇에 책임을 물어야 할 이유는 기각된다. 더구나 그런 책임성은 엄격한 인과성(causality)의 해명에 기반해야 하는데, 실제로는 그런 책임을 어떻게 물을지 모호하다. 로봇의 머리 속에 자리잡은 블랙박스 같은 인공지능이 행한 결과가 설계자가 유도한 결과인지 설계자의 의도와는 전혀 무관하게 발생한 것인지를 투명하게 밝히기 어렵다.\n이런 난해한 문제는 비단 인공지능과 인간 개인의 관계에서만 발생하지 않는다. 인공지능이 대량의 정보를 처리하면서 사회가 돌아가는 오늘날 거시적인 측면에서 발생하는 도덕적 해악과 고통을 누구의 책임으로 물어야 할지는 점점 더 복잡하고 불투명한 문제가 되고 있다. 그 이유 중 하나는 인간과 다양한 기술적 요인들이 복잡하게 얽혀 있기 때문이다. 오늘날 현대 사회에서의 도덕적 이슈들은 단순히 특정 행위자를 단일 원인으로 하기보다, 복잡한 행위자들의 하이브리드적 연결망(network) 차원에서 발생하고 있음에 주목해야 한다. 윤리학적 이슈와 쟁점들이 연결망의 차원에서 발생하는 현실에서 인간 행위자의 윤리적 의도와 동기에 근거한 윤리학적 사고는 실제 현실에서 윤리적 반성이나 성찰을 이끌어내기 어렵다는 한계점도 고려해야 한다. 포스트 휴머니즘 시대에 인간의 가치를 비판적으로 재해석하는 이론가들이 강조하는 것처럼, 인간 중심의 윤리학이란 애초부터 서구 자유주의에 대한 옹호 속에서 태어난 것에 불과하며(Braidotti, 2019), 디스토피아 담론들이 말하는 인간성이라는 가치는 보편적인 가치로 존재한 적이 없다(Ferry, 2015). 인간 중심의 윤리는 권력을 다른 말로 표현한 것의 일부이며, 따라서 비서구, 여성, 유색 인종은 물론이거니와 동물 윤리, 생태 윤리 같은 다른 차원의 윤리적 요청들로부터 끊임없이 도전받은 역사를 갖고 있다.\n인공지능에 관한 많은 논의들이 인공지능이 인간의 가치를 위협할 것이란 가정에 의존하고 있지만, 실제 현실을 보면 인공지능은 인공지능을 정치적, 경제적으로 이용하거나 이를 악용하려는 인간적, 집단적 의지의 도구로 전락하는 경우들이 보다 진실에 가깝다. 인공지능, 빅데이터, 로봇이 새로운 위험사회의 기술적 요인이라는 비판들과 달리 그 기술들이 윤리적으로 대우를 받지 못하는 문제를 거꾸로 살피는 것이 정보사회의 윤리가 된다고 말할 수 있다. 인공지능이나 로봇이 학대를 당하는 경우도 인간의 눈으로만 보아서는 기술과 인간의 윤리적 공존에 적절한 결과를 이끌어내기 어렵다. 플로리디가 윤리적 세계에서 행위자 범주를 기존의 인간 중심 또는 넓게 봐서 생명 중심에서 존재 중심적 관점으로 대체하고, 기술적 인공물들까지 포용하려는 배경이 여기에 있다.\n플로리디의 정보윤리학은 윤리적 책임을 규명하는 방식에 대해서도 새로운 구상을 고민한다. 기존의 윤리학은 도덕적 이슈와 쟁점을 행위자의 책임을 엄격하게 해명하는 방식에 의존한다. 이 말은 반대로 말하면 행위되지 않은 일에는 윤리적 책임을 논하기가 불가능하진 않더라도 매우 까다롭다는 의미(Scheffler, 2001; Singer, 1975)를 내포한다. 책임이 있는 행위자와 그렇지 않은 행위자를 명확히 구별하기 어려운 상황에서도 마찬가지다. 가령 사물인터넷 등 지능화된 디지털 테크놀로지들이 정교하고 복잡하게 결합된 스마트 시티에서 벌어지는 문제들은 그 책임 주체가 정확히 누구일까? 즉 책임의 원인이 누구에게 얼마만큼 할당되는지가 불분명한 상황에서는 타인과 사회에 도덕적 해악이나 고통을 발생시킨 원인 제공자를 특정하고 문제가 된 행위와 원인 제공자 사이의 명확한 인과성을 규명하기 어렵다. 인간과 기계, 기계와 기계 사이의 경계가 모호해지고 신경과학과 나노 테크놀로지 등 과학기술의 발달과 유기체와 기계 장치들 사이의 결합이 점점이 점점 더 이음새나 마찰 없이 연결됨에 따라서 이런 문제는 더 보편성을 띌 것으로 전망된다.\n플로리디는 책임이 형성되는 방식이 달라진 상황에서 각각의 행위자들의 성격은 도덕적으로 중립적인 행위자가 되어간다는 점 또한 문제시 삼는다(Floridi, 2013b). 인터넷 상에서 미디어 이용자들은 미시적 수준에서 도덕적 이유에 대한 책임을 져야 할 이유가 없는 행동들을 하지만, 이 행동들이 누적되고 중첩, 연결되면서 강한 도덕적 파급력을 지닌 결과들이 산출되는 일은 정보사회에서 흔한 일이 되었다. 그렇다면 과연 이들에게 어떻게 책임을 물어야 할까?\n이런 딜레마를 돌파하기 위해 플로리디는 행위자 중심의 윤리학을 폐기하고 대신 피동자 중심 (patient oriented)의 윤리학 또는 존재 중심적 윤리학(onto-centric ethics)을 도입함으로써 정보권(inforsphere) 시대의 도덕적 책임에 관한 윤리학을 재구성하려고 한다. 피동자 중심의 윤리학이란 정보권을 구성하는 일원으로서 정보적 시스템으로 이해가 가능한 모든 존재들에 해당되는 윤리학이다. 피동자 개념이 윤리학적 주체에 적용되어야 할 근거는 행위하지 않더라도 도덕적 책임을 적용할 수 있어야 하기 때문이다. 따라서 정보권을 구성하는 모든 존재들은 도덕적 의지와 마음을 갖춘 인간 뿐만 아니라 그 이외에 존재들, 특히 인공지능, 로봇, 사물 인터넷과 그밖의 미디어들을 모두 포함한다. 도덕적 존재들의 범위를 확장한다는 것은 도덕적으로 사고하고 행동할 줄 아는 인간이 아니더라도 도덕적 고려의 대상이 되어야 할 필요가 있기 때문에 이들의 자격 요건을 최소한도로 낮춘다는 것이다.\n피동자들에 대한 정보윤리학적 주목은 기존의 생명 중심의 윤리에서 정보 중심의 윤리로의 전환을 의미한다. 디지털 생태계에서 작동하는 기술적 인공물들은 생명이 없다 하더라도 윤리적으로 대우를 받아야 할 존재들이며, 여기서 인간 윤리냐 로봇 윤리라는 구분은 조금 덜 중요하다. 예를 들어 국내에서 인공지능 윤리 논쟁을 불러일으켰던 이루다 논란의 경우를 보면, 이루다가 도덕적으로 사고하거나 행동할 수 있는 존재이냐 아니냐가 중요한 것이 아니라 성희롱, 성착취 같은 비윤리적 행위에 대한 문제의식이 핵심이다. 이처럼 우리 모두가 피동자라는 동등한 자격을 부여받게 되면 서로가 서로를 윤리적으로 대해야 할 의무가 도출될 수 있다. 생명 중심의 윤리는 생명이 있는 것은 윤리적으로 우대하지만, 생명이 없는 무기 물질이나 기계는 함부로 대할 수도 있는 가치를 은연 중에 함축한다. 따라서 도덕적 사고와 윤리적 고려를 비생명적 존재들까지 확대해서 적용한다는 것은 행위자들로 하여금 자신의 도덕적 판단과 그에 입각한 행위 수행 시 그로부터 영향을 받는 모든 존재들을 적극적으로 고려해야 한다는 윤리적 요청의 당위적 토대를 제공한다.\n\n\n2.3.2 인간-비인간 행위자 네트워크와 분산된 도덕성 이론\n다음으로 분산된 도덕성에 관한 플로리디의 윤리학적 구상은 정보권의 특성에 따라 요청되는 윤리학적 모델이다. 플로리디는 정보권 하에서 도덕적 해악은 행위자들 사이의 복잡한 상호작용 과정에 의해 윤리적 문제로 쟁점화 되고 있다는 점에 주목한다(Floridi, 2013a, 2014). 이 점이 도덕과 윤리에 미치는 영향은 어떤 윤리적 이슈가 개별 행위자에 의해 일어난다기보다는 행위자들의 연합 즉 네트워크에 의해서 발생한다는 점에 있다. 이러한 상황은 도덕적 문제에 대한 책임의 특정한 원인 제공자를 확정하기가 어려우며, 그에 따라 인간, 비인간 존재들이 상호작용하는 과정들 속에서 각각의 행위자들 혹은 행위자처럼 기능한 기계들은 대부분 도덕적으로 중립적인 지위를 가지는 것처럼 보이는 결과를 야기하며, 기술과 인간의 상호작용이 점점 더 복잡한 양상을 그려감에 따라 도덕적 윤리적 문제에 대한 대처는 까다로워진다.\n플로리디는 이러한 난관을 돌파하기 위하여 분산된 도덕성(distributed morality)과 분산된 도덕적 행위(distributed moral action) 개념을 도입한다(Floridi, 2013a, p. 137, pp. 261-276). 분산된 도덕성 개념을 통해 플로리디는 대부분의 행위자들이 도덕적으로 중립적인 것처럼 보이지만 실제로는 도덕적 해악에 관여되는 상황의 윤리적 책임을 규명할 수 있는 대안을 제시한다. 분산된 도덕성의 상황에서 원인 제공자를 특정하기가 어려운 상황은 반대로 말하면 전체 행위자들의 작은 참여와 노력을 통해서 도덕적 상황을 개선할 수 있는 역설적 가능성을 갖는다. 가령 영향력 측면에서 무시해도 좋을 개별 행위자들의 상호작용이 가능하다면, 디지털 네트워크 상에서 발생하는 도덕적·윤리적 이슈들에 대한 대처는 효율적으로 가능하다(Floridi, 2013b).\n분산된 도덕성 모델은 도덕적으로 중립적인 것처럼 보이지만 실상은 도덕적 해악에 연관되는 행위자들 사이의 상호작용에 주목함으로써, 어떤 도덕적 악이 초래될 때 그것에 책임을 할당하는 문제를 어떻게 이론적으로 재구성할 것인지에 주목한다(Floridi, 2013a, 2013b). 이를 위해 플로리디의 분산된 도덕성 모델은 강한 인과성의 규명에 의존하기보다는 관련성(relevance)에 가까운 약한 인과성(week causality)에 주목한다. 내가 의도가 있었는지 여부보다는 어떤 연관성이 있었는지에 따라 인과적 해명 책임이 요구될 수 있다. 이러한 접근의 이점은 도덕적 관련성이 있는 주체들의 참여와 연대를 유도할 수 있다는 장점을 제공한다. 이런 접근에서 중요한 점은 의지나 의도가 없더라도 책임에 관해 고려해야 할 필요가 있다는 것이다. 플로리디가 “마음이 없는 윤리학(mindless ethics)” 혹은 “과실이 없는 책임” 이론의 필요성을 강조하는(Floridi, 2013a, pp. 148-152) 이유다. 그가 윤리학적 개념들을 이렇게 마음, 의식, 의도 없는 행위/행위자의 측면에서 재정의하려는 이유는 다시금 자율성의 문제가 관건이 되기 때문이다. 인간 이외의 알고리즘이나 인공지능, 로봇 같은 인공적인 행위자들이 강한 인과성이 없더라도 도덕적 판단에 개입하는 과정에서 발생하는 윤리적 책임을 어떻게 귀속시킬 것인가를 둘러싼 이론적 발판을 모색하는 것이다.\n\n\n\n&lt;그림 3&gt; 분산된 도덕성 모델 (distributed morality model)\n\n\n출처: https://link.springer.com/article/10.1007/s11948-012-9413-4\n그렇다면 이 지점에서 왜 우리가 피동자들을 윤리적으로 대우해야 하고, 보다 광범위한 존재들 그리고 그 존재들 사이에 도덕적 관계의 윤리학적 당위와 규범을 설정해야 하는가라는 질문이 제기될 수 있다. 왜 기계들에게도 인간 사이에만 성립하는 윤리적 관계라는 문제를 적용해야 하는가? 이에 대해 플로리디는 고통 같은 질적인 의미를 갖는 윤리학적 개념을 엔트로피(entropy)라는 정보적인 개념으로 대체하여 정보 환경에서 왜 윤리가 보편적으로 적용되어야 하는지에 대한 나름의 논리를 제공한다(Floridi, 2013a, pp. 65-84). 그것은 고통 같이 윤리학적으로 우리가 피해야 하는 개념을 엔트로피라는 정보적인 표현 형식으로 대체하는 논리로, 생명의 개념에서 보다 확장된 존재 일반에 있어서 고통은 정보적인 무언가로 재개념화하려는 시도다. 모든 정보/존재는 자신의 본질적인 가치, 즉 의미를 지니고 있고 존재 자체가 번성할 권리를 갖는다. 만약 어떤 정보나 정보적 존재, 예를 들어 로봇이 인간의 학대나 오용으로 인하여 자신의 존재가 궁핍해지는 상황에 처한다면 엔트로피는 증가한다. 플로리디는 이를 정보윤리 원칙으로 설명한다(Floridi, 2013a, pp.70-73). 그 원칙은 첫째, 엔트로피는 정보 환경에서 방지되어야 하고, 둘째, 엔트로피는 정보 환경에서 제거되어야 하고, 셋째, 전체 정보 환경은 물론 개별 정보적 존재들의 번영, 즉 그들의 잘 삶(well-being)이 고취되어야 한다는 것이다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>인공지능과 정보철학/윤리</span>"
    ]
  },
  {
    "objectID": "info.html#정보권과-윤리적-인간상-인간-중심주의를-재구성하기",
    "href": "info.html#정보권과-윤리적-인간상-인간-중심주의를-재구성하기",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "2.4 정보권과 윤리적 인간상: 인간 중심주의를 재구성하기",
    "text": "2.4 정보권과 윤리적 인간상: 인간 중심주의를 재구성하기\n플로리디 정보윤리학은 인간 뿐만 아니라 비인간 존재들까지 윤리학적 고려 대상으로 간주하려는 윤리학의 확장은 윤리적 존재로서의 인간이 갖는 가치나 의미를 축소시키려는 접근으로 평가될 수 있다. 그러한 점에서 인포스피어에서 인간의 존재 이유, 인간의 의미와 가치 그리고 윤리적 역할 상은 어떻게 되는 것인가에 관한 물음이 제기된다. 이에 대해 플로리디는 호모 포이에티쿠스(homo poieticus)라는 은유를 통해서 정보권에서 인간의 윤리적 지위를 재모색한다.\n호모 포이에티쿠스 개념은 호모 파베르(homo faber), 호모 에코노미쿠스(homo economicus), 호모 루덴스(homo ludens)라는 인간에 대한 기존 개념들과는 구별되는 윤리적 인간 상을 조명하는 개념이다. 호모 파베르는 자연, 생명 그리고 기계를 비윤리적 목적으로 활용하는 인간의 모습이 반영되어 있다. 호모 에코노미쿠스도 마찬가지로 인간의 경제적 측면, 즉 자본의 증식과 유통 그리고 소비에 매몰된 인간의 한계성을 담고 있다. 호모 루덴스 또한 유희적 인간이 타자를 배려하지 못하거나 자신의 놀이와 유희에 빠진 나머지 그로부터 유발될 수 있는 도덕적 해악에 대한 책임성이 결여되어 있는 인간상을 설명하는 개념이다. 인간에 대한 이러한 기존 정의들은 공통적으로 인간 중심적 인간성에 내재된 윤리적인 한계를 안고 있다. 이에 반해 플로리디는 정보권에서 인간이 가진 윤리적 역할을 강조하기 위한 의도에서 호모 포이에티쿠스 개념을 내세운다(Floridi, 2013a, pp.161-179). 호모 포이에틱스로서의 인간은 에코 포이에틱한 존재인데, 이는 정보권 내에서 모든 존재들을 관리해야 하는 윤리적 책임을 자각하는 인간이자 정보권 환경을 도덕적으로 구성하는 과정에 적극 참여해야 하는 윤리적 의무를 지닌 인간 존재를 뜻한다.6\n플로리디는 정보권 시대에 인간의 역할이 축소되거나 배제되는 기술소외론 또는 기술 혐오론과는 정반대로 인간의 역할에 더 큰 의무와 책임을 부여하고 있다. 이는 인간 이외에도 기술이 스스로 사고하고 행동하는 주체로 발전할 수 있는 가능성이 점쳐지는 인공지능 시대의 조류와는 맞지 않는 분석인 것처럼 보일 수 있다. 그러나 플로리디가 말하고자 하는 바는 인간과 비인간은 정보권을 함께 만들어가는, 즉 구성해가는 존재라는 점에 주목해서 포스트 휴머니즘 시대에 인간의 윤리적 역할이 더 축소된다는 관점을 거부해야 한다는 것이다. 왜 이렇게 바라볼까? 그것은 인간이 피동자 혹은 존재들에 대한 더 많은 의무와 책임을 지는 상황이 연출되고 있기 때문이다. 예컨대 과거의 테크놀로지가 사회에 미치는 영향력의 도달 범위나 파급력은 제한적이었지만 현대사회의 기술들이 사회에 미치는 영향력의 도달 범위와 파급력은 다르다. 그런 기술을 개발, 보유, 운영하는 인간은 환경에 더 큰 영향력을 행사할 수 있다. 정보권에서 인간에 더 큰 윤리적 의무와 책임이 부여되는 것은 이 때문이다. 인공지능과 관련해서 생각해 보자. 인간은 인공지능이라는 아이를 낳아서 키우는 부모 입장이 된다. 따라서 인공지능이라는 존재를 돌볼 도덕적 의무와 윤리적 역할은 여전히 중요하다. 아이가 자라 스스로 사고하고 행동한다 하더라도, 그 아이의 생각과 행동에 대해 부모가 윤리적 책임을 지는 문화는 아이가 진짜 인간이든 로봇이든 차이를 두어야 할까? 오히려 그 차이를 두지 않을 때 인간이라는 존재의 윤리학적 의미는 더욱 풍성해지고 그 존재 의의가 지속되는 것은 아닐까?\n인공지능이 단순한 도구가 아니라 인간과 같은 진정한 행위자로 진화한다 해도 인간이 가진 존재론적 의미는 퇴색되지 않는다는 플로리디의 정보철학은 요즘 유행하는 인공지능에 대한 인문학적 비평들이 가진 한계점을 비판한다. 전통적 휴머니즘이나 고전적 인문학의 관점에서 기술의 비윤리성을 무작정 비판만 하는 것으로는 현실의 윤리적 변화를 이끌어 내기 어렵기 때문이다. 오히려 무작정 인간 중심성을 외치는 방식은 휴머니즘의 미래 가능성을 제약하는 효과만 일으킬 수 있다. 심지어 전통적인 휴머니즘에 입각한 기술 비평들은 현실에서 인간에 의해 야기되는 도덕적 해악들을 제대로 비판하지도 못한다는 점에서 쓸모가 적다고 말할 수도 있다. 기술을 윤리적으로 악용하는 사회 구조나 질서 그리고 정치경제학적 불평등 같은 문제들을 이야기하지 않은 채 ’인간’의 가치만을 이야기하는 것은 추상적으로는 아름답게 들릴지 몰라도 실제 현실을 분석하는 데에는 실천적인 이론이 되지 못한다.\n이러한 맥락에서 플로리디의 정보철학은 단순히 인간 중심성을 극복하거나 탈피하자는 최근의 포스트휴머니즘적 사고와 미묘한 차이를 보인다. 즉 그는 인간 중심주의를 폐기하기보다는 오히려 인간 중심주의를 재구성하고, 그 본래 의미를 재발견 해가자는 쪽에 가깝다. 실제로 플로리디의 정보철학에서는 존재론의 확장과 이를 수용하는 윤리학의 체계를 이야기할 뿐, 기술과 인간 사이에 누가 더 우월한지, 기술과 인간 사이의 위계가 어떻게 결말 지어질 것인가에 대한 이론적 시나리오를 상정하지 않는다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>인공지능과 정보철학/윤리</span>"
    ]
  },
  {
    "objectID": "info.html#결론",
    "href": "info.html#결론",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "2.5 결론",
    "text": "2.5 결론\n플로리디의 정보윤리학은 정보적 세계에 걸맞은 보편 윤리학의 토대를 제시한다. 그의 윤리학은 존재론적 토대 위에서 세계 내 존재들 간의 윤리적 관계를 어디까지 전망할 수 있을지에 관해서 또 한번의 새로운 확장을 요청한다. 이렇게 윤리학의 경계를 확장하려는 시도는 물론 현대 윤리학에서 새로운 시도는 아니다. 윤리학은 과학기술윤리, 동물윤리, 환경윤리 등 윤리학의 경계에 대한 끊임없는 확장을 추구해 왔다. 플로리디는 마치 라뚜르(B. Latour)처럼 인간과 비인간의 복잡다양한 연결들에 기초해 구성되는 정보권이 도덕적인 세계로 나아가기 위해서 기존 윤리학의 대전제들부터 넘어서고자 한다. 마음이 없는 윤리학, 피동자 중심/존재 중심의 윤리학, 분산된 도덕성 모델들은 바로 그런 원리들이다.\n물론 플로리디의 정보윤리학적 원리들은 추상적인 수준의 설명에 머물러 있기 때문에 이러한 원리들을 현실에 구체적으로 적용할 수 있는 수준에서 윤리학적 처방을 제시해주진 않는다. 그럼에도 불구하고 플로리디의 정보윤리학이 제공하는 시사점은 적지 않다. 무엇보다 윤리적 세계의 범위를 어디까지 얼마만큼 설정할 것인가와 관련해서 그의 정보권적 관점은 온라인과 오프라인의 경계가 해체되고 정보가 영향을 미치는 범위가 급속도로 확산되는 오늘날 상당한 설득력을 보여준다. 디지털 장치와 네트워크에 의존하는 사회적 커뮤니케이션 시스템 안에서 인간이 커뮤니케이션 하는 대상은 이미 인간이 아닌 지능화된 컴퓨터 장치가 되고 있다. 이러한 소통적 조건에서 더 이상 윤리적 타자는 인간 범주로 환원되지 않으며, 윤리적 타자는 이제 인공지능 같은 기술적 인공물들로 확대되어야 한다.\n지금의 많은 인공지능 윤리 담론들은 자본과 권력, 민족과 인종, 젠더 등 다양한 차원에서 위계적이고 불균등한 인간 관계들이 지속되고 있는 사회적 조건의 지속 안에서 기술이 차이와 다양성을 넘어서 불평등과 차별을 야기하는 비윤리적 상황을 막기 위해서 인간 중심주의에 호소하고 있다. 그러나 이런 인공지능 윤리 담론들은 인간과 비인간들이 상호작용하는 네트워크로서의 사회를 실질적으로 윤리화 하자는 지향보다는 인공지능이 기술 제공자와 소비자 사이에서 윤리적 충동을 일으키지 않는 범위 내에서 작동하게 만드는 데 큰 관심이 있는 것처럼 보인다. 인공지능 윤리가 이렇게 협소하게 적용되면 인공지능 윤리라는 개념은 인공지능과 더불어 변화하는 세계에서 발생 가능한 비윤리적 측면들을 윤리적으로 세탁하는 도구로 전락될 수도 있다(Vincent, 2019).\n인공지능이 진정한 도덕적 행위자가 될 수 있느냐 없느냐 같은 본질주의적 접근 방식으로 인공지능 윤리를 논쟁하는 것은 어쩌면 덜 중요한 쟁점이다. 그렇게 될 가능성이 있든 없든, 정보권이 도덕적인 세계로 나아갈 수 있는지 없는지가 윤리학적 핵심이다. 따라서 트랜스 휴머니즘이나 포스트 휴머니즘의 거친 파도 속에서도 정보적 세계에 대한 윤리적 책임을 회피하지 않으려는 인간적 태도를 가진 인간관이 더욱 절실히 요청되는 역설에 주목할 필요가 있다. 그런 면에서 플로리디의 정보철학은 전통 윤리학과 탈 인간 중심주의 사이를 연결짓는 다리 이론인 것으로 보인다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>인공지능과 정보철학/윤리</span>"
    ]
  },
  {
    "objectID": "info.html#더-생각해볼-문제",
    "href": "info.html#더-생각해볼-문제",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "2.6 더 생각해볼 문제",
    "text": "2.6 더 생각해볼 문제\n\n’사회’라는 개념과 ’정보권’이라는 개념의 차이가 무엇일지 고민해 보자.\n’인간을 위한 인공지능’이나 ’좋은 사회를 위한 인공지능’이란 표현을 윤리학의 관점에서 비판적으로 평가해 보자.\n로봇, 인공지능, 스마트 미디어를 왜 윤리적으로 대해야 할까에 대해 고민해 보자.\n디지털 상에서 발생하는 도덕적 이슈들에 대한 책임을 어떤 방식으로 따져야 할지에 대해 고민해 보자.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>인공지능과 정보철학/윤리</span>"
    ]
  },
  {
    "objectID": "info.html#더-읽을거리",
    "href": "info.html#더-읽을거리",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "2.7 더 읽을거리",
    "text": "2.7 더 읽을거리\n목광수 (2023). 루치아노 플로리디, 정보 윤리학. 서울: 커뮤니케이션북스.\nLuciano Floridi. 석기용 역. (2022). 정보철학 입문. 서울: 필로소픽.\n마크 코켈버그. 신상규, 석기용 역. (2023). AI 윤리에 대한 모든 것. 파주: 아카넷.\nDurante, M. (2018). Ethics, Law and the Politics of Information: A Guide to the Philosophy of Luciano Floridi. Dortrecht: Springer.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>인공지능과 정보철학/윤리</span>"
    ]
  },
  {
    "objectID": "info.html#참고문헌",
    "href": "info.html#참고문헌",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "2.8 참고문헌",
    "text": "2.8 참고문헌\n허유선·이연희·심지원(2020). 왜 윤리인가: 현대 인공지능 윤리 논의의 조망, 그 특징과 한계. &lt;인간·환경·미래&gt;, 통권24권, 165-209.\n손화철 (2018). 인공지능 시대의 과학기술 거버넌스. &lt;철학사상&gt;, 68권, 267-299.\nBraidotti, R. (2019). Posthuman knowledge. Cambridge, UK: Polity Press.\nCoeckelbergh, M. (2014). The moral standing of machines: Towards a relational and non-Cartesian moral hermeneutics. Philosophy & Technology, 27(1), 61-77\nFerry, L. (2015). La révolution transhumaniste. Paris, France: Plon.\nFloridi, L. (2008). The method of levels of abstraction. Minds and Machines, 18(3), 303–329.\nFloridi, L. (2013a). The Ethics of Information. Oxford: Oxford University Press.\nFloridi, L. (2013b). Distributed morality in an information society. Science and Engineering Ethics, 19, 727–743.\nForidi, L. (2014). The Fourth Revolution: How the infosphere is reshaping human reality. Oxford: Oxford University Press.\nFloridi, L. (2017). A plea for non-naturalism as constructionism. Minds and Machines, 27, 269–285.\nFloridi, L., & Sanders, J. W. (2001). Artificial evil and the foundation of computer ethics. Ethics and Information Technology, 3(1), 55–66.\nFloridi, L., & Sanders, J. W. (2004). On the morality of artificial agents. Minds and Machines, 14(3), 349–379.\nFormosa, P. (2021). Robot autonomy vs. human autunomy: Socia robots, Aritificial Intelligence(AI), and the nature of autonomy. Minds and Machines, 31(4), 595-616\nLatour, B. (1993). Nous n’avons jamais été modernes : Essai d’anthropologie symétrique (C. Porter, Trans.). We have never been modern. Cambridge, Mass: Harvard University Press. (Original work published 1991)\nMazlish, B. (1995). The fourth discontinuity: The co-evolution of humans and machines. New Haven, CT: Yale University Press.\nScheffler, S. (2001). Boundaries and allegiances: Problems of justice and responsibility in liberal thought. Oxford, England: Oxford University Press.\nSinger, P. (1975). Animal liberation: A new ethics for our treatment of animals. New York, NY: New York Review.\nVincent, J. (2019, April 3). The problem with AI ethics. The Verge. Retrieved from https://www.theverge.com/2019/4/3/18293410 /ai-artificial-intelligence-ethics-boards-charters-problem-bigtech",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>인공지능과 정보철학/윤리</span>"
    ]
  },
  {
    "objectID": "info.html#footnotes",
    "href": "info.html#footnotes",
    "title": "2  인공지능과 정보철학/윤리",
    "section": "",
    "text": "이 글은 필자의 연구논문 “인공지능 시대의 새로운 윤리학: 플로리디 정보윤리학을 중심으로”의 일부를 재구성하여 작성한 글임을 밝힙니다.↩︎\neudemonia38@naver.com↩︎\n플로리디는 기술을 1차 기술, 2차 기술 그리고 3차 기술의 개념들로 구분하고, 기술과 관련한 주/객체의 틀을 사용자(성), 촉진자(성), 그리고 사이(성)으로 유형화한다. 여기서 사이성이 바로 3차 기술에 관계된 것으로, 이 기술은 기술과 기술 사이를 매개하는 기능을 담당한다. 이러한 3차 기술이 출현한 결과 인간과 기술과의 관계라는 기존의 정식 이외에 기술과 기술의 관계에 관한 정식이 새롭게 논의될 필요가 도출된다. 나아가 인간은 이제 기술의 사용자가 아니라 거꾸로 기술의 소비자, 즉 기술이라는 주체(혹은 라뚜르 식으로 말하면 행위소)에 의해 작용되는 대상이 되기도 한다. 플로리디의 기술 구분론은 바로 이런 관계의 새로운 존재론적 양상들을 보여준다는 점에서, 현대 디지털 존재론에서 차별적인 이론적 지위를 차지한다고 볼 수 있다.↩︎\n인간의 자율성과 기계의 자율성의 차이에 대한 철학적 고찰로는 포모사(Formosa, 2021)의 논의를 참조.↩︎\n최근들어 인공지능 기술과 관련한 철학적 논의들 중에서 ’디지털 존재론’이니 ’정보적 존재론’이니 하는 테마들이 각광받는 이유도 이 틀에서 벗어나지 않는다.↩︎\n플로리디는 이 인간상을 그리스 로마 신화 속 데미우르고스 (demiurge), 즉 만드는 자라는 비유를 통해 부연한다. 데미우르고스는 물질 세계를 창조하는 역할을 맡은 신으로 과거 희랍 신화에서는 신중에서도 최고의 신으로 간주되었다. 기독교의 창조론에서와 같이 창조주가 존재하고 있는 질료들(물질들)을 가지고 자연과 인간을 만들었지만 그 정신만은 데미우르고스에 의해 만들어졌다는 신화다.↩︎",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>인공지능과 정보철학/윤리</span>"
    ]
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "",
    "text": "3.1 우리 모두는 포스트휴먼\n새로운 인공지능 기술이 등장할 때마다 어김없이 나타나는 반응은 놀라움과 두려움이다. 2017년 5월 인공지능프로그램 알파고가 이세돌 9단과 대국을 벌였을 때도, 2022년 11월 ChatGPT라는 생성형 인공지능기술이 등장했을 때도 인공지능을 둘러싼 반응은 크게 이 두 가지였다. 놀라움이라는 반응은 인공지능기술을 활용하여 수익을 내고자 하는 산업과 경제정책을 중심으로 형성되는데, 이런 반응은 온갖 미디어를 통해 확장되면서 인공지능을 급속히 받아들이는 기제로 작동한다. 이에 비해 두려움이라는 반응은 인공지능이 인간의 일자리를 빼앗거나 인간과 경쟁하는 맥락에서 주로 나타나며, 극단적으로는 인간의 생존을 위협하거나 인류를 파멸시키는 기술로까지 묘사된다.\n우리는 미디어담론을 지배하는 기술숭배와, SF영화들에서 지속적으로 재생되어온 기술공포라는 양극단의 어디쯤에서 인공지능기술을 신기해하면서도 두려워하는 양가적인 마음을 가지고 인공지능기술이 만들어내는 환경에서 살고 있다.\n스마트폰은 아침에 우리의 잠을 깨우고 날씨가 어떤지, 오늘 일정이 무엇인지 알려준다. 밤새 친구들이 전해온 메시지와 친구들의 일상을 들여다보게 해주고, 세상에 어떤 일들이 벌어졌는지를 전해준다. 버스와 지하철이 언제 오는지, 도로상황은 어떤지 알려주고, 택시를 불러주고 퀵보드와 자전거의 위치를 보여주기도 한다. 음악과 영화와 드라마를 골라주고, 관심과 취향에 맞는 콘텐츠를 찾아주고, 다양한 사람들을 연결해주며, 음식을 배달하고, 음식점을 추천하며, 쇼핑을 돕고 여행계획을 짜주기도 한다. 잠이 들면 혈압과 맥박, 수면상태를 살펴보고 건강상태를 체크한다. 코로나19 같은 팬데믹 상황에서는 QR코드로 동선을 보고하고, 확진자 동선알림 앱으로 안전을 확인하며, 공공앱으로 백신접종을 예약하고, 전자백신접종증명서로 출입을 허락받는 등 미디어와 함께 재난을 관리하기도 한다. 스마트폰이 없었다면 팬데믹 상황에서 우리는 생존할 수 없었을 것이다. 스마트폰이 없는 삶은 이제 상상할 수도, 가능하지도 않다. 2021년 10월 25일 발생한 KT 인터넷 서비스 중단사고는 지능화미디어가 인간과 공생하는 사회임을 단적으로 보여주었다. 인터넷 접속이 중단되자 음식주문과 배달앱의 불통으로 식사라는 기본 생존이 위협받았고, 백신접종과 병원진료가 불가능했으며, 비대면 수업 등 교육에 차질을 빚었고, 지도앱이 작동하지 않아 사람들을 만나러가는 데 어려움을 겪었다.\n이처럼 우리의 일상과 문화적 취향, 건강상태 등을 관리하는 지능화미디어는 우리와 ‘함께 산다’. 지능화미디어는 단순히 일상의 편리를 도와주는 도구가 아니라 우리의 삶 속에 스며들어 우리와 분리될 수 없는 존재가 된 것이다. 우리의 삶은 인간과 미디어 어느 한편의 일방적 지시나 통제로 이루어지는 것이 아니라 인간과 미디어가 함께 수행하는 행위들로 이루어진다. 빅데이터와 AI 알고리즘, 네트워크 등 미디어테크놀로지는 이러한 행위들을 통해 인간의 육체와 감각을 미디어와 결합시킨다. 우리가 미디어와 결합되는 접점 즉 미디어 인터페이스가 영화스크린에서 텔레비전 수상기로, 컴퓨터 단말기로, 노트북으로, 태블릿으로, 스마트폰으로, 손목시계로, 안경으로, 렌즈로, 나노 칩1으로 점점 거리를 좁히며 인간 몸으로 스며들수록 인간과 미디어의 결합은 더욱더 공고해진다. 지능화미디어는 인간의 육체와 감각과 경험이 더 이상 미디어와 분리될 수 없도록 만든다. 인간은 미디어와 공생체로서 세계를 경험하고 이해하며, 우리의 몸과 감각, 그리고 경험은 끊임없이 변주된다. 이런 점에서 우리는 인간과 미디어테크놀로지가 결합된 포스트휴먼이다.\n우리 모두가 포스트휴먼이라는 선언은, 인공지능이 인간과 경쟁하거나 인간을 위협하는 기술적 대상이 아니라 인간과 분리될 수 없는, 인간과 함께 하는 공생체임을 드러내는 것이다. 이런 선언은 인간과 비인간 사이의 경계를 가로질러 새로운 존재론과 윤리가 필요함을 역설한다. 경계짓기에 기인한 차별과 배제를 해결할 윤리적 실천의 가능성을 발견하고, 인공지능기술을 바라보는 시각, 즉 인간만이 우월한 존재이자 만물의 척도라는 인간중심주의 시각을 바꾸어야 함을 요청한다. 포스트휴머니즘은 바로 이러한 사유의 전환을 담고 있다. 포스트휴먼 시대의 윤리는 합리성과 이성에 근거한 근대적 사유에서 우선시한 존재들의 순위를 바꾸자는 것이며, 기존 질서와 다른 사회를 구성하자는 전복의 윤리이기도 하다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>포스트휴머니즘과 윤리적 인공지능</span>"
    ]
  },
  {
    "objectID": "post.html#포스트휴먼과-트랜스휴머니즘",
    "href": "post.html#포스트휴먼과-트랜스휴머니즘",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.2 포스트휴먼과 트랜스휴머니즘",
    "text": "3.2 포스트휴먼과 트랜스휴머니즘\n\n3.2.1 포스트휴먼: 사이보그와 안드로이드\n포스트휴먼은 정보기술과 나노기술, 생명공학과 인지공학 등 테크놀로지 발달로 유기체인 인간과 인공적인 기계가 결합한 혼성적 존재이다. 포스트휴먼은 인간과 기계가 수렴하여 결합하는 과정에서 출발점이 무엇인가에 따라 사이보그와 안드로이드로 나뉠 수 있다. 인간과 기계가 결합한 혼성적 존재는 사이보그로 통칭되지만, 보다 명확하게 말하자면 사이보그는 유기체인 인간이 기술과 결합하여 점점 기계화되는 것을 의미하고, 안드로이드는 기계에 인간적 요소를 삽입하여 점점 인간화되는 것을 의미한다.\n먼저 사이보그는 살(flesh)과 전자회로의 물리적 병합이라는 이미지를 떠올리게 만드는 우리시대의 막강한 문화적 아이콘이다(Clark, 2003, p. 5). 사이보그 담론은 만화, 잡지, 텔레비전, 영화, 비디오 게임, 컴퓨터 게임 등 다양한 장르의 소재이고, 과학분야 뿐만 아니라 심리학, 정신분석학, 사회학, 철학, 문학, 문화학 등 다양한 분야에서 주요 관심사이다.\n사이보그는 두 가지 유형으로 대별할 수 있다(Tomas, 1991, p. 32; 김선희, 2005). 먼저 하드웨어 인터페이스 사이보그(hardware-interfaced cyborg)는 탈유기적인 형태로 단순히 인간의 신체에 부분적으로 기계적/인공적 부품을 대체하거나 기계적 장치를 이식한 고전적 형태의 사이보그이다. 인간의 신체일부를 인공안구나, 인공심장, 인공관절 등 인공장기로 교체하거나 인공보철물을 이식함으로써 인간능력의 한계를 극복한 사이보그가 이에 해당된다. 두 번째 유형은 소프트웨어 인터페이스 사이보그(software-interfaced cyborg)로 데이터에 기반하여 신체의 경계가 불분명한 인간구성물이다. 현실세계와 가상세계를 연결하는 네트워크화된 사이보그로서 어디에나 존재하고(편재성)과 눈에 잘 보이지 않는다(비가시성). 스마트폰을 사용하는 우리는 모두 네트워크화된 사이보그라고 할 수 있다. 클락(Clark, 2003)은, 인간은 도구나 컴퓨터 같은 테크놀로지를 통해 생각하고 커뮤니케이션하는 정신적 능력을 확장시킴으로써 궁극적으로 정신을 해방시키고 있으며 그런 점에서 우리는 ’인간-테크놀로지 공생체(symbionts)’이고 타고난 사이보그 (natural-born cyborg)이다.\n다음으로 안드로이드는 인공뇌, 인간의 감정 등을 담은 인조인간을 의미한다. 안드로이드는 반드시 인간의 형상을 한 인공물에 국한되지 않는다. 알파고를 개발한 딥마인드를 자회사로 둔 구글이 2007년 모바일 시장에 진출하면서 공개한 개방형 운영체계 이름을 안드로이드로 정한 것은, 궁극적으로 인간의 지능을 가진 디지털 존재를 개발하려는 전략을 상징적으로 보여준다. 실제 구글은 인공지능 기술을 주도하는 글로벌 기업으로서 위상을 구축하고 있고, 알파고의 바둑대국도 이러한 전략의 일환으로 만들어진 이벤트이다. 알파고나 ChatGPT 같은 생성형 인공지능, 인공지능 스피커, 인공지능 의사 왓슨, 사우디아라비아 시민권을 가진 인공지능 로봇 소피아, 인간과 교감하는 소셜로봇, 호텔이나 공공기관, 음식점에서 일하는 서비스 로봇 등은 인간의 지능과 감정을 탑재하여 인간과 가까워지려는 안드로이드의 형상들이다. 인간과 기계, 둘 중 어디에서 출발하는가와 상관없이 사이보그와 안드로이드 모두 유기체적인 것과 인공적인 것이 결합한 포스트휴먼이다. 그럼에도 불구하고 인간과 기계의 결합을 어떠한 관점에서, 어떠한 지향점을 가지고 바라보는가에 따라 포스트휴먼에 관한 논의는 트랜스휴머니즘과 포스트휴머니즘으로 크게 나누어볼 수 있다.\n\n\n3.2.2 트랜스휴머니즘\n트랜스휴머니즘(transhumanism)은 ’합리적 방법을 사용하여 인간은 육체적, 정신적, 사회적으로 보다 높은 단계로 발전할 수 있고 발전해야 한다는 철학’을 의미한다(Sandberg, 2000). 트랜스휴머니즘은 인간의 육체를 기술적 보철물로 대체하는 탈육화(disembodiment)를 지향한다. 탈육화는 고깃덩어리에 불과한 거추장스러운 육체를 벗어던지고 한계를 지닌 인간의 몸을 기계장치를 통해 교체하고 확장하고자 하는 육체이탈의 욕망을 드러낸 것이다. 트랜스휴머니즘에서 나타나는 육체이탈 담론은 인간의 능력을 확장하거나 강화함으로써 영생을 꿈꾸는 트랜스휴먼을 실현하고자 한다. 특히 인간의 두뇌에 AI를 직접 탑재하거나 컴퓨터 칩을 이식하고, 인간의 신경계에 컴퓨터 전자장치를 연결하여 신체 외부의 장치를 작동시키는 신경보철은 몸과 마음의 기능을 개선하는 장치이다. 왼쪽 팔 피부에 컴퓨터 칩을 이식하여 자신의 위치신호를 컴퓨터로 전송한 케빈 워릭(Kevin Warwick)2이나, 팔에 귀를 이식하거나 인공보철물로 제3의 팔을 만드는 등 자신의 몸을 기계인간으로 개조한 스텔락(Sterlac)은 기계가 말초신경의 심연에까지 침투할 수 있음을 보여준다(마정미, 2008, 205~206쪽). 워릭이나 스텔락은 몸에 컴퓨터 칩을 내장하였지만 신경보철이 궁극적으로 지향하는 것은 뇌-컴퓨터 인터페이스(brain-computer interface)이다. 미국 국방부 산하 연구기관인 DARPA(Defense Advanced Research Projects Agency)는 뇌를 컴퓨터와 직접 연결해 뇌 신호를 포착함으로써 인간의 생각만으로 기계를 움직이게 하는 인터페이스를 개발하고 있다. 뇌가 컴퓨터를 신체의 일부로 인식하고 불완전한 정보를 보완하기 위해 새로운 능력을 개발하여 인공장치에 스스로를 적응시킴으로써 인간과 컴퓨터가 결합한 사이보그가 될 수 있다. 물론 이러한 실험의 근저에는 인간 뇌의 가소성과 신경망의 유연성에 대한 신뢰가 깔려있다.\n사고능력과 커뮤니케이션 능력을 향상시키기 위해 인터넷, 모바일미디어와 웨어러블(wearable) 컴퓨터 등 컴퓨터의 도움을 받는 것에서부터 소프트웨어 에이전트를 사용하는 것, 나아가 정신을 재구조화하여 인간-AI로 탄생하는 것은 트랜스휴머니즘으로 가는 단계들이다. 이러한 과정을 거쳐 인간은 전례가 없는 육체적, 지적, 심리적 능력을 가진 불멸의 인간인 포스트휴먼이 되는 것이다.\n트랜스휴머니즘이 지향하는 탈육화는 심신이원론에 기반한다. 근대서구사상의 근간을 이루는 데카르트의 심신이원론은 정신과 물질(육체)을 분리한다. 심신이원론에 따르면 연장(extension) 즉 공간을 점유하는 특성을 본질적 속성으로 하는 물질과, 사유를 그 본질적 속성으로 하는 실체로서의 정신이 독립적으로 존재한다(심상규, 2008, 39~54쪽). 데카르트가 확립하고자 의도했던 철학적 결론은, 정신이 육체의 우위에 있다는 것이며, 정신 혹은 사유작용을 통한 인간의 존엄성을 확보하고 동물 같은 다른 생명체보다 인간이 더 우월한 지위에 있다는 것을 드러내는 것이다. 육체이탈을 사이보그의 열망으로 보는 트랜스휴머니즘은 육체를 정신의 산물인 육체 이미지로 대체함으로써 탈육화된 정신이나 육체의 부정이라는 데카르트적 이원성을 강화시킨다3(Penny, 1994, p. 243).\n이러한 트랜스휴머니즘은 생물학적이고 지능적인 인간의 한계, 물리적 환경의 한계, 그리고 개인과 집단의 발전을 제약하는 문화적, 역사적 한계를 초월하고자 한다. 기술을 통해 인간육체의 한계를 벗어나 인간의 능력을 향상시킬 수 있다는 트랜스휴머니즘은 기술숭배에 기반한 기술적 낙관론이자 기술 진화론에 기반한다. 기후위기 같은 인류세 문제를 인공지능 기술을 통해 해결할 수 있다고 주장하는 에코모더니즘 역시 트랜스휴머니즘의 기술숭배적인 주장과 맥을 같이 하며, 인공지능을 둘러싼 주류담론들이라 할 수 있다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>포스트휴머니즘과 윤리적 인공지능</span>"
    ]
  },
  {
    "objectID": "post.html#포스트휴머니즘의-세-가지-사유-탈이원론-탈인간주의-탈인류중심주의",
    "href": "post.html#포스트휴머니즘의-세-가지-사유-탈이원론-탈인간주의-탈인류중심주의",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.3 포스트휴머니즘의 세 가지 사유: 탈이원론, 탈인간주의, 탈인류중심주의",
    "text": "3.3 포스트휴머니즘의 세 가지 사유: 탈이원론, 탈인간주의, 탈인류중심주의\n포스트휴머니즘은 서구유럽의 이상적 인간상인 단일하고 통일된 정체성을 지닌 비트루비우스적 인간을 제외하고 그 어떤 존재도 허용하지 않는 일자성을 거부한다. 그 대신 성과 인종, 자연의 차이를 인정하되 이러한 차이가 위계화되거나 차별화 기제로 작용하지 않는 비일자성 즉 복수성을 받아들인다. 이러한 포스트휴먼의 대표적 도상이 도나 해러웨이의 사이보그이다. 도나 해러웨이(Haraway, 1991)의 사이보그 선언은 1990년대 과학기술에 대한 숭배가 만연하던 신냉전하에서 사회주의자들이 주장하는 기술혐오를 반대하며 등장한다. 해러웨이는 지배자의 도구로 사용되던 과학 기술을 전유하여 여성 해방에 적극적으로 사용하자고 주장한다. 이러한 맥락에서 해러웨이가 사이보그 선언에서 설명하는 도상(그림 1) 속 여성은 기술을 두려워하지 않는 여성이다. 코요테 가죽을 뒤집어쓰고, 가슴에 반도체 칩을 부착한 채 컴퓨터 앞에 앉아 있는 유색인종 여성은 기술에 충실하면서도 기술을 배반하는 ’모순의 전략’을 통해 남성중심적 기존 질서를 전복하는 사이보그를 상징한다. 동시에 인간과 기계, 인간과 동물, 인공물과 자연의 합성물로서 이러한 이분법적 경계를 가로질러 생산적 결합을 만들어내는 유용한 연결장치로서 상상력의 원천으로 강조된다.\n\n유기체적인 것과 인공적인 것의 경계가 사라진 포스트휴먼은 유목적 주체(Braidotti), 인간-기술 공생체(Clark), 인간-기계 앙상블(Simondon), 인간-행위자 네트워크(Latour)로 논자에 따라 상이하게 명명되지만, 인공지능을 대상화하거나 경쟁상대로 보는 인간중심주의의 실체적 관점에서 벗어나, 인간과 기계가 동등한 위치에서 공진화하는 관계적 실재임을 드러낸다.\n포스트휴먼은 탈인간중심주의, 탈인류중심주의, 탈이원론이라는 포스트휴머니즘의 세 가지 사유를 실천하는 존재이다(Ferrando, 2019/2021).\n우선, 탈인간중심주의는 오랫동안 서구근대의 사고체계를 지배해온 서구근대 인간중심주의를 비판한다. 인간중심주의는 인간이 자기의식과 자율성, 창조성을 지닌다는 이유로 서구 근대 계몽된 인간이 세상을 지배하는 주체가 될 수 있으며, 다른 존재들보다 우위에 있다고 주장한다. 인간중심주의는 레오나르도 다빈치의 비트루비우스적 인간을 서구근대인의 완벽한 표상으로 설정하는데, 비트루비우스적 인간은 서구/백인/남성/이성애자/비장애인/정착민이다. 이 외의 인간 존재 즉 비서구/유색인종/여성/성소수자/장애인/난민은 ’인간’이 아니며, 따라서 차별과 배제, 혐오의 대상이 된다.\n탈인류중심주의는 인간종족주의 및 인간예외주의를 통해 종간 위계를 만들어내는 인류중심주의를 비판한다. 팬데믹은 인간이 다른 생명과 공존하며 상호의존적이고 연결된 삶을 살고 있음을 적나라하게 보여주었다. 남아프리카 공화국 사파리 도로 위에서 낮잠 자는 사자의 무리는 인간의 것이라고 당연시되던 자리가 실은 인간에게 빼앗긴 다른 생명의 자리였음을 보여준다. 사스, 메르스, 코로나19 같은 바이러스의 창궐은 개척이라는 명목으로 자행된 산림파괴와 동물서식지의 변화, 이로 인한 생물다양성의 불균형이 초래한 파국을 절감하게 한다. 여기에는 오랜 기간 당연시해왔던 인간예외주의가 자리한다. 인류중심주의는 동물이나 식물, 바이러스와 균류 등 비인간 존재자들을 자본주의적 이윤을 추구하기 위한 착취와 개발의 대상으로만 여겨 함께 일궈온 생태계를 파괴하게 만든다. 이런 점에서 기후위기와 재난을 야기한 인류세는 무어가 말하는 자본세 또는 해러웨이가 말하는 츌루세이기도 하다. 인공지능이 형성한 디지털 생태계 역시 인류세라는 곤경을 만들어내는 인류중심주의의 산물이라고 할 수 있다.\n이와 같은 인간중심주의와 인류중심주의는 차별화의 기제로 작동하는 이분법적 사고에 기반을 둔다. 인간과 자연, 인간과 동물, 인간과 기계, 남성과 여성, 문화와 자연 등 이분법의 쌍들은 경계 안을 우선시하고 경계 밖을 배제하면서 전자와 후자 사이에 주인과 노예라는 사회적 위계질서를 구축한다. 포스트휴머니즘은 이러한 이분법적 사고에 근거한 대립과 차별의 인식체계와 조직원리 등에 저항하며, 인간 아닌 존재들과의 공생과 공존, 공진화를 지향한다.\n이와 같이 포스트휴머니즘은 탈인간중심주의, 탈인류중심주의, 탈이원론을 지향한다. 포스트휴먼은 유목적 주체라는 새로운 주체로서 기술을 통한 인류진보라는 이면에 감추어진 불평등과 차별, 소외받고 주변화된 존재들, 인간이외에 동물이나 지구, 기계 등 비인간적 존재들을 논의의 중심으로 끌어들이고 모든 종을 가로질러 연대할 가능성을 열어놓는다. 유목적 주체는 유전공학과 인공지능 살상무기 등 포스트휴먼의 곤경을 바꿀 수 있는 포스트휴먼 주체로서 ’되기’라는 윤리적이고 정치적인 실천한다. 브라이도티가 제안하듯이 포스트휴먼은 복수의 주체성을 실험하는 하나의 방식으로 기계-되기, 동물-되기, 소수자-되기를 제안한다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>포스트휴머니즘과 윤리적 인공지능</span>"
    ]
  },
  {
    "objectID": "post.html#포스트휴머니즘의-윤리-공-산의-윤리post-4",
    "href": "post.html#포스트휴머니즘의-윤리-공-산의-윤리post-4",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.4 포스트휴머니즘의 윤리: 공-산의 윤리4",
    "text": "3.4 포스트휴머니즘의 윤리: 공-산의 윤리4\n현재 인류가 처한 곤경은 인류의 오랜 삶의 방식인 공존과 공생의 삶을 위태롭게 하고 있다. 이것에 대해 근본적인 성찰이 이루어지지 않는다면, 포스트휴먼의 사회는 배제와 차별, 불평등의 사회로 전락할 위험에 처할 것이다. 여기서는 포스트휴머니즘의 사유를 바탕으로 포스트휴먼이 만들어갈 사회성의 윤리 및 사회문화적 실천들을 탐색하고자 한다. 먼저 포스트휴먼이 상호의존하는 다른 종들과 공생하는 생태계를 만들기 위한 실천으로, 유목적 주체로서 연결짓기와 친척만들기를 제안한다. 이것은 경계짓기와 위계구조를 근간으로 하는 배타적・차별적 생태계를 구조적으로 해체하는 실천이다. 응답하기와 책임지기는 공존과 공생의 생태계를 유지하는 데 필요한 윤리로서 사회적 유대를 확장하기 위한 태도적 차원의 실천이다. 마지막으로 자동화에 저항하기는 데이터-네트워크-인공지능의 기술생태계를 전유하려는 자본과 권력에 대항해 정치사회적 연대를 형성하는 실천이다. 이들 각각은 포스트휴먼이 새로운 생태계를 형성하고 유지하고 지켜내기 위해 유기적으로 결합시켜야 하는 환경적, 태도적, 행위적 차원의 실천들이다.\n\n3.4.1 유목적 주체로서 연결 짓기\n공존과 공생의 윤리를 실천하는 포스트휴먼 주체로서 유목적 주체는, 기술을 통한 인류진보라는 이면에 감추어진 불평등과 차별로 소외받고 주변화된 존재들, 인간이외에 동물이나 지구, 기계 등 비인간종들과 연대할 수 있는 가능성을 열어놓는다(Braidotti, 2013/2015). 유목적 주체의 이러한 횡단적 주체성은 복잡한 상호관계의 생기적 네트워크에서 타자들과 유대할 수 있는 가능성을 제공한다. 이런 점에서 포스트휴먼은 노마디즘을 삶의 양식으로 삼는다. 노마디즘은 정주성이 가져온 구별과 분리, 차별에 저항하면서 끊임없이 탈주하려는 삶에 대한 사유 혹은 윤리를 내포한다.5 노마디즘은 서구근대가 만든 고정된 관념과 거대담론, 위계화와 정체성의 정치를 벗어나고자 한다는 점에서 포스트휴머니즘과 맥이 닿아있다.\n노마디즘의 실천행위는 낯선 것들과의 끊임없는 연결짓기이다. 연결짓기(connecting)는 클릭 하나로 맺어지는 단순한 ‘연결’(networking)이 아니라 접촉(contact)을 내포하는 보다 호혜적인 관계를 의미한다. 그것은 불안하고 유동적인 상태에서 무한히 변주될 수 있는, 위계화되지 않은 관계들을 만들어내는 실천이다. 연결짓기는 들뢰즈의 리좀(Rhizome)으로 형상화할 수 있는데, 리좀은 하나의 점, 하나의 질서를 고정시키는 나무나 뿌리와는 전혀 다른, 어떤 지점에서든 다른 지점과 연결 접속될 수 있는 중심없는 뿌리이다(Deleuze & Guattari, 1980/2001, 19~20쪽). 리좀은 시작도 끝도 없이 언제나 사이(milieu)에 있으며, 이 사이에서 “변이, 팽창, 정복, 포획, 꺾꽂이를 통해 나아간다”(47쪽). 리좀의 원리처럼 연결짓기는 모든 종들을 가로질러 다양한 이질적인 것들과 접속하며 끊임없이 탈주하는 지도그리기와 같다.\n연결짓기는 또한 이곳과 저곳, 안과 밖을 구분하고 단절시키는 경계(boundary)를 무너뜨리고, 낯선 사람들의 사회적 접촉이 활발하게 일어나는 접경(border)을 만들어내는 것을 의미한다(Sennett, 2012/2013, 138~140). 연결짓기는 같은 처지나 생각을 가진 동질적인 사람들이 아닌 이질적이고 낯선 사람들과 함께 하는 접경지대에서 이들과 공통감각을 키우는 실천이다. 접경지대에서는 고정된 정체성으로부터 벗어나 새롭게 마주한 사람들과 정보를 공유하거나 감정적 유대를 나누는 사회성을 경험할 수 있다.\n포스트휴먼의 연결짓기는 공생과 환대의 윤리를 사회성의 원리로 삼는다. 노마드는 자신이 신뢰하는 무리들과 함께 이동하며, 자신들이 기대어 사는 자연을 파괴하지 않고, 손님을 공손하게 맞이하고 융숭히 대접하며, 다른 가족에게 자신이 갖고 있던 것을 나눠주고 함께 음식을 만들어 먹는 나눔의 의무를 실천한다(Attali, 2003/2005, 75~97쪽). 이러한 노마드적 삶은 자연과 타자와 공동체에 대한 신뢰와 환대, 그리고 공생의 윤리가 어떻게 삶 속에 구현될 수 있는지를 보여준다.\n이와 같이 포스트휴먼은 무한한 접속을 통해 새로운 관계 맺음의 가능성을 항상 열어놓고 경험을 공유하고 소통하며 사회적 유대를 쌓아가는 연결짓기를 실천할 필요가 있다. 그러나 이러한 연결짓기는 누가, 어떤 이유로, 어떻게 연결하느냐에 따라 사회성 형성에 차이가 있을 수 있다. 이미 목격하고 있듯이 AI 알고리즘은 비슷한 성향을 가진 사람들만을 연결하여 필터버블에 가두고, 자신의 세계 밖에 있는 사람들을 혐오하게 만들어 사회적 연대와 유대형성을 방해한다. 플랫폼 기업은 이윤추구 목적으로 팬데믹 사회의 생존 인프라가 된 플랫폼 네트워크를 전유한다. 이처럼 포스트휴먼 공생체의 연결짓기는 언제든 권력과 자본에 포획될 위험에 처해 있는 것이다. 따라서 차별과 배제, 불평등을 거부하는 유목적 주체의 연결짓기에는 해러웨이가 말한 공-산의 사유가 깃들여야 한다. 가령 백신제조사의 지적재산권 면제를 주장하며 추가접종 대신 저소득국가의 백신접종률을 높이자고 전 세계 음악가들과 지도자들이 동참한 글로벌 시티즌 라이브(Global Citizen Live)6 같은 ‘백신 평등’ 운동이나, 팬데믹으로 공연을 할 수 없는 첼리스트들이 각자의 장소에서 연주한 동영상을 유튜브로 중계하는 코비드 첼로 프로젝트(Covid Cello Project)7, 자신이 살고 있는 집의 창문 밖 풍경을 영상으로 업로드하여 코로나19로 여행할 수 없는 서로에게 창밖 풍경을 선사하는 윈도우스왑(WindowSwap) 프로젝트8 등은 국경을 넘어 서로를 염려하고 위로하며 누구나 접속할 수 있는 느슨한 연결짓기의 사례들이라 할 수 있다.\n\n\n3.4.2 친척 만들기\n근대 인간중심주의는 인종, 젠더, 섹슈얼리티, 계급, 국적 등 정체성이라는 경계짓기의 틀을 동원하여 다른 존재들과의 관계를 사유한다. 이런 사유에서는 중심과 주변, 우월한 것과 열등한 것, 경계 내와 경계 밖이라는 대립적이고 위계적인 관계가 당연시 된다. 그런데 이러한 서구-백인-남성-이성애자-기업가 시각에 근본적으로 대항하는 방식은 그 대척점에 있는 비서구-흑인-여성-레즈비언-노동자를 주체로 세우는 것으로 충분하지 않다. 이러한 방식은 인간 아닌 존재들과의 관계에서도 경계를 구분하고 획정하는 틀을 지속적으로 필요로 하기 때문이다. 따라서 포스트휴머니즘은 위계질서를 무너뜨리기 위해 정체성의 정치를 전복하고, 사회적 관계를 바라보는 시각 자체를 재구성할 것을 요구한다.\n그 방법으로 해러웨이는 ’자식이 아닌 친척(kin)을 만들자’고 제안한다. 혈통이나 계보에 묶인 자식이 아니라, 지구상에서 번성하고 있는, 낯설고, 불가사의하고, 끊임없이 출몰하는, 활동적인 무엇, 인간과 인간 아닌 것들을 포함한 복수종(multispecies)들을 친척으로 만들자는 것이다(Haraway, 2016/2021, 177~178쪽). 서로를 구속하고 길들이며 배타적 친밀함을 유지하는 폐쇄적 가족이 아니라, 상호의존의 공생 관계에 있는 모든 인간, 동물, 기계를 친척으로 만들어 그들과 사회적 유대를 나누는 것이다. 이것은 서로를 보살피고 지지하고 묶어주는 거대한 그물망을 만드는 것이며, 서로의 삶이 깊이 연결되어 있음을 확인하는 과정이기도 하다. 그러나 친척만들기는 이 그물망에서 서로 협력하며 평화를 누리는 것이라기보다 서로를 돌보고 길들이면서도 끊임없이 ’트러블을 일으키고 트러블과 함께 하는 것’이기도 하다.\n친척을 만드는 것은 특정한 연결짓기 즉 절합(articulation)를 통해 이루어질 수 있다. 가령 아마존 카야포족 남자의 캠코더 촬영은 비디오캠코더, 땅, 식물들, 동물들, 비디오를 보게 될 청중들, 그리고 기타의 유권자들로 구성된 인간/비인간의 집합체를 만들어낸다(최유미, 2020, 259쪽). 이 집합체는 아마존 숲의 생태를 염려하는 각자의 정치적 의사를 표명하며, 연대를 통해 서로를 친척으로 삼는다.\n코로나19 팬데믹에서 위기를 맞고 있는 돌봄9에서도 친척만들기 실천을 찾아볼 수 있다. 돌봄은 가부장적 질서에서 여성만의 노동으로 폄훼되고, 신자본주의 수익창출의 수단으로 여겨진다. 돌봄의 다면적이고 심각한 위기상황을 이해하고 해결하기 위해 여러 분야의 전문가가 모인 더케어컬렉티브 (The Care Collective)는 돌보는 관계를 확장하고, 충분한 돌봄자원을 확보하기 위해 ‘난잡한’(promiscuous) 돌봄 윤리를 제안한다(The Care Collective, 2020). 난잡한 돌봄은 “인간과 비인간을 막론하고 모든 생명체 사이에서 이루어지는 모든 형태의 돌봄이 필요와 지속가능성에 따라 공평하게 그 가치를 인정받고 사용되어야” 함을 의미한다(79~80쪽). 이 윤리는 신자유주의의 진정성 없는 돌봄을 거부하고 인간-비인간 모두가 차별받지 않고 돌봄의 관계를 확장해나갈 것을 요구한다. 이것은 돌보는 친척의 범위를 가능한 한 넓히는 것이며, 난잡한 돌봄을 통해 사회성의 역량을 키우는 것이다.\n바이런(Byron, 2021)의 연구는 난잡한 돌봄의 사례를 보여준다. 바이런은 LGBTQ+ 젊은이들이 디지털 미디어를 이용해 어떻게 서로를 돌보는지를 구체적으로 분석한다. 폭력과 자살 위험에 노출되고, 공적 보건의료와 사회적 지지에서 배제된 LGBTQ+ 젊은이들은 소셜미디어 플랫폼에서 익명으로 정보와 조언과 감정적 지지를 나누며 서로를 돌보는 일상적 실천을 수행한다. 이러한 실천은 소셜미디어 플랫폼에 모인 사람들을 친척으로 만들며 ’디지털 돌봄 문화’를 만들어나간다.\n이러한 사례들은 비디오캠코더와 소셜미디어 같은 미디어테크놀로지가 특별한 연결짓기를 통해 친척 만들기에 기여할 수 있음을 보여준다. 그러나 AI 알고리즘이 보여주듯이 미디어테크놀로지는 공통의 관심사로 구획지어진 폐쇄적이고 파편화된 배타적 영역을 만들어낼 수도 있다. 그렇게 되면 위계화된 정체성 정치의 폭력성을 비판하며 차이에 기반을 둔 새로운 사회적 관계를 만들어내고자 하는 포스트휴머니즘의 친척 만들기는 실패할 수밖에 없다. 따라서 상이한 종들을 가로질러 친척을 만드는 것은, ’자식’으로 상징되는 사회적 관계를 전복하고 사회성의 구조를 새로 짜는 부단한 노력을 필요로 한다.\n이것은 인공지능과의 관계로도 확장될 수 있다. 대화형 인공지능 이루다는 사회적 약자 및 소수자에 대한 혐오와 차별 발언으로 사회적 물의를 일으켰는데, 이는 우리사회의 혐오와 차별을 그대로 반영한 것이다. 하지만, 보다 중요한 것은 우리가 어떤 인공지능을 원하는가이다. 인공지능 이루다는 친밀함의 욕망을 실현할 대상으로 기획되었기 때문에 연인끼리 주고받은 카톡 대화를 데이터로 사용한다. 따라서 젊은-여성-AI인 이루다는 욕망의 대상일 뿐 처음부터 친구가 될 수 없다. 반면에, 목적지향 대화형 인공지능인 사라(SARA, the Socially Aware Robot Assistant)10는 이용자가 편안하고 친밀한 관계에서 업무를 수행할 수 있도록 도와주는 비서로 기획되었다. 사라는 통제된 상황에서 특수한 목적으로 이루어진 대화데이터, 사회적 상호작용의 기본 원칙 및 비언어상호작용이 적용된 인공데이터를 사용한다. 사라는 사회적-감정적 유대를 강화하는 사회성을 학습하기 때문에 이용자와 인공지능은 서로에게 반응하면서 보다 친밀하고 편안한 관계를 만들 수 있다. 이런 점에서 친구의 자리를 대체하려는 이루다는 결코 친구가 될 수 없지만 이용자와 협력하는 사라는 사회적 유대를 형성하는 친척이 될 수 있다.\n\n\n3.4.3 응답하기와 책임지기\n유목적 주체의 연결짓기가 수평적이고 횡단적이며 상호의존적인 관계가 아닌 소수의 이익만을 극대화하는 불평등 구조로 작동하지 않기 위해서는 응답하기와 책임지기라는 윤리에 기반해야 한다.\n어지럽고 불안하고 뒤죽박죽인 시대에 트러블을 만들고 트러블과 함께 살아가기를 주장하는 해러웨이는 이분법적 질서와 정체성에 기반을 둔 정치를 거부하면서 인간종이 아닌 사이보그나 반려종 같은 다양한 종들간의 공-산(sym-poiesis)을 제안한다(Haraway, 2016/2021; 최유미, 2020). 공-산은 인류가 한번도 벗어난 적이 없는 오랜 삶의 방식으로서, 상호이익을 위해 협력만 하는 것이 아니라 실패를 통해 변화를 만들어가며 새로운 관계를 생성해내는 함께-만들기를 의미한다. 해러웨이의 ’실뜨기’나 키머러의 ’향모 땋기’는 공-산의 삶을 사유하기 위한 하나의 은유이다.\n\n“실뜨기는 주고받기의 리듬이 유지되는 한 모든 종류의 수족으로 다수가 놀 수 있는 것이다. 학문과 정치도 실뜨기를 닮았다. 열정과 행동, 가만히 있기와 움직이기, 정박과 출항이 필요한 꼬임과 뒤얽힘 속에서 건네주기”(Haraway, 2016/2021, 23쪽).\n\n실뜨기가 공-산인 이유는, 서로 주고 받는 방식을 통해 새로운 것을 만들어내면서 이 과정을 지속하기 위해서는 서로에게 성실하게 응대해야 함을, 우리 삶의 방식이 서로가 서로에게 기대는 상호의존성에 있음을 깨닫게 만들기 때문이다.11 여기서 중요한 것이 바로 응답하기로, 응답하기는 상호의존성의 윤리적 실천이라 할 수 있다. 이 윤리는 상대를 책임(responsibility)지는 것, 즉 서로 얼굴을 마주하고 귀를 기울이며 상대의 요구에 즉각적으로 응답(response)할 수 있는 능력(ability)을 키우는 것이다(최유미, 2020, 43~45쪽).\n실뜨기와 마찬가지로 향모12 땋기도 공-산의 사유를 드러낸다. 서로 마주보고 머리를 맞댄 채 서로의 손을 바라보며 이야기를 나누는 향모 땋기는, 원주민 토박이의 지혜와 과학지식, 그리고 이 둘을 한 데 모으는 과학자의 이야기가 함께 녹아 있다(Kimmerer, 2013/2020). 향모 땋기는 인간과 다른 종들이 상호의존 관계임을 깨닫는 과정이며, 인류세가 만든 인간과 자연의 대립, 옛것과 새것의 갈등, 이로 인한 관계의 상실을 다른 종들과의 호혜적 관계를 통해 치유하고 복원하는 것이다. 그리고 이것은 우리가 주고받은 것들에 대해 책임지는 과정을 상징한다. 이처럼 실뜨기나 향모 땋기는 포스트휴먼이 가져야 할 응답하기와 책임지기라는 윤리를 실천한다. 이 과정에서 중요한 것이 이야기하기이다. 이야기하기는 자신의 경험과 처지를 이야기하고 다른 사람의 이야기를 들음으로써 공감과 감응, 사회적 유대를 만드는 역능이다. 이야기하기는 새로운 연결을 계속해서 만들어가는 과정이며, 차별과 편견, 혐오를 없애고 세상을 바꿀 수 있는 힘을 지닌다.\n유튜브 채널인 닷페이스13는 코로나19 팬데믹 상황에서 확진자를 치료해야하는 감염병동 의료진, 집집마다 방문하여 사람들을 접촉할 수밖에 없는 가스검침원, 거동이 불편한 노인들을 돌봐야 하는 요양보호사 등 비가시적 노동자들이 자신의 이야기를 할 수 있도록 채널을 내어준다. 이들의 이야기는 사회적 편견과 불평등을 드러내는 것임과 동시에 자신들을 이해하게 만드는 것이며, 이야기를 듣는 사람들을 자신의 친척으로 만드는 것이다. 이야기를 들은 사람들은 댓글로 응답하며 함께 변화를 만들어나가는 ’친척-되기’를 실천한다. 서로의 이야기에 응답하기는 이웃의 삶을 찬찬히 들여다보되, 그 삶에 적극적으로 개입하지 않으면서도 약한 유대를 회복하고 사회적 연대로 나아갈 수 있는 기초적 실천이다. 따라서 포스트휴먼인 우리는 자기 자신과 자신이 서있는 자리에 관해 이야기할 수 있어야 하며, 서로의 이야기를 가치있는 것으로 인정하고 지지해주고, 그 이야기의 가치를 실현할 수 있는 조건들을 만들어주어야 한다.\n이처럼 이야기하고 응답하고 책임지기는 경계짓기에 의한 배제가 아니라 이질적인 것들을 받아들이며 새로운 연결짓기를 생성하는 것이기도 하다. 그것은 ’정중함의 미덕을 가지고 방문하기’일 수도 있고(최유미, 2020, 95쪽), 자리를 내어주며 사회적 성원으로 환대하는 것일 수도 있다(김현경, 2015). 중요한 것은 일상의 안부를 묻는 소소한 의례이든 사회적 지지를 확인하는 공동체 의례이든 모든 관계에는 응답하기와 책임지기가 수행되어야 한다는 것이다. 이런 점에서 응답하기와 책임지기는 짐멜이나 마페졸리가 강조하는 ’관계를 만들어내는 근본 역능’을 일상적으로 실천하는 것이며, 사회적 유대의 결핍을 채우고 사회성의 탈구현상을 해결하는 윤리이기도 하다.\n\n\n3.4.4 자동화에 저항하기\n포스트휴먼은 삶 자체가 데이터를 생산하고 이용하는 데이터 기반의 삶이지만, 데이터를 수집하고 분석하는 빅데이터 기술 및 AI 알고리즘은 객관적이지도 중립적이지도 평등하지도 않다. 즉 누가 어떤 이유로 어떤 관점에서 어떻게 데이터를 수집하는가, 누구의 데이터는 수집하고 누구의 데이터는 배제하는가, 데이터 축적과 가공에는 어떤 알고리즘이 작동하는가 등 데이터 수집과 분석, 활용에는 정치적, 사회적 논리들이 작동한다. 특정 집단의 데이터만을 위해 데이터 알고리즘이 설계될 경우 데이터 수집에서 제외된 사람들은 그 시스템의 혜택으로부터 소외될 수밖에 없다.14\n우선, 빅데이터 기술은 사회적 약자를 데이터 수집에서 배제시킴으로써 차별을 고착화한다. ’인간은 곧 남성’이라는 위계화된 이분법적 사고는, 여성의 데이터를 배제하는 데이터 편향성의 논리로 작동한다. 일상과 직장, 설계, 의료, 공공생활, 재난 등 많은 분야에서 여성에 관한 정보와 지식이 무의식적으로 배제됨을 실증적으로 제시한 페레즈(Perez, 2019/2020)는 데이터 수집의 편향성과 여성의 비가시성이 얼마나 만연해있는가를 보여준다. 이러한 ’젠더 데이터 공백’은 여성의 건강과 안전을 위협하고, 여성을 차별하며, 존재 자체를 투명하게 만든다. 데이터가 곧 존재의 증명인 포스트휴먼 사회에서 여성은 존재하지 않으며, 여성을 위한 제도나 사회설계는 아예 고려조차 되지 않는다. 빅데이터 기술에서 배제되는 사람들은 대부분 여성, 빈민, 노숙자, 아동 같은 사회적 약자들이다.\n둘째, AI 알고리즘은 불평등을 자동화한다는 점이다. 기술숭배론은 빅데이터 기술과 알고리즘이 적절한 공공자원을 제공함으로써 사회적 불평등을 해소할 것이라고 기대하지만, 실제 연구는 이러한 기대가 틀렸음을 보여준다. 공공서비스에 도입된 자동화된 의사결정 시스템을 분석한 유뱅크스(Eubanks, 2018)는, 예측 알고리즘, 위험모형, 자동화된 복지수급자격 판정시스템 같은 정교한 데이터 기반 기술이 공공서비스에 도입되면서 불평등이 자동화되었다고 비판한다. 디지털 빈곤관리 도구인 자동화된 시스템이 가난을 지속시키고 공공자원의 요청을 단념시킴으로써 자동화된 불평등을 고착한다는 것이다. 이런 식으로 알고리즘과 빅데이터, 위험모형은 가난한 사람들을 분석하고 감시하고 처벌하는 ’디지털 구빈원’으로 작동한다. 더구나 데이터 수집과 분석 알고리즘이 투명하지 않으면 자동화 알고리즘에 저항하거나 수정하는 것이 불가능하다. 결국 자동화된 알고리즘은 사회적 불평등을 구조화한다.\n셋째, 데이터-네트워크-인공지능 기술은 사회성을 변형시킨다는 점이다. 코로나19로 생계에 타격을 입은 소상공인들은 자신의 피해를 입증하는 데이터를 인정받아야만 공적 지원을 받을 수 있다. 문제는 이런 식의 재난지원시스템이 사회적 유대 형성에는 아무런 도움이 되지 않는다는 점이다. 공적 지원의 자동화는 자신의 생존에만 집중하게 함으로써 타인의 고통과 사회적 불평등 문제를 외면하게 만든다. 본래 공적 부조를 돕는 조력자였던 공무원 역시 사회적 지지 네트워크로서보다는 소상공인이나 해고노동자의 데이터만을 평가하는 심판관이 된다. 정서적 유대는 업무의 효율성을 떨어뜨리는 비합리적 요소일 뿐이다. 이처럼 자동화된 행정시스템에서는 사회적 약자의 고통과 절망에 공감하며 유대를 형성할 여지가 거의 없다. 결국 자동화시스템은 사회적 유대를 약화시키고 사회성을 변형시키는 기제로 작동한다.\n이와 같이 편향된 데이터 테크놀로지와 자동화된 의사결정시스템은 차별과 불평등을 양산하고, 사회적 유대를 약화시킨다. 이러한 지능화테크놀로지에 대항하는 것은 데이터-네트워크-인공지능 기술생태계를 재배치할 수 있는 공-산의 윤리를 실천하는 것이다.\n우선, 데이터 액티비즘(Data Activism)은 데이터를 활용하여 사회문제를 해결하고 민주주의를 진작시키려는 일련의 사회적 실천이다.15 하나의 예로, 마스크 재고 알림 앱은 다양한 행위주체들이 연결짓기를 통해 코로나19 확산 초기 마스크 대란을 해결하는 데 기여하였다. 약국이 마스크 재고를 실시간으로 입력하면 질병청이 이 데이터를 API화하고, 이를 기반으로 협동조합, 시민단체, 개발자, 서비스 운영자로 구성된 ’코로나19 공공데이터 공동대응팀’이 앱을 개발하면 통신사가 서버를 제공하고 포털이 서비스를 제공하는 방식으로 프로젝트를 진행한 것이다(권오현, 2021). 이 프로젝트는, 상이한 배경을 가진 주체들이 공동의 프로젝트라는 접경지대에서 서로의 요구에 응답하고 책임지며, 공공의 이익을 위해 연대한 사회적 실천이라고 볼 수 있다.\n보다 근본적인 실천방식은 자동화를 변화시키는 비자동화 능력 즉 일을 발명하는 것이다. 디지털 테크놀로지를 파르마콘으로 보는 스티글레르(Stiegler & Kyrou, 2015/2018)는 자동화가 소비주의와 신자본주의의 무관심 경제를 만들어내는 것을 경계하면서, 자동화가 공공에 기여하는 약이 되도록 전환해야 한다고 주장한다. 스티글레르는 그 가능성을 모든 사람에게 개방되어 있는 프리웨어나 비자동화에 기초한 해석 웹 시스템에서 발견한다. 노트공유, 해석적 소셜네트워크, 온라인 토론지원 시스템 등의 플랫폼에서 아이디어나 실행방식, 모든 종류의 주장들을 대조하는 알고리즘을 체계적으로 실행하는 프로젝트이다(73~76쪽). 이 프로젝트의 목적은 무관심 경제로 인한 정신의 자동화를 변화시킬 수 있는 일의 발명, 즉 앎-살 줄-앎, 함께 할 줄-앎, 생각할 줄-앎을 키우는 것이다. 데이터 액티비즘이나 스티글레르의 일의 발명은 기술혐오와 기술숭배라는 이분법을 뛰어넘는다. 데이터-네트워크-인공지능 생태계가 이미 포스트휴먼의 삶의 조건이 된 현실에서 이러한 이분법은 대안이 되기 어렵다. 지금 우리에게 필요한 포스트휴먼의 윤리적 실천은 지능화테크놀로지를 사회적으로 재구성하는 것이다. 그것은 어떠한 목표와 가치를 설정하느냐, 얼마나 다양한 사회문화적 배경을 가진 사람들이 관여하느냐, 누구의 관점에서 데이터에 접근하느냐, 어떻게 알고리즘을 설계하느냐에 따라 달라질 수 있으며, 여기에 불평등과 차별을 해소하기 위한 공-산의 윤리가 개입되어야 한다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>포스트휴머니즘과 윤리적 인공지능</span>"
    ]
  },
  {
    "objectID": "post.html#우리-모두는-트러블과-함께-하는-반려종",
    "href": "post.html#우리-모두는-트러블과-함께-하는-반려종",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.5 우리 모두는 트러블과 함께 하는 반려종",
    "text": "3.5 우리 모두는 트러블과 함께 하는 반려종\n영화 &lt;노마드랜드&gt;16는 2008년 세계금융위기로 가족과 직장을 잃고 밴을 집으로 삼아 떠도는 노마드의 삶을 다룬다. 이 영화는 지금까지 논의한 포스트휴먼의 윤리적 실천을 구체적으로 형상화한다. 주인공 펀17은 이동 지역의 식당이나 농장, 아마존 물류센터 등에서 일하며 노마드로서 살아간다. 펀이 정주하며 일했던 석고보드회사는 전형적인 제조업 기업인 반면, 임시로 일하는 아마존은 디지털 자본주의의 최전선에 서있는 플랫폼 기업이다. 이것은 산업구조의 변화를 보여주는 것이기도 하지만, 플랫폼 자본주의의 불안정하고 일시적인 노동형태를 드러내며 플랫폼 기업과 노동자 사이의 갈등을 드러낸다. 실제 미국에서 노마드는 금융위기로 집을 압류당하고 해고로 인한 경제위기와 가족해체 등으로 정주의 삶을 포기할 수밖에 없는 사람들이 대부분이다.\n그럼에도 노마드로서의 삶은 고립된 삶이 아니라 계속해서 연결되는 삶이다. 캠핑장18이라는 물리적 공간은 노마드로서의 삶을 지탱하는 베이스캠프가 되고, 사람들은 이곳을 거점으로 모이고 흩어지며 새로운 사람들을 계속해서 만난다. 이것이 가능한 것은 노마드들간의 연결지점 혹은 통로 역할을 하는 밥 웰스의 웹사이트와 블로그, 그리고 각자의 스마트폰이 있기 때문이다. 미디어테크놀로지와 결합한 노마드들은 유목적 주체로서 낯선 사람들과 위계화되지 않은 횡단적 연결짓기를 수행한다. 때때로 트러블을 일으키지만 자기돌봄뿐만 아니라 함께-돌봄을 실천하며 서로의 이야기를 듣고 서로에게 반응하며 사회적 유대를 형성한다. 이곳에서의 교류는 길을 떠나는 사람들에게 노마드로서의 존엄성을 지키게 해주는 정서적 자원을 제공한다. 그래서 포스트휴먼인 노마드들은 고독하지만 외롭지 않은 삶, ’따로 또 같이’라는 유목적 삶을 살면서 자연과 사람을 친척으로 만드는 것이다.\n우리사회에도 공존과 공생의 삶을 만들어가는 다양한 실험들이 존재한다. 청년주거 안정과 자치공동체를 활성화하기 위한 사회주택협동조합19은, 집을 매개로 한 연결짓기와 일상적 유대를 통해 청년들의 사회성을 강화하려고 시도한다. 이 시도는 낯선 사람들과 함께 사는 경험을 공유하는 공간으로 집을 재구성한다. 가족으로 구성된 공간에서, 가족 아닌 사람들과 함께 사는 공간으로 집에 대한 인식을 확장시키고, 이를 통해 혈연에 기반을 둔 가족을 벗어나 상호의존하며 공생하는 친척을 만들자는 것이다. 지자체가 빈집 정보를 제공하는 아키야뱅크(빈집은행)를 운영하고, 빈집을 사들여 전 세대가 어울려 살 수 있는 공공주택을 마련하거나 지역커뮤니티로 재생하는 일본의 사례 역시 함께 살아가기를 실천하는 방안이라 할 수 있다(남지현, 2018).\n소셜 다이닝(social dinning)도 식사라고 하는 일상적 의례를 통해 사회적 유대를 형성하려는 시도이다. ’식구’라는 말처럼 일반적으로 식사는 친밀한 관계 속에서 이야기하기와 듣기를 수반하여 이루어지는데, 소셜 다이닝은 식구의 의미를 낯선 사람들에게로 확대한다. 다른 처지에 놓인 사람들이 식탁에 앉아 각자의 이야기를 나누는 행위는 친척을 만드는 실천이기도 하다. 그러나 온라인 플랫폼 기반 소셜 다이닝은 대부분 성공하지 못했는데, 식사조차 ’경험의 공유’를 판매하는 수익 창출 도구로 활용하기 때문이다.20 소셜 다이닝의 사례는 자본의 개입이 사회성을 훼손시킬 수 있음을 방증한다.\n인간의 가장 기본적 생존조건인 주거와 식사를 통해 사회적 유대를 강화하려는 이러한 시도들은 플랫폼이라는 온라인 접속과 대면 접촉을 통한 함께-하기가 유기적으로 결합될 때 실현될 수 있다. 비대면 삶의 방식이 강화될수록 오히려 대면 접촉을 확장하기 위한 노력을 더 많이 기울여야 한다. 사회적 유대를 돈독히 하기 위해 ’쓸모없는 함께-하기’라는 순수한 사회성의 형식을 만들어내는 것도 하나의 방법이다(Maffesoli, 2000/2017, 154~159쪽). 쓸모없는 함께-하기는 일상적 삶에서 아주 소소한 경험들을 만들어내면서 수평적이고 감성적인 연대를 형성하는 것이다. 코로나19의 사회적 거리두기로 대면 접촉이 줄어드는 상황에서 서로의 안부를 묻고 곁을 내어주는 이러한 사회적 실천이 어느 때보다 절실히 필요하다. 팬데믹으로 팽배해진 인종주의와 동성애 혐오 등 사람들의 편견도 사회적 접촉을 통해 해소될 수 있다. 다른 생각을 하는 사람들과 더 많이 접촉하고 더 가까이 있을수록 편견이 줄어들기 때문이다(Berbner, 2019/2021).\n무엇보다 중요한 것은 포스트휴먼이 사회성의 역능을 키울 수 있는 공-산의 생태계를 만드는 것이다. 데이터-네트워크-인공지능의 기술적 생태계가 자본과 차별의 논리로 작동하지 않도록 이분법적 경계들을 지우고 이질적인 것들과 접촉하면서 이들과 친척이 되는 새로운 생태계를 만들어야 한다. 새로운 생태계가 공생과 환대의 생태계가 되기 위해서는 서로에게 응답하고 서로의 이야기에 귀 기울이며 서로를 책임지는 무수한 실천들을 통해 상호의존적 연결관계를 회복하는 것이 필요하다. 더불어 기술적 생태계의 자동화된 불평등으로부터 새로운 생태계를 지켜내기 위한 저항도 요구된다. 이러한 포스트휴먼의 윤리적 실천들은 사회적 연대와 유대의 탈구현상을 절합하고 사회성의 역능을 키우며, 데이터-네트워크-인공지능 기술생태계를 공-산의 생태계로 재배치하는 데 기여할 것이다.\n이 과정에서 포스트휴먼인 우리는 대문자 인간(Humanity)이 아닌 인류(humankind)로서 기술, 동물, 바이러스 등 무수한 비인간 존재들과 상호의존적인 공생적 실재임을 깨닫고 이들과 함께 자본주의 논리에 저항하는 생태학적 인식을 가져야 한다(Morton, 2017/2021). 이것이 진정 포스트휴머니즘의 윤리인 공-산을 실천하는 포스트휴먼의 자세일 것이다.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>포스트휴머니즘과 윤리적 인공지능</span>"
    ]
  },
  {
    "objectID": "post.html#더-생각해볼-문제",
    "href": "post.html#더-생각해볼-문제",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.6 더 생각해볼 문제",
    "text": "3.6 더 생각해볼 문제\n\n인공지능을 공포나 숭배의 대상이 아니라 다른 존재로 볼 수 있을까? 그 존재와 어떤 관계를 형성할 수 있을까? 인공지능과 더불어 산다는 것은 어떤 의미이고 어떤 경험일까?\n우리의 일상을 지배하는 배달플랫폼 노동 같은 플랫폼 자본주의 문제를 포스트휴머니즘의 관점에서 접근한다면 어떤 이야기들을 만들어내고 어떤 해결책을 찾아낼 수 있을까?\n기후재앙으로 일컬어지는 인류세 시대에 포스트휴먼으로서 어떤 태도와 윤리적 실천이 가능할까?",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>포스트휴머니즘과 윤리적 인공지능</span>"
    ]
  },
  {
    "objectID": "post.html#더-읽을거리",
    "href": "post.html#더-읽을거리",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.7 더 읽을거리",
    "text": "3.7 더 읽을거리\n신상규 외 (2020). &lt;포스트휴먼이 몰려온다: AI 시대, 다시 인간의 길을 여는 키워드 8&gt;. 파주: 아카넷.\n김초엽·김원영 (2021). &lt;사이보그가 되다&gt;. 파주: 사계절.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>포스트휴머니즘과 윤리적 인공지능</span>"
    ]
  },
  {
    "objectID": "post.html#참고문헌",
    "href": "post.html#참고문헌",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "3.8 참고문헌",
    "text": "3.8 참고문헌\n권오현 (2021). 디지털시민주권을 확립하는 시민개발자(시빅해커)들. 《한국언론정보학회 봄철 정기학술대회 발표문》. 2021. 5. 29.\n김상호 (2009). 확장된 몸, 스며든 기술: 맥루한 명제에 관한 현상학적 해석. 「언론과학연구」, 제9권 2호, 167~206.\n김선희 (2005). 사이보그와 개인동일성의 문제: 컴퓨터와의 융합을 통하여 우리는 영생할 수 있는가? 「철학」, 제85집, 171~191.\n김재희 (2014). 우리는 어떻게 포스트휴먼 주체가 될 수 있는가? &lt;철학연구&gt;, 제106집, 215-242.\n김재희 (2017). &lt;시몽동의 기술철학: 포스트휴먼 사회를 위한 청사진&gt;. 파주: 아카넷.\n남지현 (2018). 일본에서는 빈집을 어떻게 활용하고 있나? 《세계와 도시》, 22호, 12~25.\n마정미 (2008). 포스트휴먼과 탈근대적 주체에 관한 연구. 「인문콘텐츠」 제13호, 193~212.\n박선희 (2022). 인간-미디어 공생체의 사회: 포스트휴먼의 사회성과 윤리적 실천. &lt;언론과 사회&gt;, 29권 4호. 5~49.\n브루노 라투르 외 (2010). &lt;인간·사물·동맹: 행위자네트워크 이론과 테크노사이언스&gt;. 서울: 이음\n신상규 (2008). 「푸른 요정을 찾아서: 인공지능과 미래인간의 조건」. 서울: 프로네시스.\n이수안 (2017). 테크노페미니즘으로 본 몸의 물질성과 감각의 확장: 오를랑(Orlan)의 테크노바디를 중심으로. &lt;한국여성학&gt;, 제33권 1호, 77~105.\n임석원 (2013). 비판적 포스트휴머니즘의 기획: 배타적인 인간중심주의 극복. 이화인문과학원 편 &lt;인간과 포스트휴머니즘&gt; (61~82쪽). 서울: 이화여자대학교출판부.\n전혜은 (2010). &lt;섹스화된 몸: 엘리자베스 그로츠와 주디스 버틀러의 육체적 페미니즘&gt;. 서울: 새물결.\n최유미 (2020). 《해러웨이, 공-산의 사유》. 서울: 도서출판b.\nAttali, J. (2003). L’Homme Nomade. 이효숙 (역) (2005년). 《호모 노마드, 유목하는 인간》. 서울: 웅진지식하우스.\nBerbner, B. (2019). Geschichten gegen den Hass. 이승희 (역) (2021). 《혐오없는 삶: 나와 다른 사람과 친구가 될 수 있을까?》. 서울: 판미동.\nBraidotti, R. (2013). The posthuman. 이경란 (역) (2015). 《포스트휴먼》. 파주: 아카넷.\nByron, P. (2021). Friendship and digital cultures of care. London: Routledge.\nClark, A. (2003). Natural-born cyborgs: Minds, technologies, and the future of human intelligence. 신상규 옮김 (2015). &lt;내추럴-본 사이보그: 마음, 기술, 그리고 인간지능의 미래&gt;. 파주: 아카넷.\nCoeckelbergh, M. (2020). AI ethics. 신상규・석기용 옮김 (2023). . 파주: 아카넷.\nDeleuze, G., & Guattari, F. (1980). Mille plateaux: Capitalisme et schizophrenie. 김재인 (역) (2001). 《천 개의 고원: 자본주의와 분열증 2》. 서울: 새물결.\nEubanks, V. (2018). Automating inequality: How high-tech tools profile, police, and punish the poor. 김영선 (역) (2018) 《자동화된 불평등: 첨단 기술은 어떻게 가난한 사람들을 분석하고, 감시하고, 처벌하는가》. 서울: 북트리거.\nFerrando, F. (2020). Philosophical posthumanism. 이지선 옮김 (2021). &lt;철학적 포스트휴머니즘: 포스트휴먼 시대를 이해하는 237개의 질문들&gt;. 아카넷.\nHaraway, D. (1991). Simians, cyborgs and women: The reinvention of nature. 황희선·임옥희 옮김 (2023). &lt;영장류, 사이보그 그리고 여자: 자연의 재발명&gt;. 파주: 아르테.\nHaraway, D. (2016). Manifestly Haraway. 황희선 옮김 (2019). &lt;해러웨이 선언문&gt;. 책세상\nHaraway, D. (2016). Staying with the trouble: Making kin in the Chthulucene. 최유미 (역) (2021). 《트러블과 함께하기: 자식이 아니라 친척을 만들자》. 서울: 마농지.\nHayles, N. K. (1993). The seductions of cyberspace. In D. Trend. (Ed.), Reading Digital Culture (pp. 305~321). MN: Univ. of Minnesota Press.\nHayles, N. K. (1996). Embodied virtuality: Or how to put bodies back into the picture. In M. A. Moser (Ed.), Immersed in Technology: Art and Virtual Environments (pp. 1~28). Cambridge, MA: MIT Press.\nHayles, N. K. (1997). Voices out of bodies, bodies out of voices: Audiotape and the production of subjectivity. A. Morris. (Ed.). Sound state: Innovative poetics and acoustical technologies(pp. 74-96). Chapel Hill, NC: The University of North Carolina Press.\nHayles, N. K. (1999). How we became posthuman: Virtual bodies in cybernetics, literature, and informatics. 허진 (역) (2013). 《우리는 어떻게 포스트휴먼이 되었는가》. 파주: 플래닛.\nHerbrechter, S. (2009). Posthumanismus: Eine kritische Einführung. 김연순·김응준 옮김 (2012). &lt;포스트휴머니즘: 인간 이후의 인간에 관한 문화철학적 담론&gt;. 서울: 성균관대학교 출판부.\nKimmerer, R. W. (2013). Braiding sweetgrass. 노승영 (역) (2020). 《향모를 땋으며: 토박이 지혜와 과학 그리고 식물이 가르쳐준 것들》. 서울: 에이도스.\nLum, C. (Ed.) (2006). Perspectives on culture, technology and communication: The media ecology tradition. 이동후 (역) (2008). 《미디어생태학사상: 문화, 기술 그리고 커뮤니케이션》. 서울: 한나래.\nLynch, M. P. (2016). The Internet of us: Knowing more and understanding less in the age of big data. New York, NY: Liveright Publishing. 이충호 옮김. &lt;인간 인터넷: 사물 인터넷을 넘어 인간 인터넷의 시대로&gt;. 서울: 사회평론.\nMaffesoli, M. (2000). Le Temps des tribus. 박정호·신지은 (역) (2017). 《부족의 시대: 포스트모던 사회에서 개인주의의 쇠퇴》. 서울: 문학동네.\nMcLuhan, M. (1964). Understanding media: The extensions of man (Critical ed.) (2003). 김상호 (역) (2011). 《미디어의 이해: 인간의 확장》 (비평판). 서울: 커뮤니케이션북스.\nMiah, A. (2008). A critical history of posthumanism. In B. Gordijn & R. Chadwick. (Eds.), Medical enhancements and Posthumanity (pp. 71-94). New York, NY: Routledge.\nMorse, M. (1998). Virtualities: Television, Media Art, and Cyberculture. Bloomington: Indiana University Press.\nMorton, T. (2017). Humankind: Solidarity with non-human people. 김용규 (역) (2021). 《인류: 비인간적 존재들과의 연대》. 부산: 부산대학교출판문화원.\nO’Neil, C. (2016). Weapons of math destruction: How big data increases inequality and threatens democracy. 김정혜 (역) (2017). 《대량살상수학무기: 어떻게 빅데이터는 불평등을 확산하고 민주주의를 위협하는가》. 서울: 흐름출판.\nPenny, S. (1994). Virtual reality as the completion of the enlightenment project. In G. Bender & T. Druckrey (Eds.). Cultures on the Brink: Ideologies of Technology (pp. 231~248). Seattle: Bay Press.\nPerez, C. (2019). Invisible women: Data bias in a world designed for men. 황가한 (역) (2020). 《보이지 않는 여자들: 편향된 데이터는 어떻게 세계의 절반을 지우는가》. 서울: 웅진지식하우스.\nPeters, J. D. (2015). The marvelous clouds: Toward a philosophy of elemental media. 이희은 (역) (2018). 《자연과 미디어: 고래에서 클라우드까지, 원소 미디어의 철학을 향해》. 서울: 컬처룩.\nSennett, R. (2012). Together: The rituals, pleasures, and politics of cooperation. 김병화 (역) (2013). 《투게더: 다른 사람들과 함께 살아가기》. 서울: 현암사.\nSimondon, G. (1958). Du mode d’existence des objets techniques. Éditions Aubier. 김재희 (역) (2011). 《기술적 대상들의 존재 양식에 대하여》. 서울: 그린비.\nSrnicek, N. (2017). Platform capitalism. Cambridge. 심성보 (역) (2020). 《플랫폼 자본주의》. 서울: 킹콩북.\nStiegler, B., & Kyrou, A. (2015). L’emploi est mort, vive le travail!. 권오룡 (역) (2018). 《고용은 끝났다, 일이여 오라! : 베르나르 스티글레르와의 대담》. 서울: 문학과지성사.\nThe Care Collective (2020). The Care manifesto. 정소영 (역) (2021). 《돌봄 선언: 상호의존의 정치학》. 서울: 니케북스.\nTsing, A. L. (2015). The mushroom at the end of the world: On the possibility of life in capitalist ruins. 노고운 옮김 (2023). 《세계 끝의 버섯: 자본주의의 폐허에서 삶의 가능성에 대하여》. 서울: 현실문화.\nWarwick, K. (2002). I, Cyborg. 정은영 (역) (2004). 《나는 왜 사이보그가 되었는가》, 서울: 김영사.",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>포스트휴머니즘과 윤리적 인공지능</span>"
    ]
  },
  {
    "objectID": "post.html#footnotes",
    "href": "post.html#footnotes",
    "title": "3  포스트휴머니즘과 윤리적 인공지능",
    "section": "",
    "text": "트랜스휴머니스트인 케빈 워릭(Warwick, 2002/2004)은 자신과 부인의 왼쪽 팔 피부에 전자 칩을 이식하여 자신의 위치신호를 컴퓨터로 전송하고, 부인과 전자신호로 교감하는 실험을 진행하였다.↩︎\n워릭이 사이보그가 되는 자세한 과정에 대해서는 Warwick(2002/2004) 참조.↩︎\n김선희(2005)는 신체동일론에 입각하여 사이보그를 분석하고 있는데, 고전적 사이보그의 경우 사이보그 신체가 시공적으로 물리적 공간에서 지속된다는 점에서 신체동일성이 보장되는 반면, 탈고전적 사이보그의 경우 개인이 유기적 네트의 일부가 되거나 신체의 물리적 경계가 사라짐으로써 신체동일성의 문제도 해소되어버린다고 주장한다.↩︎\n박선희(2022)의 일부내용을 재인용함.↩︎\n노마디즘이 국경을 넘어 이윤을 추구하는 글로벌기업을 옹호하고, 노동유연화에 내몰려 떠돌 수밖에 없는 노동조건을 은폐하며, 경계를 넘나들 수 없는 난민이나 이주노동자에 대해 무관심한 것을 정당화한다고 바판받기도 하지만, 이 논문에서 노마디즘은 근대의 고정된 위계구조와 그로 인한 사회적 불평등에 대해 문제제기하는 하나의 은유 혹은 사유를 의미한다.↩︎\nhttps://www.globalcitizen.org/en/live/↩︎\nhttps://www.youtube.com/watch?v=0ly61HpQ3mU&t=4s↩︎\nhttps://www.window-swap.com/Window↩︎\n돌봄은 직접 누군가에게 육체적・심리적 도움을 주는 것만을 의미하는 것이 아니라 사회적 역량이자 삶에 필요한 모든 것을 보살피는 사회적 활동이며, 우리의 상호의존성을 인지하고 포용하는 것을 의미한다(The Care Collective, 2020, 17쪽).↩︎\nhttp://articulab.hcii.cs.cmu.edu/projects/sara/↩︎\n실뜨기 놀이를 경험한 사람이라면, 놀이를 할 때마다 매번 새로운 패턴이 만들어지고, 실뜨기를 계속하기 위해 서로 노력해야 하며, 서로의 노력 여하에 따라 놀이가 지속될 수도 중단될 수도 있음을 알 것이다. 상대와 놀고 싶은 마음이 없을 때, 또는 성실한 응대를 하고 싶지 않을 때 두세 번만에 놀이를 중단시킬 수도 있다.↩︎\n향모는 향기롭고 성스러운 풀로, 부족사람들에게는 ’어머니 대지님의 머리카락’이다. 세 갈래로 땋아서 선물을 주거나, 태워서 제의적 검댕을 만든 후 몸과 영혼을 치유하는 데 사용한다.↩︎\nhttps://www.youtube.com/c/facespeakawake↩︎\n데이터 수집・분석・활용은 프라이버시 침해와 감시를 동반한다. 그러나 아이러니하게도 빅데이터 기술의 혜택을 받기 위해서는 프라이버시 침해와 감시를 감수하고 자신의 정보를 모두 내놓아야 한다. 근대국민국가의 통치성인 조직화와 감시가 샴쌍둥이라는 기든스(Giddens, 1985)의 성찰이 자동화사회에서도 적용될 수 있음을 보여준다.↩︎\n데이터 액티비즘은, 오픈소스운동, 시빅해킹(civic hacking) 등 여러 형태로 이루어지며, 그 활동범위도 감시활동과 환경보호, 건강, 구호활동 등 매우 다양하다.↩︎\n이 영화는 언론인 제시카 브루더(Jessica Bruder)가 밴과 트레일러를 타고 다니며 생활하는 사람들을 3년간 취재한 논픽션 《노마드랜드: 21세기 미국에서 살아남기》(Nomadland: Surviving America in the Twenty-First Century, 2017)를 원작으로 만들어졌다.↩︎\n펀(Fern)은 땅에 심는 씨앗이 아니라 흩뿌려지는 포자로 생존하는 양치식물을 상징하는데, 특정 정착지 없이 떠도는 삶을 사는 노마드를 의미한다. 특정한 목적없이 어디로든 뻗어나갈 수 있다는 점에서 펀이라는 양치식물의 이미지는 들뢰즈와 가타리의 리좀과 닮아있다.↩︎\n아이러니하게도 이 캠핑장은 아마존과 협력관계인데, 이는 플랫폼이 삶의 터전이 될 수도 자본주의의 전유공간이 될 수도 있음을 암시한다.↩︎\n민달팽이주택협동조합 https://minsnailcoop.com/↩︎\n소셜다이닝은 플랫폼 스타트업으로 시작되었으나 이벤트 중심의 운영과 수익성 문제로 오래 지속되지 못했다. 금천구청이 지원하는 대대식당과 소소식당 같은 지역기반 소셜다이닝은 유지되었으나, 현재는 코로나19로 중단된 상태이다.↩︎",
    "crumbs": [
      "1부: 인공지능 윤리의 철학적 기초",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>포스트휴머니즘과 윤리적 인공지능</span>"
    ]
  },
  {
    "objectID": "tai.html",
    "href": "tai.html",
    "title": "4  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "",
    "text": "4.1 인공지능 신뢰성 문제\n인공지능(artificial intelligence, AI) 기술이 빠르게 발전하면서 사회 전반에 AI 전환(AI transformation, AIX)에 대한 기대가 커지고 있다. 그러나 다른 한편으로는 AI가 초래할지도 모를 부정적 영향에 대한 우려도 적지 않다.\nAI는 알고리듬에 의한 모델링 편향(modeling bias) 문제, 학습데이터에 의한 교육 편향(bias in training) 문제, AI를 악용하는 사람이나 조직의 문제 등으로 공정성 문제가 발생할 수 있다(손영화, 2023). 그 결과 AI가 편견을 학습해 범죄 예측이나 채용, 복지 제공 등에서 성이나 인종, 국적 차별을 하는 사례가 보고됐다. 학습데이터에 포함된 개인정보를 유출한다거나, 자살이나 위험한 놀이를 권유하거나, 체스 게임 중에 인간을 위협하고 실제 상해를 입히는 사례도 있었다 (한국정보통신기술협회, 2023; 손영화, 2023). 챗GPT의 환각(hallucination), 생성 AI를 활용한 가짜 뉴스, AI 생성물을 논문이나 시험, 과제물, 창작물 등에 비공개로 사용하는 것 등도 AI의 신뢰도를 훼손한다. 전쟁에 사용되는 킬러 로봇(killer robots)에 대한 우려도 커지고 있다(Beutel, Geerits, & Kielstein, 2023; Krishnan, 2016).\n게다가 이러한 문제를 개선하고자 하더라도 AI의 설명가능성 문제가 제기된다. 많게는 수조 개의 매개변수를 학습하는 딥러닝 방식이 일반화되면서 AI가 왜 그런 결과를 내놓았는지를 이해하기 어렵기 때문이다(고학수 등, 2021). 때문에 2024년 1월 현재, 딥러닝의 대부인 요슈아 벤지오(Yoshua Bengio) 캐나다 몬트리올대학교 교수와 미래학자인 유발 하라리, 일론 머스크 테슬라 사장, 에마드 모스타크 스테빌리티AI 사장 등 3만3000명이 넘는 인물들이 GPT-4를 넘어서는 AI 개발을 6개월 간 중단하자는 서한에 서명하기도 했다(Futrue of life, 2023.3). AI의 성능 개선 속도가 너무 빠르기 때문에 예기치 못할 AI의 해악에 대응할 시간을 벌자는 취지다.\nAI가 야기할지도 모르는 정치적, 경제적, 사회적, 문화적, 국제적 측면의 전방위적 문제에 대한 논의는 AI 윤리, AI 공정성(fairness), 설명가능한 인공지능(eXplainable AI, XAI), 책임 있는 AI(responsible AI) 등의 논의를 거쳐 일단 신뢰할 수 있는 인공지능(trustworthy AI, TAI)이라는 개념 아래 종합되는 추세다.\n이 장에서는 TAI의 개념과 전개, 그리고 저널리즘 분야에 적용 가능성을 살펴보도록 한다. 내용을 간략하게 살펴보면 다음과 같다. 우선 2절에서는 학제적인 신뢰 개념을 검토하고 이를 바탕으로 TAI를 정의한다. 우선 신뢰의 하위 개념으로는 신뢰성과 신뢰도를 살펴본다. 다음으로 신뢰의 두 유형으로 인간 신뢰와 기계 신뢰를 논의한다. 이어 TAI의 개념을 신뢰성과 신뢰도, 인간 신뢰와 기계 신뢰 측면에서 개념화한다. 3절에서는 TAI의 논의가 어떻게 전개됐는지 유럽연합 집행위원회(European Commission, EC), 경제협력개발기구 (Organisation for Economic Co-operation and Development, OECD), 그리고 국내에서 진행된 TAI 논의를 중심으로 살펴본다. TAI의 논의는 윤리적인 측면에서 기술적인 측면으로 구체화됐다. 4절에서는 TAI의 구체적인 영역(domain)으로서 저널리즘 분야를 놓고, TAI가 저널리즘 분야의 AI, 즉 저널리즘 AI에 어떻게 적용될 수 있는지 살펴본다. 저널리즘 AI의 신뢰성은 AI로서의 신뢰성 외에도 언론인과 언론사의 신뢰성을 제고함으로써 달성된다. 이를 위해서는 언론인이 저널리즘 AI의 기술적 루프(loop) 속에 참여해야 한다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인</span>"
    ]
  },
  {
    "objectID": "tai.html#신뢰할-수-있는-인공지능의-개념",
    "href": "tai.html#신뢰할-수-있는-인공지능의-개념",
    "title": "4  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "4.2 신뢰할 수 있는 인공지능의 개념",
    "text": "4.2 신뢰할 수 있는 인공지능의 개념\n\n4.2.1 신뢰의 정의\nTAI를 논의하기에 앞서 신뢰의 개념을 살펴보자. 우선 신뢰는 신뢰자(trustor)가 신뢰 대상(trustee)을 믿는 것이다. 그러나 신뢰는 단순히 믿는 것 이상을 의미한다. 신뢰에 대한 심리학, 사회학, 경제학 분야의 연구들을 검토한 바에 따르면, 신뢰는 타인의 의도(intention)나 행동(behavior)에 대한 긍정적인 기대(positive expectation)에 근거하여 취약성(vulnerability)을 수용하려는 의지에 따른 심리적 상태로 정의할 수 있다(Rousseau et al, 1998).\n무엇보다 신뢰는 위험(risk)이 존재한다. 이러한 면에서 신뢰는 확신(confidence)과 다르다. 확신에는 위험 감수가 없다. 그러나 신뢰에는 위험 감수가 존재한다(Luhmann, 2000). 위험은 신뢰 대상의 의도와 미래의 행동이 불확실성(uncertainty)을 갖기 때문에 발생한다. 즉 신뢰 대상은 반사회적인 의도와 행동을 할 수 있다. 이러한 위험이 있는 상태에서 신뢰자는 신뢰 대상에 대한 긍정적 기대를 바탕으로 자신의 취약성을 받아들일 때 신뢰자는 신뢰 대상을 신뢰한다고 할 수 있다. 위험은 신뢰의 기회를 마련한다. 신뢰는 위험 감수에 도움을 준다. 위험 감수는 신뢰를 강화한다(Coleman, 1990).\n신뢰는 상호적이다. 신뢰를 위해서는 신뢰자가 신뢰 대상을 신뢰할 뿐만 아니라, 신뢰 대상이 신뢰자를 신뢰해야 한다. 즉 신뢰 관계에서 신뢰자는 신뢰 대상으로, 신뢰 대상은 신뢰자로 바뀔 수 있다.\n신뢰의 기능으로는 거래 비용(transaction cost)을 감소시키고 조직 통합을 강화한다는 점을 들 수 있다(Gambetta, 1988; Meyerson et al., 1996; Shapiro et al., 1992). 사실 완전한 확신을 얻기란 쉽지 않다. 때문에 신뢰는 불확실성 상황에서 협력의 지름길 역할을 한다.\n신뢰는 제도적으로 공급되어야 한다. 신뢰는 오랜 시간 동안 형성되고, 안정과, 해체, 그리고 재부상하는 과정을 거친다(Fukuyama, 1995; Miles et al., 1995). 신뢰는 훼손되기 쉬우며 때문에 과소 공급되는 공공재와 같다. 신뢰는 법적 통제에 의존하지는 않는다. 신뢰를 지키지 않았을 때 처벌하는 식의 법적 통제는 오히려 신뢰를 훼손시킬 수도 있다(Sitkin & Bies, 1993). 그러나 신뢰를 뒷받침할만한 제도적 장치는 신뢰 증진에 도움이 된다(Nooteboom, Berger, & Noorderhaven, 1997).\n\n\n4.2.2 신뢰성과 신뢰도\n신뢰 아래의 긍정적 기대는 다양하다. 우선 상대가 자신에게 이로운 행동을 할 것이라는 호의(benevolence)에 대한 기대가 있다. 신뢰 대상이 사회적, 도덕적 원칙을 지킬 것으로 믿는 진실성(integrity)에 대한 기대도 있다. 신뢰 대상이 기대하는 바를 실행할 수 있으리라는 능력(competence)에 대한 기대도 포함된다(김길수, 2020; Mayer et al., 1995). 호의를 갖고 있지만 진실성이 없다면 그것은 정파성을 띄게 된다. 상대방이 호의와 진실성을 갖고 있다고 하더라도 이를 실현할 능력이 없다면 신뢰할만한 대상이 되지 않는다.\n신뢰와 관련된 개념으로 신뢰도(credibility)와 신뢰성(trustworthiness)이라는 용어가 사용된다. 이러한 용어들이 혼용되는 경향이 있지만, 대체로 신뢰도는 신뢰자의 속성, 신뢰성은 신뢰 대상의 속성을 일컫는 말로 구분할 필요가 있다. 신뢰도는 신뢰자가 신뢰 대상을 얼마나 신뢰하는지를 나타내는 심리적 수준을 의미한다. 이는 신뢰의 개인심리적 측면으로 볼 수 있다. 신뢰성은 신뢰 대상의 의도와 행동, 그리고 능력과 관련된다. 즉 신뢰성은 신뢰의 사회제도적 측면에 더 초점을 두고 있다.\n예컨대 언론 신뢰는 “언론사가 만족스러운 방식으로 기능을 수행할 것이라는 기대를 바탕으로 뉴스 콘텐츠를 기꺼이 받아들이려는 수용자의 의향”이라고 정의할 수 있다(Hanitzsch et al., 2018). 여기서 수용자의 의향은 언론 신뢰의 개인심리적 측면, 즉 신뢰자의 개인심리적 신뢰도를 의미한다. 언론사의 기능 수행은 언론 신뢰의 사회제도적 측면, 즉 신뢰 대상으로서 언론사라는 제도의 신뢰성을 뜻한다. 신뢰도는 수용자의 인지적 반응과 정서적 반응을 포괄한다. 각각에 대응하여 신뢰성은 사실성과 공정성을 모두 충족해야 한다. 즉 신뢰성 문제는 언뜻 보면 기능에 치우친 것처럼 보일 수 있으나 실은 가치 문제를 포함한다.\n\n\n4.2.3 인간 신뢰와 기계 신뢰\n전통적으로 신뢰는 개인 수준이든 집단 수준이든 인간에 대한 신뢰(trust in people)의 측면에서 다뤄졌다. 그러나 현대 사회에서는 기술을 신뢰 대상으로 하는 기술 신뢰(trust in technology)의 중요성이 커지고 있다. 특히 디지털 시대에 기술이 인간의 역할을 점점 더 많이 대체하면서 기술 신뢰에 대한 연구가 늘고 있다(김길수, 2020).\n기술 신뢰는 인간 신뢰와 다른 점이 있다. 우선 기술 자체는 의도를 갖고 있지 않다. 때문에 기술은 신뢰 대상이 아니라든가, 기술 신뢰를 다룰 때 일종의 능력인 성능을 중시하는 경향이 있었다. 그러나 기술도 인간과 마찬가지로 위험 요소를 갖고 있다. 즉 기술 역시 신뢰 대상으로 간주할 수 있다(김길수, 2020).\n전통적으로 사회과학에서 기술 신뢰는 기술을 활용하는 개인이나 조직을 대상으로 했으나 최근에는 서비스나 기술 자체에 대한 신뢰 문제로 확대되는 추세다(Jarvenpaa & Leidner, 1999; Mcknight et al., 2011; McKnight et al., 2002). 다른 한편 공학적으로 기술 신뢰는 시스템의 성능 문제를 중심으로 다루어졌으나 최근에는 기술의 사회적 영향력을 고려하고 기술의 사회적 구성을 고민하는 방향으로 발전하고 있다. 종합하면, 기계 신뢰는 기계 자체에 대한 신뢰성을 바탕으로 인간 신뢰를 부분적으로 통합하는 형태를 띄고 있다.\n\n\n4.2.4 신뢰할 수 있는 인공지능의 개념\n신뢰의 개념에 비추어 볼 때, TAI가란 신뢰성을 갖춘 AI, 그리고 이를 통해 인간이 신뢰도를 갖는 AI으로 볼 수 있다. 신뢰성을 갖춘 AI는 기본적으로 목표 과업(task)을 만족스러울만한 성능으로 수행해야 한다. 예컨대 객체 탐지를 위한 모델은 개와 고양이를, 행동 인식을 위한 모델은 걷기와 달리기를, 상황 이해를 위한 모델은 화재나 교통 사고를 빠르고 정확하게 파악해야 한다. 프롬프트를 입력하면 이미지를 만드는 멀티모달 AI(multimodal AI) 모델은 사용자가 “불 속에서 달리는 고양이”를 그리라고 했을 때 사용자 의도에 부합하는 영상을 생성해야 한다. 더 나아가 신뢰성을 가진 AI는 AI 기술과 서비스 자체의 신뢰성과 함께, AI를 기획, 개발, 운영하는 인간이나 집단의 신뢰성을 포괄한다. 이러한 신뢰성에는 신뢰자인 사용자에 대한 신뢰성 역시 포함된다. 즉 AI 신뢰성에는 사용자가 AI를 악의적으로 사용하지 않을 것이라는 기대도 포함된다.\nTAI는 기계 신뢰에 속한다. 우선 AI를 기획, 개발, 운영하는 인간이나 집단의 신뢰성을 생각해볼 수 있다. 이들은 의도를 가질 수 있다. 신뢰자는 신뢰 대상인 인간 기획자, 개발자, 운영자의 선의를 기대하는 방식으로 신뢰도를 갖는다. 다음으로 AI의 신뢰성을 살펴보자. AI는 실제로는 의도를 갖고 있지 않다. 즉 불확실성이 없다. 때문에 AI 자체는 엄밀한 의미에서 신뢰 대상이 될 수 없다고 간주할지 모른다. 그러나 AI는 두 가지 측면에서 의도를 고려해야 한다. 우선 AI는 종종 의도를 가진 것처럼 느껴진다. AI는 기계 중에서도 가장 자율적으로 실행된다. 인간이 개입하도록 따로 설계하지 않는 한, 즉 인간이 루프 속에 들어가고(human in the loop), 인간이 최종 결정하는 식의 인간 중심적으로(human-centered) 설계되지 않는 한 AI는 매우 높은 수준 자동으로 의사 결정을 할 수 있도록 만들어진 자율적이고 지능적인 자동장치(automata)이자 에이전트(agent)이다. 특히 딥러닝을 포함하는 기계학습 방식의 AI는 학습을 통해 입력 값과 출력 값의 쌍으로 이루어진 데이터세트를 바탕으로 의사 결정에 필요한 함수를 스스로 찾는다. 그리고 스스로 찾은 함수를 이용해 입력 값에 대한 출력 값을 예측, 분류, 생성한다.\n또 하나는 AI는 설사 그것을 기획, 개발, 운영, 사용하는 인간이 선한 의도를 갖고 있다고 하더라도 실제로는 해악이 되는 결과를 만들어 낼 수 있다. 이는 우선 AI의 성능 문제일 수 있다. 즉 인간의 선의를 AI가 제대로 구현하지 못할 수 있다. 예컨대 기계 번역기가 영어를 한국어로 제대로 번역하지 못할 수 있다. 이는 성능 개선을 통해 해결할 수 있다. 더 중요한 문제는 AI가 창발적 해악(emergent harm)을 초래할 수도 있다는 점이다(박도현, 2021). AI는 기본적으로 학습데이터 자체가 아니라 학습데이터의 극히 유한한 질서, 즉 특징(feature)을 무한한 가능성을 가진 벡터 공간(vector space)에 임베딩(embedding) 내지 인코딩(encoding)하고, 이를 바탕으로 새롭지만 극히 유한한 상황에 대해 예측하는 식으로 판별(discriminative model) 또는 생성(generative model)한다. 이 과정에서 AI는 학습데이터 자체와는 다른 새로운 상황을 판별하고 생성할 수 있다. 이를 창발로 부를 수 있다. 이러한 창발은 의도에 부합할 수도 있고 그렇지 않을 수도 있다. 즉 인간이 선의를 갖고 AI를 기획, 개발, 운영하고, 사용자도 선의를 갖고 AI를 사용한다고 하더라도, AI가 해악을 산출할 수도 있다.\n정리하면 AI는 AI과 관련된 인간의 불확실성, AI 성능의 불확실성, AI의 창발적 해악에 대한 불확실성을 갖는다. 따라서 TAI는 AI과 관련된 인간과 조직에 대한 신뢰성, AI의 성능 측면의 신뢰성, 그리고 AI가 인과적 해악(causal harm)은 물론 창발적 해악도 산출하지 않을 것이라는 신뢰성을 갖춰야 한다. 이러한 신뢰성을 바탕으로 AI에 대한 신뢰도가 높아지면 AI 신뢰 역시 높아지게 된다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인</span>"
    ]
  },
  {
    "objectID": "tai.html#신뢰할-수-있는-인공지능의-전개",
    "href": "tai.html#신뢰할-수-있는-인공지능의-전개",
    "title": "4  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "4.3 신뢰할 수 있는 인공지능의 전개",
    "text": "4.3 신뢰할 수 있는 인공지능의 전개\nAI의 사회적 영향력에 대한 논의는 AI 윤리, 설명가능한 인공지능 측면에서 진행되다가, TAI의 논의로 종합되는 추세이다. 이 절에서는 TAI 논의의 전개를 TAI 관련 주요 가이드라인인 EC의 신뢰할 수 있는 인공지능 윤리 가이드라인(Ethics guidelines for trustworthy AI), OECD의 AI 권고안(Recommendation of the Council on Artificial Intelligence), 그리고 한국정보통신기술협회(Telecommunications Technology Association, TTA)의 &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;를 중심으로 살펴보도록 한다.\n\n4.3.1 EC의 신뢰할 수 있는 인공지능 논의\n2019년 4월 EC의 &lt;신뢰할 수 있는 인공지능 윤리 가이드라인&gt;은 TAI의 논의가 본격화된 시발점이라 할 수 있다. 이 가이드라인에서는 TAI를 1) 합법적이고(lawful), 2) 윤리적이며(ethical), 3) 강건한(robust) AI로 규정한다. 합법성은 모든 법률과 규정을 따르는 것이다. 윤리성은 윤리 원칙과 가치를 존중하는 것이다. 강건함은 기술적인 측면과 사회적 측면을 모두 고려한다. 흔히 강건함은 AI가 학습데이터가 아닌 다양한 실제 상황(in the wild)에서도 객체 탐지나 행동 인식과 같은 목표 과업을 높은 성능으로 수행할 수 있음을 의미한다. 그러나 TAI에서 강건함은 창발적 해악까지 예방한다는 의미로 해석할 수 있다.\nTAI를 실현하기 위한 핵심 요구사항(key requirements)으로는 1) 인간 기관의 관리 감독(human agency and oversight), 2) 기술적 견고성 및 안전성(Technical Robustness and safety), 3) 개인 정보 보호 및 데이터 거버넌스(privacy and data governance), 4) 투명성(transparency), 5) 다양성, 비차별성 및 공정성(diversity, non-discrimination and fairness), 6) 사회적, 환경적 복지(societal and environmental well-being), 7) 책무성(accountability) 등 일곱 가지를 제시했다.\n1)은 AI 시스템이 루프 속 인간을 통해 인간의 감독을 받아 인간 기본권 증진과 의사 결정에 도움을 주도록 작동해야 한다는 것을 의미한다. 2)는 AI가 문제 발생시 항상 대응할 수 있도록 유연하게 설계되어 의도치 않은 해악까지도 방지할 수 있어야 한다는 것을 뜻한다. 3)은 개인정보 보호는 물론 데이터에 대한 합법적 접근과 관리를 위한 데이터 거버넌스를 구축해야 한다는 것을 말한다. 4)는 설명가능한 AI에 대한 요구사항이다. AI와 관련된 데이터, 시스템, 비즈니스 모델이 투명하고 추적 가능해야 한다. 또한 이해 당사자에게 AI의 기능과 한계를 적절하게 설명해야 한다. 5)는 취약 계층에 대한 부정적 영향을 제거하는 것을 의미한다. 또한 장애와 무관하게 AI에 접근가능해야(accessible) 한다. 6)은 지속가능성(sustainability) 측면의 복지 증진과 관련된다. 즉 AI가 현 인류는 물론 미래 세대에게 혜택을 줄 수 있어야 한다. 또한 다른 생명체를 포함한 환경의 영향을 고려해야 한다. 7)은 AI 시스템의 알고리듬, 데이터, 설계, 그리고 그 결과에 대한 감사가능성(auditability), 책임 요구에 대한 응답가능성(responsibility), 그리고 적절한 보정 가능성이 보장되어야 한다는 것을 의미한다.\nEC는 가이드라인을 바탕으로 2020년 7월 &lt;신뢰할 수 있는 인공지능 자체 평가 목록&gt;(Assessment List for Trustworthy Artificial Intelligence, ALTAI)를 내놓았다(EC, 2020). 이어 이러한 노력은 세계 최초의 AI 규제 법안 인공지능법(Artificial intelligence Act)으로 결실을 맺는다. 인공지능법의 초안은 2021년 4월 발의됐다(손영화, 2021). 이후 EU는 수정을 거쳐 2023년 12월 법안에 합의했다(김진희, 2023.12.10). 해당 법안은 테러나 범죄 예방, 법 집행, 국가 안보 등 일부 분야를 제외한 안면인식과 대규모 언어 모델(large language model, LLM) 사용을 엄격히 규제하고 있다. 또한 자율 주행이나 의료 등 고위험 기술에 대해서는 데이터 공개와 엄격한 테스트를 강제했다. 이를 위반할 경우 최대 3500만 유로, 또는 전 세계 매출의 7%에 해당하는 벌금을 부과하는 강제 수단도 포함됐다.\n\n\n4.3.2 OECD의 신뢰할 수 있는 인공지능 논의\nEU의 적극적인 움직임은 국제적으로 TAI 논의 확산을 가속화했다. EC 가이드라인이 나온지 한 달 뒤인 2019년 5월 경제협력개발기구(Organisation for Economic Co-operation and Development, OECD)는 AI 권고안(Recommendation of the Council on Artificial Intelligence)을 발표했다(OECD, 2019). 권고안에 따르면 TAI란 AI 시스템의 기획, 개발, 구축, 운영 등 전 단계에서 신뢰 가능한 AI의 원칙들이 실현된 AI 시스템을 의미한다. 이는 AI 생태계 전반에 걸쳐 작동해야 한다. 이를 위해 OECD는 TAI를 위한 5개 원칙을 제시했다.\n첫째, 포용 성장, 지속가능 발전과 복지 증진(inclusive growth, sustainable development and well-being)이다. AI는 인간의 능력, 창의력, 소수집단에 대한 포용력을 증진시키고, 사회적 불평등을 해소하며, 환경을 보호하는데 사용되도록 노력해야 한다.\n둘째, 인간 중심적 가치와 공정성(human-centered values and fairness)이다. AI는 인권, 자유, 민주적 가치, 인간의 존엄성, 자율성, 노동권, 평등, 다양성, 공정성, 사회 정의, 개인정보 보호 등을 개선하는데 사용되어야 한다는 것을 의미한다.\n셋째, 투명성과 설명가능성(transparency and explainability)이다. EC 가이드라인과 마찬가지로, AI 행위자(actors)는 사용자나 고객 등 AI 이해관계자에게 AI 시스템의 개발, 운영 등에 대해 정보를 투명하게 제공해야 하며, 그 의사결정과 핵심 내용을 이해하기 쉽게 설명해야 한다는 것을 뜻한다.\n넷째, 강건함, 보안, 안전(robustness, security and safety)이다. AI 시스템이 이러한 요건을 갖추기 위해 AI의 전체적인 사용 주기 전반에서 위험이 지속적으로 모니터링되고 추적 가능해야 한다. 위험이 발견됐을 때는 이를 분석하고 대응할 수 있도록 만들어져야 한다.\n다섯째, 책임성(accountability)이다. AI 행위자는 AI 시스템의 신뢰성을 구현할 수 있도록 행동 강령이나 안내서를 명시하고, 관련 문서를 공개하며, 필요한 감사를 받음으로써 책임성을 증명할 수 있어야 한다.\n이 밖에도 권고안에는 TAI에 대한 국가 정책 및 국제 협력에 대한 제안도 포함됐다. 여기에는 TAI 연구에 대한 공공 투자와 민간 투자 장려, 규제 프레임워크와 평가 메커니즘의 개발, 노동 시장 변화에 대응하기 위한 사회적 협의와 교육 지원, 개발도상국에 대한 지원과 국제 표준 마련을 포함한 국제 공조 등을 제안한다. 이에 따라 미국, 일본, 한국, 싱가포르, 호주 등 각국은 TAI를 위한 정책을 발표했다(The White House Office of Science and Technology Policy, 2020). 국제표준화기구(International Organization for Standardization)와 국제전기기술위위원회(International Electrotechnical Commission, IEC), 즉 ISO/IEC의 AI 위원회 JTC1/SC42 산하 그룹 중 신뢰성 작업 그룹(Working Group 3, WG3) 등은 기술적 관점에서 TAI 표준화 작업을 진행하고 있다(곽준호, 2022).\n\n\n4.3.3 한국의 신뢰할 수 있는 인공지능 논의\n한국은 2020년 11월 과학기술정보통신부가 &lt;국가 인공지능 윤리 기준&gt;을 발표했다(과학기술정보통신부, 2020). 우선 3대 기본 원칙으로 1) 인간 존엄성 원칙, 2) 사회의 공공선 원칙, 3) 기술의 합목적성 원칙을 꼽았다. 1)은 인간이 AI와 교환 불가능한 가치를 가지므로 인간에 해가 되지 않도록 개발되어야 한다는 것을 의미한다. 2)는 AI가 사회적 약자와 취약 계층의 접근성을 보장하고, 인류 보편적 복지를 향상하도록 개발되어야 한다는 것을 뜻한다. 3)은 AI가 인류의 삶과 번영에 필요한 도구라는 목적에 맞게 윤리적으로 개발되어야 한다는 것을 말한다.\n3대 원칙을 실행하는 10대 요건으로는 1) 인권보장, 2) 프라이버시 보호, 3) 다양성 존중, 4) 침해 금지, 5) 공공성, 6) 연대성, 7) 데이터 관리, 8) 책임성, 9) 안전성, 10) 투명성이 포함됐다. 각각의 내용은 앞서 설명한 EC의 가이드라인이나 OECD 권고안과 일맥상통한다.\n2021년에는 4차산업혁명위원회가 &lt;사람이 중심이 되는 AI를 위한 신뢰할 수 있는 인공지능 실현 전략(안)&gt;을 내놓았다(4차산업혁명위원회, 2021). 그 내용으로는 1) 신뢰 구현을 위한 법‧제도, 윤리적, 기술적 요구사항을 종합한 AI 개발 가이드북 제작과 보급, 2) AI 제품이나 서비스에 대한 민간 자율의 인증제 도입 및 지원, AI 일괄 지원 플랫폼 운영, 3) 설명 가능성‧공정성‧견고성 등을 향상시키기 위한 신뢰성 원천기술 개발 추진, 4) 학습용 데이터의 신뢰성 확보를 위한 표준 기준 제시 및 데이터 개방, 5) 고위험 AI에 대한 국민 안전‧신뢰성 향상 방안 연구, 6) AI 윤리 기준 실천을 위한 윤리 교육 강화 및 개발자‧이용자용 체크리스트 보급, 윤리‧신뢰성 향상을 위한 공론의 윤리 정책 플랫폼 운영 등이 있다.\n이어 2023년 7월 TTA는 EC의 ALTAI와 유사한 &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;를 내놓았다(한국정보통신기술협회, 2023). 기존 가이드라인에 비해, TTA의 안내서는 기술적으로 구체화된 TAI 개발 및 검증 방법론을 제시한다. 안내서는 전 분야에 해당하는 일반 분야와 함께 공공·사회 분야, 의료 분야, 자율주행 분야 등 세부 분야별 안내서를 제시했다.\n안내서에는 TAI의 설계 요소를 AI 신뢰성 프레임워크로 제시한다. 해당 프레임워크는 크게 세 가지로 구성된다. 첫째, 신뢰성 확보 대상이다. AI 데이터, AI 모델과 알고리즘, AI 시스템, 사람-AI 인터페이스가 해당된다. 둘째, 생애주기별 요구사항 분류이다. AI 생애주기란 AI 시스템을 구현하고 AI 서비스를 운영하는 과정을 뜻한다. AI 생애주기는 계획 및 설계, 데이터 수집 및 처리, AI 모델 개발, 시스템 구현, 운영 및 모니터링 등 5단계로 나뉜다. 셋째, AI 윤리 기준 준용이다.\n신뢰성 요건으로 AI 윤리 기준의 10대 요건 중 기술적 적용 가능한 4개 요건을 선별했다. 4개 요건은 다양성 존중, 책임성, 안전성, 투명성이다. AI 신뢰성 프레임워크를 정리하면 &lt;그림 1&gt;과 같다.\n\n\n\n그림 1. 인공지능 신뢰성 프레임워크(출처: 한국정보통신기술협회, 2023)\n\n\n또한 신뢰성 요건별 세부 속성 및 키워드는 &lt;표 1&gt;과 같다(한국정보통신기술협회, 2023). 그리고 신뢰성 요건을 충족시키기 위한 요구사항을 생애주기별로 15개 제시했다. AI 생애주기별 요구사항과 적용되는 신뢰성 요건 및 각 요구사항은 &lt;표 2&gt;와 같이 정리할 수 있다(한국정보통신기술협회, 2023).\n\n표 1. 신뢰성 요건별 정의와 관련 속성 및 관련 키워드(출처: 한국정보통신기술협회, 2023 일부 수정).\n\n\n\n\n\n\n\n\n신뢰성 요건\n정의\n관련 속성\n관련 키워드\n\n\n\n\n다양성 존중\nAI가 특정 개인이나 그룹에 대한 차별적이고 편향된 관행을 학습하거나 결과를 출력하지 않으며, 인종･성별･연령 등과 같은 특성과 관계없이 모든 사람이 평등하게 AI 기술의 혜택을 받을 수 있는 것\n공정성(fairness), 정당성(justice)\n편향(bias), 차별(discrimination), 편견(prejudice), 다양성(diversity), 평등(equality)\n\n\n책임성\nAI가 생명주기 전반에 걸쳐 추론 결과에 대한 책임을 보장하기 위한 메커니즘이 마련되어 있는 것\n책임성(responsibility), 감사가능성(auditability), 답변가능성(answerability)\n책임(liability)\n\n\n안전성\nAI가 인간의 생명･건강･재산 또는 환경을 해치지 않으며, 공격 및 보안 위협 등 다양한 위험에 대한 관리 대책이 마련되어 있는 것\n보안성(security), 견고성(robustness), 성능보장성(reliability), 통제가능성(controllability)\n적대적 공격 (adversarial attack), 복원력(resilience), 프라이버시(privacy)\n\n\n투명성\nAI가 추론한 결과를 인간이 이해하고 추적할 수 있으며, AI가 추론한 결과임을 알 수 있는 것\n설명가능성(explainability), 이해가능성(understandability), 추적가능성(traceability), 해석가능성(interpretability)\nXAI(eXplainable AI), 이해도(comprehensibility)\n\n\n\n\n\n\n생애주기\n요구사항\n신뢰성 요건\n\n\n다양성\n책임성\n안전성\n투명성\n\n\n1. 계획 및 설계\n1. AI 시스템에 대한 위험관리 계획 및 수행\n\nO\n\nO\n\n\n2. 거버넌스 체계 구성\nO\nO\nO\nO\n\n\n3. AI 시스템의 신뢰성 테스트 계획 수립\n\n\nO\nO\n\n\n2. 데이터 수집 및 처리\n4. 데이터의 활용을 위한 상세 정보 제공\n\nO\n\nO\n\n\n5. 데이터 강건성 확보를 위한 이상(Abnormal) 데이터 점검\n\n\nO\n\n\n\n6. 수집 및 가공된 학습 데이터의 편향 제거\nO\nO\n\nO\n\n\n3. AI 모델 개발\n7. 오픈소스 라이브러리의 보안성 및 호환성 확보\n\nO\nO\n\n\n\n8. AI 모델의 편향 제거\nO\n\n\n\n\n\n9. AI 모델 공격에 대한 방어 대책 수립\n\n\nO\n\n\n\n10. AI 모델 명세 및 출력 결과에 대한 설명 제공\n\nO\n\nO\n\n\n4. 시스템 구현\n11. AI 시스템 구현 시 발생 가능한 편향 제거\nO\n\n\n\n\n\n12. AI 시스템의 안전 모드 구현\n\nO\nO\nO\n\n\n13. AI 시스템의 설명에 대한 사용자의 이해도 제고\n\n\n\nO\n\n\n5. 운영 및 모니터링\n14. AI 시스템의 추적가능성 확보\n\n\nO\nO\n\n\n15. 서비스 제공 범위 및 상호작용 대상에 대한 설명 제공\n\nO\n\nO\n\n\n\n  \n:표 2. 신뢰성 요건 충족을 위한 AI 생애주기별 요구사항 (출처: 한국정보통신기술협회(2023), 일부 수정)\n요구사항은 다시 대분류 34개, 소분류 67개의 2단계의 체크리스트로 세분화했다. &lt;표 3&gt;은 생애주기 중 AI 모델 개발 단계의 요구사항 및 대분류 수준의 체크리스트의 예이다.\n소분류 수준의 체크리스트는 매우 구체적인 기술적 방안을 담고 있다. 예컨대 요구사항 9의 ’AI 모델 공격에 대한 방어 대책 수립’과 같은 내용은 일종의 AI 시스템의 모델을 훔쳐가는 모델 추출 공격(model extraction attack)에 대한 질의(query) 횟수 제한과 같은 기술적 방어 기법이나, AI 시스템에 적용된 모델을 속이는 모델 회피 공격(model evasion attack)에 대한 적대적 훈련(adversarial training)과 같은 방어 기법의 적용 여부를 따진다.\n\n\n\n표 3 신뢰할 수 있는 인공지능 모델 개발 단계의 요구사항과 체크리스트 (출처: 한국정보통신기술협회(2023))",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인</span>"
    ]
  },
  {
    "objectID": "tai.html#저널리즘-분야에서-신뢰할-수-있는-인공지능의-함의",
    "href": "tai.html#저널리즘-분야에서-신뢰할-수-있는-인공지능의-함의",
    "title": "4  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "4.4 저널리즘 분야에서 신뢰할 수 있는 인공지능의 함의",
    "text": "4.4 저널리즘 분야에서 신뢰할 수 있는 인공지능의 함의\n사회 전반의 AI 전환(AIX) 추세에 따라 저널리즘 분야에서도 AI 활용 가능성이 높아지는 추세다(한국언론진흥재단, 2020; 이현우ˑ이성민ˑ이상규, 2023; Beckett, 2019; Diakopoulos, 2019; Jones et al., 2022). 저널리즘에 활용되는 AI를 저널리즘 AI라고 부를 수 있다(박대민, 2023; Beckett, 2019). 이 절에서는 저널리즘 AI에서 TAI의 함의를 살펴본다.\n사실 TAI 관련 논의에서 저널리즘은 물론 미디어에 대한 관심은 높지 않다. EU의 AI 정책 문서에서 미디어에 대한 언급은 많지 않고 저널리즘에 대한 언급은 더욱 적다. 언론사 역시 AI 활용에 대한 높은 관심에도 불구하고 자사의 윤리 강령에 AI 활용 가이드라인을 포함시키는 사례는 많지 않다(Porlezza, 2023). 그러나 탈진실 사회(post truth society)로도 불리는 현재에, 딥페이크, 가짜 뉴스, 편향, 반향실 효과 등 AI가 촉발할 것으로 예상되는 미디어와 저널리즘 분야의 신뢰성 위기는 심각한 수준이다(박대민, 2023; 박대민, 2022; Edelman, 2023; Newman et al., 2023).\n저널리즘 AI의 신뢰성은 크게 세 가지 측면에서 논의될 필요가 있다. 첫째, 저널리즘 AI는 AI 기술이므로 TAI로 설계되어야 한다. 둘째, 저널리즘 AI의 신뢰성은 AI를 사용하는 인간과 조직의 신뢰성과도 연계된다. 즉 언론사를 비롯한 미디어의 신뢰성이 저널리즘 AI의 신뢰성에 영향을 준다. 셋째, 저널리즘 AI의 신뢰성은 결국 사용자의 신뢰도를 높이는 방향으로 사용되어야 한다.\n사실 TAI의 논의에서는 첫번째 사항만 고려되는 경향이 강하다. TTA의 안내서를 저널리즘과 같이 특정 영역에 구체적으로 적용할 때 크게 두 가지 측면에서 한계가 보인다(박대민, 2023). 첫째, 해당 안내서가 AI 생애주기 중심으로 작성되어, 저널리즘 분야의 기사 생애주기와 맞지 않다. 즉 저널리즘 AI에서 TAI를 구현하려면, AI 생애주기를 기사 생애주기 관점에서 통합해야 한다. 둘째, TTA의 신뢰성을 구현에서 언론인과 같은 도메인 전문가의 역할이 과소평가되어 있다. 이는 비록 TTA가 AI 생애주기 전체를 고려한 요구사항을 만들었지만, 실제로는 기획과 개발 중심이고 운영을 과소평가했기 때문이다. 즉 기획과 개발에 생애주기 5단계 중 4단계를, 15개 요구사항 중 13개를 할당하고 서비스 운영에는 생애주기 1단계와 요구사항 2개만 할당한 것에서도 나타난다. 그러나 AI를 활용하는 각 영역 입장, 저널리즘 AI를 활용하는 미디어의 입장에서는 운영이 거의 전부나 다름없다. 물론 저널리즘 AI에 TAI를 적용할 때 운영만 고려하라는 것은 아니다. 핵심은 저널리즘 AI를 TAI로 구현할 때 언론인과 같은 영역 전문가의 역할이 훨씬 더 강화되어야 한다는 것이다. 기획, 개발, 운영 전반에 루프 속 인간으로서 언론인이 개입할 수 있도록 해야 한다(박대민, 2023; Broussard et al., 2019; Gutierrez-Lopez et al. 2019; Wu et al., 2022).\n한편 저널리즘 AI의 신뢰성이 언론사의 신뢰성의 영향을 받는다면, 저널리즘 AI의 신뢰성은 저널리즘의 신뢰성을 제고하고 사용자의 신뢰도를 높이는 방식으로 활용되어야 한다. 다행히 저널리즘의 가치 지향과 TAI의 가치 지향은 어느 정도 통약 가능성(commensurability)을 갖고 있다. TAI 투명성과 안전성은 저널리즘 사실성의, TAI의 다양성과 책임성은 저널리즘 공정성의 전제 조건이다. 즉 탈진실 사회에서 TAI 기반 저널리즘 AI를 활용함으로써 저널리즘 AI의 투명성과 안전성을 높여서 저널리즘의 사실성을 개선할 수 있다. 또한 TAI 기반 저널리즘 AI를 통해 저널리즘 AI의 다양성과 책임성을 높임으로써 저널리즘의 공정성을 향상시킬 수 있다(박대민, 2023).\n이것은 AI를 쓰면 저널리즘의 사실성과 공정성이 제고된다는 기술결정론적인 입장이 아니다. 반대로 AI라는 기술을 사실성과 공정성 제고를 위해 사용할 수 있도록 구성해야 한다는 구성주의적 입장에 가깝다. 그리고 그 구성 전략으로서 TAI의 논의를 참고할 수 있다는 것이다. 뿐만 아니라 JAI를 TAI로 구성하는 과정에서 TAI의 방법론을 더욱 정교화하고 합목적적으로 사용할 수 있다는 것을 의미한다.\n정리하면 TAI 기반 JAI는 다음과 같은 세 조건을 충족해야 한다. 첫째, 저널리즘 분야에서 AIX가 진행되어야 한다. 즉 저널리즘과 AI의 실천적 결합이 전면적으로 이뤄져야 한다. 둘째, JAI에 TAI를 적용되어야 한다. 저널리즘의 AI가 TAI 관점에서 신뢰성을 확보해야 한다. 셋째, JAI가 언론 신뢰 개선에 기여해야 한다(박대민, 2023).\n이를 TAI를 다른 사회 영역에 적용할 때로 일반화할 수도 있다. 첫 번째는 AIX 조건이다. AI를 도메인에 적용하는 것을 의미한다. 둘째, TAI 조건이다. AIX에 TAI를 적용하는 것이다. 셋째, 도메인 조건이다. AI가 해당 도메인의 신뢰를 제고할 수 있어야 한다(박대민, 2023).",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인</span>"
    ]
  },
  {
    "objectID": "tai.html#더-읽을-거리",
    "href": "tai.html#더-읽을-거리",
    "title": "4  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "4.5 더 읽을 거리",
    "text": "4.5 더 읽을 거리\n\nAI 신뢰성 문제는 다양하게 제기되고 있다. 특히 AI 윤리와 관련된 최근 이슈를 알고 싶다면 다음 뉴스레터를 참조할 수 있다. AI 윤리 레터: https://ai-ethics.stibee.com/\n2023년 12월, 유럽 연합이 세계 최초로 TAI을 핵심으는 하는 인공지능법에 합의했다. 2024년 1월 5일 현재, 구체적인 내용은 아직 공개되지 않았다. 관련 내용은 추후 아래 링크에서 업데이트될 것으로 보인다. 유럽 연합의 인공지능법(AI act): https://artificialintelligenceact.eu/\nTAI 논의는 거대 담론에서 기술 구현을 논의하는 단계로 구체화되는 추세다. 그 예로는 아래의 보고서를 참고할 수 있다. 한국정보통신기술협회 (2023). &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;. 성남: 한국정보통신기술협회.\n저널리즘 분야의 인공지능에서 신뢰할 수 있는 인공지능의 적용 방향성을 모색한 연구로는 다음 논문을 참고할 수 있다. 박대민 (2023). 신뢰할 수 있는 인공지능 기반의 저널리즘 인공지능: 언론 신뢰와 인공지능 신뢰성 간 통약가능성을 바탕으로. &lt;언론과 사회&gt;, 31권 4호, 5-47.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인</span>"
    ]
  },
  {
    "objectID": "tai.html#생각해볼-문제",
    "href": "tai.html#생각해볼-문제",
    "title": "4  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "4.6 생각해볼 문제",
    "text": "4.6 생각해볼 문제\n\nAI의 신뢰성 문제를 야기하는 구체적인 사례는 어떤 것이 있는가?\n유럽연합의 인공지능법에서 TAI 관련 주요 쟁점 및 해법은 무엇이었는가? 해당 법안 이후 TAI를 각국 정부와 국내외 기업에서는 어떻게 수용하고 있는가? 그 한계와 대안은 무엇인가?\nTAI를 기술적으로 구현하려는 시도로는 어떤 것이 있는가? 이를 미디어 영역과 같이 구체적인 사회 영역에 적용할 수 있는가? 어떤 사회적, 기술적 기획이 필요한가?",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인</span>"
    ]
  },
  {
    "objectID": "tai.html#참고문헌",
    "href": "tai.html#참고문헌",
    "title": "4  신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인",
    "section": "4.7 참고문헌",
    "text": "4.7 참고문헌\n고학수·김용대·윤성로·김정훈·이선구·박도현·김시원 (2021). &lt;인공지능 원론&gt;. 서울: 박영사.\n과학기술정보통신부 (2021). &lt;신뢰할 수 있는 인공지능 실현 전략&gt;. 과학기술정보통신부.\n곽준호 (2022). 데이터의 품질과 인공 지능 시스템의 신뢰성. . 201권. 59-63.\n김길수 (2020). 인공지능의 신뢰에 관한 연구. &lt;한국자치행정학보&gt;, 34권 3호, 21-41.\n김진희(2023.12.10). EU, 37시간 진통 끝에 세계 최초 ‘AI 규제법’ 합의…주요 내용은? &lt;헬로T&gt;. Retrieved from https://www.hellot.net/news/article.html?no=84839\n박대민 (2023). 통과하면 사실로 인정되는 AI를 만들 수 있는가: 설명가능한 AI를 통한 사실성 제도로서 언론의 재구성. &lt;언론과사회&gt;. 31권 2호. 139-181.\n박대민 (2022). 미디어 인공지능: 컴퓨터 비전 분야 딥러닝 모델의 미디어 동영상 적용 가능성에 관한 연구. &lt;커뮤니케이션이론&gt;. 18권 1호, 111-154.\n박도현. (2021). &lt;인공지능과 해악&gt;. 서울대학교 법학대학원 박사학위논문. 4차산업혁명위원회 (2021). &lt;사람이 중심이 되는 AI를 위한 신뢰할 수 있는 인공지능 실현 전략(안)&gt;. 4차산업혁명위원회.\n손영화 (2023). AI 공정성에 관한 연구: 차별 없는 AI 사회의 실현. &lt;한양법학&gt;, 34권 3호. 275-304.\n손영화 (2021). EU AI 규칙안에 대한 일고찰. &lt;IP & Data 法&gt;, 1권 2호. 27-52.\n신예진 (2022). 신뢰할 수 있는 인공지능 개발 안내서. . 201권. 21-28.\n오로라(2024.1.4.). 美 최예진 교수 “안중근을 ’테러리스트’라는 AI, 韓 피해 상상 힘들어”. &lt;조선일보&gt;. Retrieved from https://www.chosun.com/economy/tech_it/2024/01/02/734OD6WEIZERFI3OARG55LQZHI/\n이현우·이성민·이상규 (2023). &lt;언론산업 인공지능(AI) 활용방안 연구&gt;. 서울: 한국언론진흥재단.\n한국언론진흥재단 (2020). &lt;2020 뉴스미디어의 신뢰·혁신·소통&gt;. 서울: 한국언론진흥재단.\n한국정보통신기술협회 (2023). &lt;신뢰할 수 있는 인공지능 개발 안내서&gt;. 성남: 한국정보통신기술협회.\n한상기 (2021). &lt;신뢰할 수 있는 인공지능&gt;. 서울: 클라우드나인.\nBeckett, C. (2019). New powers, new responsibilities: A global survey of journalism and artificial intelligence. London School of Economics & Political Science.\nBeutel, G., Geerits, E., & Kielstein, J. T. (2023). Artificial hallucination: GPT on LSD?. Critical Care, 27(1), 148.\nBroussard, M., Diakopoulos, N., Guzman, A. L., Abebe, R., Dupagne, M., & Chuan, C. H. (2019). Artificial intelligence and journalism. Journalism & Mass Communication Quarterly, 96(3), 673–695.\nColeman, J. S. (1990). Foundations of social theory. Cambridge, MA: Belknap Press.\nDiakopoulos, N. (2019). Automating the news: How algorithms are rewriting the media. Harvard University Press.\nDhiman, D. B. (2023). Does Artificial Intelligence help Journalists: A Boon or Bane?. Available at SSRN 4401194.\nEdelman (2023). 2023 Edelman trust barometer (Global Report). Retrieved from https://www.edelman.com/trust/2023/trust-barometer\nEuropean Commission (May, 2019). Ethics guidelines for trustworthy AI. Retrieved from https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\nEuropean Commission (July, 2020). Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment. Retrieved from https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment\nFukuyama, F. (1995). Trust: The social virtues and the creation of prosperity. New York: Free Press. Future of Life (2023.3) Pause Giant AI Experiments: An Open Letter. Retrieved from https://futureoflife.org/open-letter/pause-giant-ai-experiments/\nGambetta, D. (1988). Trust: Making and breaking cooperative relations. New York: Basil Blackwell.\nGutierrez-Lopez, M., Missaoui, S., Makri, S., Porlezza, C., Cooper, G., & MacFarlane, A. (2019, February). Journalists as design partners for AI. In Workshop for accurate, impartial and transparent journalism: challenges and solutions. CHI 2019.\nGuzman, A. L. (2018). What is human-machine communication, anyway? In Guzman A. L. (Eds.), Human-machine communication: Rethinking communication, technology, and ourselves (pp. 1-28). New York: Peter Lang.\nHanitzsch, T., Van Dalen, A., & Steindl, N. (2018). Caught in the nexus: A comparative and longitudinal analysis of public trust in the press. The International Journal of Press/politics, 23(1), 3-23.\nJarvenpaa, S. L., & Leidner, D. E. (1999). Communication and trust in global virtual teams. Organization Science, 10(6), 791-815.\nJones, B., Jones, R., & Luger, E. (2022). AI ‘Everywhere and Nowhere’: Addressing the AI Intelligibility Problem in Public Service Journalism. Digital Journalism, 10(10), 1731-1755.\nKrishnan, A. (2016). Killer robots: legality and ethicality of autonomous weapons. Routledge.\nLuhmann, N. (1988). Familiarity, confidence, trust: Problems and alternatives. In D. Gambetta (Ed.), Trust: Making and breaking cooperative relations (pp. 94-107). New York, NY: Blackwell Publishing.\nMayer, R. C., James H., Davis and F. David Schoorman. (1995). An Integrative Model of Organizational Trust. Academy of Management Review, 20(3). 709-734.\nMcknight, D. H., Carter, M., Thatcher, J. B., & Clay, P. F. (2011). Trust in a specific technology: An investigation of its components and measures. ACM Transactions on management information systems (TMIS), 2(2), 1-25.\nMcKnight, D. H., Choudhury, V., & Kacmar, C. (2002). Developing and validating trust measures for e-commerce: An integrative typology. Information systems research, 13(3), 334-359.\nMeyerson, D., Weick, K. E., & Kramer, R. M. (1996). Swift trust and temporary groups. In R. M. Kramer & T. R. Tyler (Eds.), Trust in organizations: Frontiers of theory and research: 166-195. Thousand Oaks, CA: Sage.\nMiles, R. E., & Creed, W. E. D. (1995). Organizational forms and managerial philosophies: A descriptive and analytical review. In B. M. Staw & L. L. Cummings (Eds.), Research in organizational behavior, vol. 17: 333-372. Greenwich, CT: JAI Press.\nNewman, N., Fletcher, R., Kirsten, E., Robertson, C. T., & Nielsen, R. K. (2023). Reuters Institute digital news report 2023. Reuters Institute for the study of Journalism. Retrieved from https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2023\nOECD (May 2019). Recommendation of the Council on OECD Legal Instruments Artificial Intelligence. Retrieved from https://legalinstruments.oecd.org/en/instruments/oecd-legal-0449\nOpdahl, A. L., Tessem, B., Dang-Nguyen, D. T., Motta, E., Setty, V., Throndsen, E., Tverberg, A., & Trattner, C. (2023). Trustworthy journalism through AI. Data & Knowledge Engineering, 146, 102182.\nPorlezza, C. (2023). Promoting responsible AI: A European perspective on the governance of artificial intelligence in media and journalism. Communications, 48(3), 370-394.\nRousseau, D. M., Sitkin S. B., Burt R. S, Camerer C., Not so Different after a Cross-discipline View of Trust. Academy of Management Review, 23(1) 393-404.\nShapiro, D., Sheppard, B. H., & Cheraskin, L. (1992). Business on a handshake. Negotiation Journal, 8: 365-377.\nThe White House Office of Science and Technology Policy (February 2020). American Artificial Intelligence Initiative: Year One Annual Report. Retrieved from https://www.nitrd.gov/nitrdgroups/images/c/c1/American-AI-Initiative-One-Year-Annual-Report.pdf\nWu, X., Xiao, L., Sun, Y., Zhang, J., Ma, T., & He, L. (2022). A survey of human-in-the-loop for machine learning. Future Generation Computer Systems, 135, 364-381.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>신뢰할 수 있는/책임있는 인공지능 윤리를 위한 가이드라인</span>"
    ]
  },
  {
    "objectID": "ineq.html",
    "href": "ineq.html",
    "title": "5  인공지능과 사회불평등",
    "section": "",
    "text": "5.1 들어가며\n2020년을 전후하여 경제협력개발기구(Organization for Economic Cooperation and Development, OECD), 유럽연합(European Union, EU), 유네스코(United Nations Educational, Scientific and Cultural Organization, UNESCO) 등을 비롯한 국제기구들이 인공지능 윤리기준을 발표하기 시작했고, 한국 역시 비슷한 시기에 과학기술정보통신부와 정보통신정책연구원이 함께 작업한 국내 인공지능 윤리 기준을 발표하였다. 모든 국제기구 및 국가의 인공지능 윤리 기준은 조직 및 국가의 성격과 맥락에 따라 조금씩 다르지만 공통적으로 다루고 있는 보편원칙들이 몇 가지가 있다. 그 중 하나가 인공지능이 야기할 수 있는 사회 불평등을 지양하고 감소시키는 방향으로 인공지능을 구현해야 한다는 점이다.\n예를 들어, OECD가 신뢰가능한 인공지능을 구현하기 위해 제시한 5가지 원칙 중 첫 번째는 “포용성장, 지속가능한 발전, 복지 증진의 지향(Inclusive growth, sustainable development, and well-being)”인데, 구체적인 내용은 소수 집단 및 사회적으로 소외된 집단을 포용하고 경제적 차별, 사회적 차별, 성차별 등의 불평등을 감소시키는 방향으로 인공지능을 구현해야 한다는 것이다(OECD, 2019). EU가 제시한 신뢰가능한 인공지능을 위한 윤리원칙 역시 “다양성, 차별금지, 공정성(Diversity, non-discrimination and fairness)”에 대한 내용이 5번째 원칙으로 직접적으로 제시되고 있으며(European Commission, 2019), 한국의 인공지능 윤리기준이 제시하는 10가지 원칙 중 3번째도 “다양성 존중”이다(과학기술정보통신, 2020).\n아래 &lt;표 1&gt;은 주요 국제기구 및 국내 인공지능 윤리 기준이 사회 불평등에 대한 윤리 원칙을 어떻게 설명하고 있는지 해당 내용을 발췌하여 정리하고 있다.\n&lt;표 1&gt; 주요 국제기구 및 국내 인공지능 윤리기준 중 사회 불평등 관련 원칙\n&lt;표 1&gt;에서 볼 수 있듯이 사회 불평등을 지양하는 인공지능 윤리는 인공지능 기술이 야기할 수 있는 사회 불평등을 최소화하고, 기술이 주는 혜택이 공정하게 분배되며, 모두가 기술을 동등하게 사용할 수 있는 권한과 능력이 주어질 수 있도록 인공지능 기술을 운용할 것을 제안하고 있다. 국내외 모든 인공지능 윤리기준이 사회 불평등에 대한 우려를 공통적으로 명시하고 있다는 사실은 인공지능 기술이 사회 불평등을 야기하거나 증가시킬 가능성이 있다는 데 이견이 없음을 보여준다. 사실상 우리는 기계학습이 산업의 영역에서 다소 급진적이고 실험적으로 도입되는 과정에서 기술을 개발하고 도입하던 당시에 품었던 장밋빛 전망과는 달리 사회 불평등이 예상치 못하게 가시화되는 것을 경험하고 목격해 왔다. 이런 경험은 국내외에 인공지능 기술을 설계하고, 만들고, 적용하는 데 ‘윤리적’ 지침이 필요함을 직시하게 했고 사회적으로도 그 필요에 대한 절대적인 동의를 얻어 왔다.\n인공지능 기술은 왜 사회 불평등에 대한 논의에서 자유로울 수 없을까? 우리는 꽤 오랫동안 ’인공지능’이라는 용어를 사용해왔지만 사실상 이 용어가 지칭하는 기술은 시대에 따라 달랐다. 지금 우리가 부르는 인공지능 기술은 사실상 패턴을 인식하는 알고리즘에 의해 작동하는 기계학습(machine learning)을 지칭하는데 이 기술은 사실상 인공적이지도 않고 지능적이지도 않다. 인공적이지 않다는 뜻은 기술이 작동하기 위한 모든 것이 물질적 자원과 인간의 물질적인 노동에 기반하고 있다는 의미이며, 지능적이지 않다는 것은 대규모 데이터 집합에서 찾아낼 수 있는 패턴을 인식하고 따르는 것일 뿐 별다른 지적 능력이 없다는 의미이다(Crawford, 2021). 그런데 인공적이지도 않고 지능적이지도 않은 이유 때문에 인공지능 기술은 필연적으로 우리가 살고 있는 사회적, 문화적, 정치적 맥락 위에 놓여 지며, 같은 이유로 끊임없이 사회 불평등을 만들고, 드러내고, 증폭시킨다. 인공지능을 구현하는 물질적 자원과 인간의 물질 노동은 언제나 불평등하게 설계되고 분배되며, 기계학습 알고리즘이 읽어내는 데이터의 패턴이라는 것은 어떤 것이 알고리즘이 읽어낼 만한 패턴으로 읽혀지고 어떤 것은 패턴이 될 수 없는지 선택하고 분류하는 불평등한 관계를 내재하고 있다.\n때문에 인공지능 기술이 신경망 기술과 통계적 패턴 인식 기술의 발전에 따라 점차 정교해지는 동안 우리는 인공지능 기술을 사회적, 문화적, 역사적으로 바라보며 “무엇이 누구를 위해 최적화되고 누구에게 결정권이 있는지” 물어야 할 필요가 있다(Crawford, 2021). 즉, 이 장에서 인공지능의 사회 불평등 문제를 다루는 방법은 지금 우리가 살고 있는 시대의 인공지능 기술이 ‘누구를 위해’ 최적화되어 개발되었으며, 그것을 결정한 집단이 ‘누구인지’ 묻는 과정이다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>인공지능과 사회불평등</span>"
    ]
  },
  {
    "objectID": "ineq.html#들어가며",
    "href": "ineq.html#들어가며",
    "title": "5  인공지능과 사회불평등",
    "section": "",
    "text": "조직 및 기관\n내용\n\n\n\n\nOECD (Principles on AI, 2019)\n1. Inclusive growth, sustainable development and well-being: “Stakeholders should proactively engage in responsible stewardship of trustworthy AI in pursuit of beneficial outcomes for people and the planet, such as augmenting human capabilities and enhancing creativity, advancing inclusion of underrepresented populations, reducing economic, social, gender, and other inequalities, and protecting natural environments, thus invigorating inclusive growth, sustainable development and wellbeing.”\n\n\nEuropean Commission (Ethics Guidelines for Trustworthy AI, 2019)\n5. Diversity, non-discrimination and fairness: In order to achieve Trustworthy AI, we must enable inclusion and diversity throughout the entire AI system’s life cycle. Besides the consideration and involvement of all affected stakeholders throughout the process, this also entails ensuring equal access through inclusive design processes as well as equal treatment. This requirement is closely linked with the principle of fairness.\n\n\nUNESCO (Recommendation on the Ethics of AI, 2021)\nFairness and non-discrimination AI actors should promote social justice and safeguard fairness and non-discrimination of any kind in compliance with international law. This implies an inclusive approach to ensuring that the benefits of AI technologies are available and accessible to all, taking into consideration the specific needs of different age groups, cultural systems, different language groups, persons with disabilities, girls and women, and disadvantaged, marginalized and vulnerable people or people in vulnerable situations…AI actors should make all reasonable efforts to minimize and avoid reinforcing or perpetuating discriminatory or biased applications and outcomes throughout the life cycle of the AI system to ensure fairness of such systems. Effective remedy should be available against discrimination and biased algorithmic determination. Furthermore, digital and knowledge divides within and between countries need to be addressed throughout an AI system life cycle, including in terms of access and quality of access to technology and data, in accordance with relevant national, regional and international legal frameworks, as well as in terms of connectivity, knowledge and skills and meaningful participation of the affected communities, such that every person is treated equitably\n\n\n과학기술정보통신부 (사람이 중심이 되는 인공지능 윤리기준, 2020)\n③ 다양성 존중 - 인공지능 개발 및 활용 전 단계에서 사용자의 다양성과 대표성을 반영해야 하며, 성별·연령·장애·지역·인종·종교·국가 등 개인 특성에 따른 편향과 차별을 최소화하고, 상용화된 인공지능은 모든 사람에게 공정하게 적용되어야 한다. - 사회적 약자 및 취약 계층의 인공지능 기술 및 서비스에 대한 접근성을 보장하고, 인공지능이 주는 혜택은 특정 집단이 아닌 모든 사람에게 골고루 분배되도록 노력해야 한다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>인공지능과 사회불평등</span>"
    ]
  },
  {
    "objectID": "ineq.html#불평등한-개발",
    "href": "ineq.html#불평등한-개발",
    "title": "5  인공지능과 사회불평등",
    "section": "5.2 불평등한 개발",
    "text": "5.2 불평등한 개발\n앞서 &lt;표 1&gt;에 정리한 국내 인공지능 윤리원칙은 인공지능 기술의 개발 및 활용의 전 단계에서 사용자의 다양성을 반영하고 편향을 최소화해야 한다고 기술하고 있다. 유럽연합에서 발표한 인공지능 윤리 원칙은 이를 좀 더 상세히 기술하는데 인공지능 시스템에 사용되는 훈련데이터 및 운영 데이터 세트가 포함하고 있는 역사적 편향이 특정 그룹이나 사람들에게 의도적, 비의도적 차별이나 편견을 유발할 수 있기 때문에 데이터를 수집하는 단계에서부터 식별 가능한 수준의 편향을 최대한 제거하고, 알고리즘이 개발된 후에도 지속적인 관리와 감독을 통해 투명하게 관리할 것을 권고하고 있다.\n이는 인공지능과 사회 불평등을 이야기할 때 가장 보편적으로 언급되고 있는 ’데이터 편향’의 문제이다. 알고리즘을 훈련하는 데이터 자체에 성별, 연령, 인종, 지역, 신체 능력, 종교, 국가 등의 사회적 특성이 편향된 수로 존재하고, 알고리즘은 편향된 분배를 인식 가능한 패턴의 일부로 읽어내는 것이다. 일례로 2014년 미국의 전자상거래 기업 아마존(Amazon)은 매 해 쏟아지는 수 만장의 채용지원서를 검토하는 절차를 축소하기 위해 입사 지원자를 빠르게 필터링하는 인공지능 채용 시스템을 도입했다. 입사 지원자를 필터링하는 방식은 기계학습의 기본 원칙에 따라 지금까지 아마존에 10여 년간 축적된 지원자 및 합격자 데이터를 바탕으로 이력서에 나타나는 5만 여개의 단어를 인식하도록 훈련된 알고리즘이 합격자의 패턴을 익혀 지원자 중 상위 5명을 뽑아내는 방식이었다. 수만 개의 지원서 중 소수의 채용 후보자만 걸러낸다는 애초의 목표는 알고리즘이 성공적으로 달성해내었지만 알고리즘이 데이터를 방식으로 후보자를 걸러내는 방식은 여성 지원자에게 매우 불리한 방식으로 이루어졌다. 정보기술 기업 종사자의 고질적인 성비 불균형을 알고리즘은 채용을 위한 데이터의 패턴으로 학습했고, 이에 따라 여성 동아리를 나왔거나 여자 대학을 나오는 등 지원자의 성별을 ’여성’으로 짐작할 수 있는 경우 감점이 이루어지는 등 시스템이 매우 체계적으로 여성 지원자를 차별하는 결과를 보여주었다. 아마존의 인공지능 채용시스템과 마찬가지로 이미지를 인식하는 알고리즘 역시 사회의 여러 영역에 적용되어 인간에 의해 수동으로 분류되어야 하는 많은 양의 이미지 자료를 쉽고 빠르게 처리해줄 것이라는 기대를 불러왔다. 실제로 미국국가기술표준원(The National Institute of Standards and Technology, NIST)이 2019년에 자발적으로 자사 알고리즘을 제출한 미국의 99개 기업의 189종류 이미지 인식 알고리즘이 몇 가지 행정 업무에 얼마나 적합한지 시험했다. 미국의 비자 신청서, 세관 데이터베이스 등에서 뽑은 사진 1800만 장 속 얼굴을 나이, 인종, 성별에 따라 얼마나 정확하게 인식하는지 시험한 것인데 결과는 여성과 유색 인종의 얼굴을 식별하는 정확도가 백인 남성을 식별하는 정확도보다 훨씬 낮았다(Grother, Ngan, & Hanaoka, 2019). 미국의 엠아이티 미디어 연구소(MIT Media Lab) 연구원 조이 부올람위니는 그녀가 연구 중이던 안면 인식 프로그램이 백인인 동료의 얼굴은 쉽게 인식하는 데 반해 유색인종인 자신의 얼굴을 인식하지 못했으며 그녀가 하얀 가면을 쓰고서야 그녀의 얼굴을 인식하는 것을 경험했다(&lt;그림 1&gt; 참고). 이를 계기로 그녀는 미국의 주요 상업용 이미지 인식 기술에 대해 성별과 인종 식별 정확도를 시험하는 연구를 진행했는데, 유색 여성에 대한 정확도가 가장 떨어진다는 결과가 나왔다(Buolamwini, 2017). 뿐만 아니라 또 다른 연구는 이 이미지 인식 기술은 여성이나 남성으로 스스로 구분하지 않는 성소수자들의 경우 젠더를 식별하지 못하거나 성 정체성과 무관하게 외형적인 모습에 따라 임의로 이분법적인 성별로 분류하고 있음을 밝히기도 했다(Scheuerman, Paul, & Brubaker, 2019).\n\n\n\n&lt;그림 1&gt; 부올람위니의 안면인식 기술의 인종 편향성 실험 장면(Coded bias, 2020)\n\n\n이는 인공지능 기술이 ‘누구를 위해’ 최적화되었는지에 대한 첫 번째 대답을 제공한다. 근본적으로 디지털로 존재하거나 전환할 수 있는 대규모 데이터에서 식별 가능한 패턴을 인식하고 학습하는 현재의 기계학습 모델은 필연적으로 ’패턴’으로 인식될 수 있는 집단의 특성을 학습하고 그들에게 최적화된 기술을 만들어낸다. 앞서 설명한 예시를 통해 살펴보면 아마존의 채용시스템은 10년 동안의 지원자와 합격자들의 특성을 학습했고, 남성의 비율이 월등하게 높은 기술 기업의 특성상 여성의 특성을 추론할 수 있는 지원자를 배제하는 결과를 낳았다. 마찬가지로 대규모 이미지 데이터를 통해 성별, 나이에 따른 얼굴 특성을 학습한 이미지 인식 알고리즘은 미국 사회에서 양적으로 훨씬 많은 수로 존재하는 백인 남성의 데이터에 대해 좀 더 충분한 훈련을 거쳤고 이는 정확도에도 영향을 미쳤다. 기계학습 초기 모델에 비해 훨씬 많은 양의 데이터를 필요로 하는 생성형 인공지능의 경우도 마찬가지다. 오픈에이아이(OpenAI)사에서 출시한 챗지피티(ChatGPT)의 경우 다양한 언어를 지원하고 있지만 영어 질문 및 답변에 대한 정확도가 다른 언어에 비해 월등히 높다. 이는 미국의 인구가 많을 뿐 아니라, 전 세계적으로 영어로 된 문서가 가장 많이 존재하며, 디지털 기술이 안정적인 미국에서 훈련 데이터로 쓸 수 있는 디지털 문서가 가장 많기 때문이다. 같은 이유로 사용하는 사람이 많지 않고, 디지털 인프라의 한계로 디지털로 변환되었거나 변환할 수 있는 문서가 적은 나라의 언어에 대해서는 그럴듯하게 문장을 만들어 낸 경우에도 답변의 정확도가 떨어질 수 밖에 없다.\n별도의 윤리적, 정책적, 기술적 개입 없이는 데이터에 존재하는 패턴을 식별하기 위해 양적으로 우세한 성별, 연령, 인종, 지역, 신체 능력, 종교, 국가 집단에게 최적화된 방식으로 인공지능은 개발되고 구현될 수밖에 없는 한계를 갖는다. 사실 기술이 특정 집단에게 유리한 방식으로 설계된 역사는 인공지능 기술보다 훨씬 오래 되었다. 같은 공간에서 에어컨을 가동했을 때 여성이 남성보다 추위를 더 타는 것은 인간의 신체에 적합한 표준 온도를 규정하는 실험이 남성 피험자를 대상으로 이루어 진 영향이 크며, 여성 게이머가 컴퓨터 키보드나 마우스를 작동하는 데 속도나 정확도가 떨어지는 이유 역시 마찬가지다. 이렇듯 기술은 일부 집단에게 좀 더 유리한 방식으로 최적화되어 왔는데 인공지능 기술에 이르러 이와 같은 편향된 설계 및 구현이 가져 올 사회적 영향력이 좀 더 커지고 있다.\n기계학습 알고리즘의 특성 상 훈련데이터에 존재하는 편향을 사전에 가능한 제거하거나 통제한다고 해서 사회 불평등을 완벽하게 해소할 수 있는 것은 아니다. 훈련 데이터를 기반으로 성능이 일정 수준에 이른 알고리즘은 이용자들에게 운용하는 단계를 거치는데 이 단계에서도 사회에 내재하는 불평등이 데이터로 학습될 수 있는 여지가 있다. 일례로 챗봇의 초기 모델들은 서비스의 운용 과정에서 알고리즘이 이용자의 편향적 발언을 데이터로 학습하면서 일부 집단에 대한 혐오 표현 등을 일삼아 서비스를 중단하는 사태도 있었다. 2021년 스캐터랩이 출시했던 챗봇 이루다는 ’연애의 과학’이라는 앱에서 이루어지는 20대들의 카카오톡 대화를 훈련 데이터로 삼아 실제 10대, 20대 이용자들과 유사한 어투를 구사하도록 알고리즘을 설계했다. 덕분에 실제 대학생들과 대화하는 것 같은 자연스러운 화법을 보여주었다. 하지만 출시 이후 남초 온라인 커뮤니티에서는 이루다를 성적 도구로 악용하는 방법이나 성희롱하는 방법이 공유되는 등의 문제가 발생했다. 이용자와의 대화를 학습 데이터로 삼는 이루다의 설계는 점차 사회적 약자 및 소수자를 향한 혐오 발언을 쏟아냈고 결국 스캐터랩은 서비스를 잠정 중단하고 이듬 해 두 번째 버전을 재출시했다. 국내의 이루다 사태 이전에 이미 유사한 사건이 미국에서도 있었는데 2016년 미국의 마이크로소프트사가 출시한 챗봇 테이(Tay)는 이용자와의 지속적인 대화를 주고받으며 챗봇이 인간의 언어를 배우고 그와 유사한 언어 및 대화 능력을 갖출 수 있는지 살펴보는 일종의 실험이었다. 마이크로소프트사는 테이가 트위터를 통해 이용자와 실시간 대화를 주고받도록 했다. 하지만 테이의 출시 이후 익명의 게시판에 테이가 사회적 편견을 양산하는 발언을 일삼도록 훈련시켜보자고 모인 일부 이용자들이 쏟아내는 욕설, 여성 혐오, 인종 비하 문장을 테이는 그대로 학습했고 결국 마이크로소프트사는 16시간 만에 급하게 서비스 운영을 중단했다.\n기계학습을 기반으로 하는 지금의 인공지능 기술을 정교화하기 위해 기술자들은 오랜 시간동안 보다 정교한 신경망 모델을 개발하고 학습하기 충분한 양의 데이터를 확보하는 데 각고의 노력을 기울여 왔다. 하지만 훈련 데이터의 크기가 커지면 지금의 인공지능 모델이 더 많은 업무를 수행할 수 있게 될 것임은 분명하지만 동시에 그에 따른 잠재적인 위험을 가져온다는 것에 대해 충분한 사회적 논의가 이루어지지 못했다(Bendal & Gebru et al., 2021). 현재는 충분한 데이터의 양을 확보하기 위해 온라인에 존재하는 디지털 데이터를 모두 모아 알고리즘이 학습하는 형태를 띠고 있는데 적절하게 취사 및 선별하지 않은 대용량 인터넷 데이터는 근본적으로 사회적 차별과 편견을 내재하고 있다. 예를 들어, GPT-2의 훈련 데이터로 사용된 온라인 커뮤니티의 사용자들의 인구통계학적 정보를 분석해보면 18세에서 29세 사이의 남성이 60%를 상회하는 비율로 구성되어 있으며, 이들에게 최적화된 방식으로 언어 모델이 구현될 수밖에 없는 한계를 갖는다.\n인터넷 특유의 개방성을 활용하여 소수 집단이 모여 일종의 데이터를 생성했다고 해도 들어오고 나가는 링크의 수가 절대적으로 적은 소수 집단의 온라인 커뮤니티는 통계적 패턴을 우선시하는 알고리즘의 설계에 포함되기 어려운 불평등한 처지에 놓인다. 뿐만 아니라 역사적으로 소외된 집단들이 점차 이들의 목소리를 인터넷 공간에서 높여간다고 할 때, 이 같은 지속적이고 역사적인 진화 과정을 현재의 기계학습은 반영하지 못한다. 일례로 미국의 흑인 인권 운동인 블랙 라이브스 매터(Black Lives Matter)는 흑인 인권 운동이 일어날 때마다 이용자들이 위키피디아에 관련 사례 및 의의를 지속적으로 추가하고 수정하면서 담론을 확장시키고 있다. 위키피디아의 블랙 라이브스 매터 페이지에 모인 이용자들은 역사 속에서 잊혀진 인종 차별 예시를 발굴해내며 양적으로나 질적으로 성장했고 이는 소수자들의 담론을 조명하는 장으로 역할을 하기 시작했다. 사회적으로 불평등한 위치에 놓인 이들의 이야기는 데이터가 되기 위해 당사자 및 관계자 스스로의 각고의 노력이 필요하다. 이들이 일반적인 경로로 인공지능의 학습 데이터로 포함될 확률은 매우 낮으며, 이는 결국 이들에 대한 인식의 정확도가 떨어지거나, 선택에서 배제되거나, 때로는 완전히 데이터에 포함되지 못하고 제외되는 불평등을 낳는다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>인공지능과 사회불평등</span>"
    ]
  },
  {
    "objectID": "ineq.html#불평등한-접근성",
    "href": "ineq.html#불평등한-접근성",
    "title": "5  인공지능과 사회불평등",
    "section": "5.3 불평등한 접근성",
    "text": "5.3 불평등한 접근성\n인공지능 윤리가 서술하는 사회 불평등 문제의 또 하나의 맥락은 기술 자체와 기술이 주는 혜택에 대한 접근성의 문제다. 한국의 인공지능 윤리 기준은 사회적 약자 및 취약 계층이 인공지능 기술 및 서비스에 대한 접근성을 충분히 갖고 있는지, 인공지능이 생산하는 사회적인 혜택이 특정 집단만을 향한 것이 아니라 모든 사람에게 골고루 분배되고 있는지 확인할 것을 권고하고 있다.\n1900년대 초반 뉴욕 주 롱아일랜드로 진입하는 곳에 세워져 아직까지도 도시 경관을 구성하고 있는 한 다리는 유달리 낮게 설계되어 승용차 이외에 높이가 높은 버스나 상업용 트럭이 진입할 수 없다. 인종차별주의자로 평가 받는 로버트 모제스가 설계한 이 다리는 별도의 신분 및 계급 확인 절차 없이 다리를 통과하는 과정 중에 롱아일랜드 휴양지에 접근할 수 있는 사람을 선별하는 효과를 노렸는데 애초에 미국 사회 안에서 자차를 소유하지 못한 사람들이 경제적으로 부유하지 못하거나 유색 인종일 확률이 높다는 점을 감안하여 이들이 대중교통이나 상업용 트럭을 타고 도시로 진입하지 못하도록 유달리 낮게 설계되었던 것이다(Winner, 2017). 이 사례는 자동차와 같은 특정 기술에 대한 접근성이 때로는 사회적, 역사적으로 존재하는 위계 구조를 가시적으로 드러낼 수 있으며, 도시 입구의 낮은 다리처럼 기술이 설계되는 독특한 방식이 사회의 인프라에 대한 접근성을 결정하기도 한다는 사실을 보여준다. 즉, 기술이 사회에 위치할 때 기술에 대한 접근성은 누구에게나 동등하지 않으며, 한 기술이 사회의 또 다른 기술이나 문화에 대한 접근성을 구성하거나 결정하기도 한다.\n\n\n\n&lt;그림 2&gt; 1924년 뉴욕 주 롱아일랜드로 진입하는 다리(Campanella, 2017. 7. 10)\n\n\n이는 인공지능 기술에 있어서도 마찬가지다. EU의 인공지능 윤리 기준은 나이, 성별, 능력 또는 특성과 상관없이 모든 사람이 인공지능 제품이나 서비스를 사용할 수 있도록 설계되어야 함을 밝히고 있다. 하지만 현실에서 인공지능 기술이 도입되는 방식은 앞서 살펴 본 뉴욕주의 다리처럼 모두에게 동등한 접근성을 제공하지 않고 있다. 예를 들어, 최근 금융권에서는 이미지 인식 기술에 기반한 안면 탐지 및 인식 기술을 도입하여 실명확인, 인공지능 은행원 등 비대면 금융 서비스를 적극적으로 도입하고 있다. 은행 업무를 보기 위해 이용자 개인의 스마트 기기를 사용할 때 신분증과 얼굴 촬영만으로 신분을 인식하는 것이다. 은행의 일부 영업점은 인공지능 은행원을 도입해 사람이 아닌 가상의 은행원과 필요한 업무를 본다. 이 같은 기술적 전환과 함께 오프라인 영업점이나 자동현금인출기는 지속적으로 줄어들고 있다. 스마트 기기에 대한 접근성이 높고 인공지능 기반 인식 기술을 사용하는 데 사회적 제약이 없는 연령, 국적, 계층의 사람들에게는 이 같은 변화가 오히려 효율성의 측면에서 반가울 수 있다. 하지만 은행업무 전반이 비대면으로 전환되고 인공지능기술을 기반으로 한 신분 확인 등의 절차는 디지털 기술에 익숙하지 않은 고령층, 장애인, 외국인 노동자 등이 은행 업무를 보는데 많은 불편을 낳고 있다. 2022년 디지털정보격차 실태조사 보고서에 따르면 모바일 기기 이용 능력을 묻는 질문에 매우 잘 활용하거나 잘 활용하는 편이라고 대답한 응답자의 비율은 일반 국민에서 81%로 나타난 데 반해 저소득층은 73.1%, 장애인은 67.6%, 노어민은 64.5%, 고령층은 61.1%로 하향세를 보였다(과학기술정보통신부, 2022). 뿐만 아니라 영업점에 설치된 안면탐지 및 인증 카메라의 위치는 국민의 평균 키를 기준으로 설치되어 있는데 고령층이나 장애인 등 이동 보조 도구를 이용해야 하는 집단에게는 카메라에 접근하는 것조차 쉽지 않다. 이는 단편적으로는 사회 집단 별로 나타나는 디지털 격차에 의해 스마트 기기나 인공지능 기술에 대한 접근성을 떨어뜨리지만, 이 같은 디지털 설계에 원만하게 포함되지 못한 이들이 결과적으로 은행 업무라는 사회 인프라를 제대로 누리지 못하며 사회 안에서 또 다른 차원의 불평등한 접근성을 초래한다.\n접근성의 문제를 논할 때 대개 리터러시 교육이나 디지털 기기 보급 등의 시도를 통해 해결하려는 모습이 두드러진다. 국내에서도 디지털 정보격차 실태조사 결과를 발표하면서 이에 대한 대응책으로 공교육과 공공 서비스를 통한 기술 교육과 포용 정책을 펼쳐 디지털 격차를 해소해야 한다는 의견이 지배적이다. 정부가 실시하고 있는 디지털 역량교육 사업 역시 전국의 복지관, 주민센터, 도서관 등에 디지털 배움터를 운영해 디지털 전문 강사를 채용하고 관련 교육을 이수하도록 지원하고 있다(한국지능정보사회진흥원, 2021). 하지만 사회 안에서 접근성의 격차는 생각보다 은밀하게 설계된다.\n앞서 살펴 본 뉴욕주의 다리와 금융권의 디지털 전환 사례의 연장선 상에서 스마트시티(Smart City)를 예시로 생각해보자. 스마트 시티는 다양한 정보통신 기술과 센서 등을 통해 시민들의 데이터를 수집하고 이를 바탕으로 도시 내 자산과 자원을 관리하는 도시를 의미한다. 무선 인터넷이나 사물인터넷을 기본 인프라로 하여 디지털 행정, 비대면 서비스, 스마트 모빌리티, 빅데이터 등이 도시 생활 전반에 결합된다. 스마트시티는 인공지능 기술과 마찬가지로 도시의 데이터를 직접 수집하여 이를 기반으로 한 도시 설계와 발전 계획을 추진한다. 몇몇 연구들은 이 같은 스마트 시티의 구성 원리가 일부 집단의 도시 접근성을 제한하고 결과적으로 사회 불평등을 도시 정경 안에서 구성할 수 있다고 지적하고 있다. 공통적으로 지적하는 문제로는 첫째, 도시의 교통을 설계하는 데 시민들의 이동 패턴에 대한 데이터는 매우 중요하게 활용되는데 사실상 이동 패턴이 자동차를 사용하는 사람들의 네비게이션 정보 위주로 수집되어 자동차를 소유 비율이 상대적으로 낮은 여성들, 고령층 및 장애인의 이동 패턴은 적극적으로 수집되지 못한다. 또한 대중교통에 있어서도 출퇴근 시간의 정보만을 우선적으로 고려하게 되어 출퇴근 시간 외 시간에 이동이 이루어지는 가사를 전담하는 사람들의 편의는 충분하게 고려되지 못한다(Macaya, Dhaou, & Cunha, 2021). 이 같은 데이터 편향을 직시하지 못한 채 도시를 설계하게 되면 일부 집단들은 도시 인프라에 대한 접근성 자체가 떨어지는 결과를 낳게 된다.\nEU의 인공지능 윤리 기준은 접근성의 문제를 논하는 데 있어 장애를 가진 사람들이 기술에 접근성을 갖도록 보장할 것을 특별히 권고하면서 동시에 서비스를 디자인할 때에 가능한 한 많은 사용자의 범위를 포함할 수 있도록 설계하도록 제안하고 있다. 실제로 일부 기업의 최근 몇몇 사례들은 인공지능 기술이 신체적 장애를 가진 이들이 사회 인프라에 대해 좀 더 높은 접근성을 가질 수 있도록 보조하는 역할을 할 수 있음을 보여주고 있다. 인공지능 스피커로 알려진 아마존 알렉사(Amazon Alexa)나 구글 어시스턴트(Google Assistant), 애플 시리(Apple Siri) 등은 신체적 장애를 가진 이들이 음성 명령을 통해 환경을 제어할 수 있도록 지원하고 있으며, 인공지능을 기반으로 한 자율운전기능을 휠체어에 적용하여 이동 중에 도로 환경 및 상황을 자동으로 인식하여 좀 더 편리한 운행을 할 수 있도록 개발 중이기도 하다. 하지만 사회 곳곳에서 섬세하게 설계되지 못한 채 다수의 편의를 위한다는 명목으로 소수집단들의 접근성을 가로막고 나아가 이들이 기술이 주는 혜택을 충분히 누리지 못하도록 가로막는 사례들이 여전히 많다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>인공지능과 사회불평등</span>"
    ]
  },
  {
    "objectID": "ineq.html#불평등한-참여",
    "href": "ineq.html#불평등한-참여",
    "title": "5  인공지능과 사회불평등",
    "section": "5.4 불평등한 참여",
    "text": "5.4 불평등한 참여\n캐서린 헤일즈(Katherine Hayles)가 2005년에 쓴 책의 제목 &lt;나의 어머니는 컴퓨터였다(My mother was a computer)&gt;는 전자컴퓨터가 아직 완전히 개발되지 않고 소프트웨어가 존재하지 않았던 1930년대에서 1940년대 미국 사회에서 프로그램을 짜고 계산하는 일에 고용되었던 사무노동자들이 주로 여성이었으며 이들이 ’연산자’를 의미하는 컴퓨터로 불렸던 역사적 사실을 드러내고 있다. 즉, 디지털사회와 지능정보사회의 기반기술인 컴퓨터는 역사적으로 직업, 혹은 이 직업에 종사하던 여성들을 지칭하는 개념이었다(Hayles, 2010). 이들은 소프트웨어가 없던 시절 수작업으로 계산과 데이터 처리 작업을 수행했다. 이들은 지금의 통계 분석과 유사하게 대형 계산기나 표와 같은 도구를 사용해서 수학적인 계산을 수행했으며, 구멍이 뚫린 펀치카드(punch card)를 사용해 정보를 입력하거나 출력하고, 이를 바탕으로 계산을 수행하는 등 데이터를 처리하는 작업을 수행했다. 데이터를 통해 패턴을 수집하고 분석하는 일 역시 이들 연산자들의 몫이었다. 당시 이들은 대부분 수학 분야의 배경을 가진 여성들이었는데 이는 반복적이고 사무적인 작업에 좀 더 성실한 노동자가 될 것이라는 젠더화된 발상의 영향이 컸다(Chun, 2004).\n컴퓨터라는 연산 직업 또한 처음부터 여성을 고용하지만 않았다. 가사 업무만을 주로 담당하던 여성들이 제2차 세계 대전 중에 남성들이 병력으로 고용되며 생긴 대규모 직업 공백을 채우는 역할을 담당하며 시작된 현상이었다. 하지만 ’기술개발’에 관련된 모든 업무를 대체했다기보다는 귀찮고 지루하며 반복적인 작업으로 여겨지던 연산직이 여성들에게 부과되었다(Light, 1999). 우리가 컴퓨터의 역사를 다룰 때 흔히 첫 번째 전자식 컴퓨터로 등장하는 애니악(Electronic Numerical Integrator and Computer, ENIAC)의 발명은 ’여성 컴퓨터’가 수행하던 계산 작업을 기계화하고 가속화하기 위해 만들어졌다. 이 때 계산의 역할은 좀 더 빠른 기계에게 전가되었지만 여전히 스위치와 전선을 조작해서 필요한 질문을 전자 컴퓨터에 수동을 입력하는 ’운영자’의 역할이 필요했고 이 업무가 다시 여성 컴퓨터들에게 부과되었다. 이들의 작업은 지금 우리가 프로그래머라고 부르는 사람들이 하는 일과 유사했지만 전자식 기계의 개발이 역사적 우선 과제였던 당시 상황에서 이러한 작업은 하찮게 여겨졌다. 기계 자체의 생산과 개발을 담당하는 남성 개발자들은 책상에 앉아 여성 컴퓨터들에게 ’명령’했고, 여성들에게 지시를 하달하던 것이 기계에게 내리는 명령으로 옮겨가면서 우리가 부르는 프로그래밍과 소프트웨어가 탄생했다(Chun, 2004).\n&lt;그림 3&gt; 첫 번째 전자식 컴퓨터인 애니악(ENIAC)을 운영하고 있는 여성 프로그래머들 (The National Women’s History Museum, 2015)\n\n\n\n\n\n\n\n\n\nSurus\n\n\n\n\n\n\n\nHanno\n\n\n\n\n\n기계 전반을 운용하던 여성 컴퓨터의 역할은 컴퓨터의 역사에서 매우 중요한 부분을 담당하지만 기계 자체를 개발했던 남성 개발자에 비해 비중있게 다루어지지 못했다. 이 같은 편향된 역사적 서술과 젠더화된 직업 인식은 전자식 컴퓨터를 지나 디지털 컴퓨터, 인공지능에 이르기까지 기술과 기계의 역사에 일종의 성별 분리를 가져왔다. 지금도 기술 기업에 종사하는 사람들을 떠올리면 지극히 남성 중심적인 이미지를 떠올린다. 앞서 살펴보았던 아마존의 인공지능 채용프로그램의 사례를 다시 살펴보면 이것이 사실임을 알 수 있다. 아마존이 인공지능 채용프로그램을 상상하기 시작한 계기는 전자상거래 플랫폼인 아마존이 자동화 시스템을 자사의 창고 관리와 이용자 제품 추천에 적용하면서 엄청난 이윤과 효용을 올렸기 때문이다. 이들은 같은 방식을 채용 시스템에 적용해서 이용자들에게 제품을 추천해주는 것과 동일한 방식으로 지원자들을 1점에서 5점까지 점수를 매기도록 설계했다. 이들이 아마존 근무자들의 10년치 이력서를 데이터로 입수하여 데이터 안에 있는 5만 가지 항목에 대해 통계 모형을 훈련시켰을 때 이들은 ‘무엇이 중요한 요소인지’ 금방 파악했다. 예를 들어, 프로그래밍 언어나 엔지니어링 관련 항목은 모든 사람들의 이력에 포함되어 있었기에 이에 대해 중요도를 낮게 평가했다. 대신 서류에 반복적으로 나타나는 특정 동사나 단어에 뚜렷한 선호도를 나타냈다. 하지만 시스템이 특정 동사나 단어에 선호도를 나타내는 만큼 특정 단어를 배제하기 시작했는데 대표적인 것이 ’여성’이라는 단어가 들어있는 이력서였다. 아마존은 한 차례 시스템을 개편하여 성별 언급의 영향을 배제하도록 수정했지만 단순한 성별 언급 외에도 한 쪽 성별에서 흔히 나타나는 언어나 단어 사용의 미묘한 형태를 파악하여 구분하기 시작했다(Dastin, 2018. 10. 11).\n앞서 살펴보았듯이 아마존의 인공지능 채용시스템 사례는 인공지능 시스템에 있어 훈련 데이터의 편향이 한 쪽 성별에 대한 불평등한 자원과 기회를 구성할 수 있다는 결정적인 결함을 발견하는 계기가 되었고 인공지능 윤리를 통해 차별 금지에 대한 원칙을 숙고하는 업계 및 사회 분위기를 만드는 데 일조했다. 하지만 이 사례는 또 한 편으로 의도하지 않게 기술 기업 내에서 역사적으로 이어져 온 다양성 결여를 가시적으로 드러내는 계기가 되었다. 이는 기술 기업 종사자의 성비 불균형을 진단하는 계기가 되었는데 2017년 2월 독일의 통계 포털 스타티스타가 주요 기술 기업(구글, 페이스북, 마이크로소프트, 페이팔, 이베이, 아마존, 트위터, 애플)의 남녀 임직원 비율을 집계한 결과를 보면 여성이 전체 종사자의 절반을 넘는 곳은 한 곳도 없다. 그나마 가장 높은 비율을 차지하는 페이팔의 경우도 44%에 그쳤고 마이크소프트는 26%로 가장 낮은 여성임직원 비율을 보였다. 그나마 여성의 비율이 높은 경우도 마케팅이나 디자인 업무에 많이 배치되어 있고 기술 개발 등의 업무에는 대부분 남성들이 근무하고 있었다. 나아가 유엔이 발표한 최신 수치에 따르면 전 세계적으로 이른 바 STEM(Science, Technology, Engineering, Mathematics)를 공부하는 여성은 전체 학생의 3분의 1밖에 되지 않으며 인공지능 분야 전공자는 이보다 더 적어서 전체 전문가 수의 5분의 1밖에 되지 않는 것으로 나타났다(UN News, 2023. 2. 10).\n이는 단순히 성별 문제만은 아니다. 국적, 인종, 학력 등 다른 사회적 범주로 기준을 확대하면 더욱 뚜렷한 편향이 나타난다. 이 같은 기술 기업 종사자의 특성 및 정체성 불균형은 기술을 개발하는데 다양한 시각을 포용할 수 없게 하는 한계를 낳는다. 국내외 인공지능 윤리원칙을 인공지능과 사회 불평등을 논의하는 맥락에서 앞서 살펴본 데이터 편향 최소화나 공평한 접근성 보장 이외에 다양성을 갖는 이해관계자들이 인공지능 시스템 개발 및 운용 전 과정에 직간접적으로 참여하여 협의할 수 있도록 권고하고 있다. 이는 정부, 기업, 학계, 시민 단체, 기술 전문가, 윤리 전문가, 소비자 등 다양한 위치의 이해관계자 뿐 아니라 이들을 구성하는 성별, 인종, 학력, 신체적 능력 등에 있어서도 다양성을 확보하는 것을 의미한다. 다양성을 갖는 이해관계자들이 참여함으로써 인공지능 개발과 적용에 대한 의사결정 과정에서 보다 다양한 관점, 이해, 경험을 이해하고 이들 간의 사회적 영향을 적절히 조화시키는 방향으로 기술 개발이 이루어져야 함을 시사하는 부분이다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>인공지능과 사회불평등</span>"
    ]
  },
  {
    "objectID": "ineq.html#나가며",
    "href": "ineq.html#나가며",
    "title": "5  인공지능과 사회불평등",
    "section": "5.5 나가며",
    "text": "5.5 나가며\n이 장을 시작하면서 인공지능과 사회 불평등을 숙고하는 작업은 우리가 살고 있는 시대의 인공지능 기술이 ‘누구를 위해’ 최적화되어 개발되었으며 그 과정에서 결정권을 갖고 있는 집단이 ‘누구인지’ 묻는 과정임을 강조했다. 인공지능 기술은 개발을 위한 자원, 운용을 위한 접근성, 전 과정에 연루된 다양한 의사 결정에 참여할 수 있는 기회에서 사회 불평등을 완전히 해결하지 못하고 있다. 인공지능 기술과 관련된 불평등의 문제를 해결하는 방법은 생각보다 복잡하다. 데이터의 편향을 극복하기 위해 데이터의 다양성을 늘리는 방식으로 기술 기업은 대응하고 있지만 결국 이 모든 기획에는 얼굴, 언어, 행동 등 외형적인 패턴이 인종, 성별과 같은 정체성의 ’차이’를 인식할 수 있는 근거가 된다는 매우 불평등한 전제에서 출발한다. 뿐만 아니라 이들이 데이터 다양성을 확보하여 완성하고자 하는 작업은 결국 매우 권위적인 체계에 의해 성별, 인종, 신용지수, 취향 등을 분류하는 작업이다. 이 제한적인 모형에 속하지 않은 이들은 있지만 없는 사람들이 되며, 이는 궁극적으로 기술과 기술이 주는 사회적 효용에 대한 접근성에 영향을 미친다. 인공지능 기술을 활용한 서비스가 사회적 편향을 가시화할 때마다 해당 기술기업은 잠시 서비스를 중단하고 정비하는 시간을 갖는다. 하지만 이들이 가져오는 해결책은 사회적 불평등을 해결하기 보다는 회피하는 전략을 갖는다. 새로운 버전으로 돌아온 챗봇 이루다는 인종이나 젠더 같은 질문에 잘 모르겠다고 대답하거나 그런 이야기는 하지 않는 것이 좋겠다고 대답한다. 유사한 위험을 감수하지 않기 위해 챗지피티는 남아프리카의 값싼 노동자를 고용하여 훈련데이터에 속한 인터넷 문서에서 혐오, 폭력, 자해, 성, 정치 등의 주제들을 모두 필터링했다. 지속적으로 유색인종에 대해 낮은 인식 정확도를 보였던 구글은 자사의 이미지 인식 프로그램에서 성별 라벨을 없애기로 결정했다. 이 같은 대응은 얼핏 보기에 적절하고 윤리적인 인공지능을 구현한 듯 보이지만 사회에 이미 내재하고 있는 불평등을 그저 회피할 뿐이며 이를 통해 인공지능의 분류체계에 속하지 못한 이들에 대한 지속적인 불평등을 양산하는 결과를 낳는다.\n인공지능 기술은 필연적으로 편향과 불평등을 갖는다. 불평등의 문제는 어디에나 있다. 하지만 이것은 기술 그 자체에서 기인하는 것도 아니며 기술 그 자체로 해결할 수 있는 것도 아니다. 국내외 인공지능 윤리기준에서 공통적으로 강조하고 있는 데이터 편향의 최소화, 동등한 접근성 보장, 다양한 이해관계자 참여 등의 사회 불평등 관련 원칙이 ‘어떤 형태로’ 구현되어야 할지, 그 결과물이 ‘누구를 위해’ 최적화 된 기술이며, 그 모든 결정 과정에 ‘누가 참여하고 있는지’ 끊임없이 질문하는 과정이 필요할 것이다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>인공지능과 사회불평등</span>"
    ]
  },
  {
    "objectID": "ineq.html#더-생각해-볼-거리",
    "href": "ineq.html#더-생각해-볼-거리",
    "title": "5  인공지능과 사회불평등",
    "section": "5.6 더 생각해 볼 거리",
    "text": "5.6 더 생각해 볼 거리\n\n&lt;표 1&gt;에서 제시한 국내외 조직 및 기관의 인공지능 윤리에서 ‘사회 불평등’ 관련 원칙을 다시 읽어보자. 각 인공지능 윤리 기준이 말하는 소수 집단이 누구를 지칭하는지 찾아보고 이들에게 ’불평등’이 어떤 형태로 나타나고 있는지 논의해보자.\n국내외 인공지능 윤리기준 책정에 참여한 사람들의 명단을 찾아보자. 이들의 성별, 인종, 국적, 학력, 종교, 신체적 능력 등의 다양성이 얼만큼 확보되어 있는지 스스로 진단해보고, 그것이 인공지능 윤리 및 기술 설계에 미칠 영향에 대해 논의해보자.\nEU의 인공지능 윤리기준은 신체능력이 일반 사람들과 다른 장애인에 대한 기술 접근성을 강조하고 있다. 동시에 EU 인공지능 윤리기준은 최대한 많은 사람의 편의와 접근성을 확보하는 기술 역시 권장하고 있다. 두 가지 윤리 원칙이 상충하고 있는 아래의 사례를 읽고 어떤 쪽이 좀 더 윤리적인 (인공지능) 기술을 구현하는 쪽이라고 생각하는지 논의해보자.\n\n사례:\n\n\n\n&lt;그림 4&gt; 전동 휠체어를 타는 중국 청년들(신경민, 2023. 7. 11.)\n\n\n2023년 7월, 중국의 사회관계망서비스 웨이보에는 20대 청년 여러 명이 전동 휠체어를 타고 거리를 돌아다니는 모습을 담은 사진이 게시되었다. 이 게시물의 제목은 “2000년생 이후 출생자의 출근법”이었는데, 이는 중국 청년들이 편리하고 경제적인 이유로 전동 휠체어를 구매하고 출퇴근 및 등교 목적으로 활용하는 사례가 증가하고 있다는 내용을 담고 있었다. 실제로 전동 휠체어를 판매하는 온라인 쇼핑몰에서는 나이나 건강 상태와 상관없이 휠체어를 구매하고 사용할 수 있는지에 대한 질문이 증가하고 있으며, 사회관계망 서비스에는 청년들의 휠체어 이용 후기도 자주 등장한다고 밝혔다. 현지 매체에 따르면 중국의 도로교통법이 전동 휠체어를 ’교통 수단’으로 규정하고 있지 않아 사람들이 다니는 인도에서도 자유롭게 사용할 수 있으며 헬멧을 착용할 필요가 없다. 또한, 전동 휠체어 기술은 인체공학적 설계에 중점을 두어 편안함을 강조하고 있어 청년들 사이에서 인기를 끌고 있는 것으로 나타났다. 특히, 충전 시간 대비 주행 시간이 길어 효율적으로 도시를 이동할 수 있다는 평가를 받고 있다. 그러나 이러한 휠체어의 대중화로 인해 생기는 윤리적인 고민도 있다. 휠체어는 원래 신체적 장애를 가진 사람들의 이동을 지원하기 위해 개발된 기술이었지만, 이를 건강한 청년들이 남용하는 상황이 법적인 제한 없이 진행되고 있으며, 이 같은 현상이 계속될 경우 도시 환경 및 전경의 변화, 신체적 장애를 가진 사람들의 휠체어에 대한 접근성 문제 등이 동시에 제기되고 있다.\n\n’공정한 알고리즘’이 어떤 모습일지에 대해 자신이 생각하는 모델을 구체적인 예시를 통해 생각해보고 논의해보자.\n사회 불평등과 관련된 인공지능 윤리는 윤리 기준의 책정보다 그것이 얼마만큼 이행되고 있는지 지속적으로 관리하는 체계가 더욱 중요하다. 인공지능 윤리기준에서 제시하는 데이터 편향 최소화, 동등한 접근성 보장, 다양한 이해관계자의 참여 확보가 어떤 형태로 이행되고 있는지 사례를 찾아보고, 그 사례가 사회 불평등에 어떤 방식으로 기여하는지 논의해보자.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>인공지능과 사회불평등</span>"
    ]
  },
  {
    "objectID": "ineq.html#더-읽을-거리",
    "href": "ineq.html#더-읽을-거리",
    "title": "5  인공지능과 사회불평등",
    "section": "5.7 더 읽을 거리",
    "text": "5.7 더 읽을 거리\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).\nCrawford, K. (2021). The atlas of AI: Power, politics, and the planetary costs of artificial intelligence. Yale University Press.\nEubanks, V. (2018). Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin’s Press.\nHicks, M. (2017). Programmed inequality: How Britain discarded women technologists and lost its edge in computing. MIT press.\n이희은. (2021). “기계는 권력의 지도” : AI와 자동화된 불평등. 문화과학, 105, 127-142.\n홍남희. (2023). 편향된 기술문화는 어떻게 작동해 왔는가: 한국 포르노그래피 규제 담론의 궤적. 컬처룩.\n김초엽, 김원영. (2021). 사이보그가 되다. 사계절.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>인공지능과 사회불평등</span>"
    ]
  },
  {
    "objectID": "ineq.html#참고문헌",
    "href": "ineq.html#참고문헌",
    "title": "5  인공지능과 사회불평등",
    "section": "5.8 참고문헌",
    "text": "5.8 참고문헌\n과학기술정보통신부 (2020). 사람이 중심이 되는 인공지능 윤리기준. https://www.msit.go.kr/bbs/view.do?sCode=user&mPid=112&mId=113&bbsSeqNo=94&nttSeqNo=3179742\n과학기술정보통신부 (2022). 디지털정보격차 실태조사 보고서.\n한국지능정보사회진흥원. (2021). 디지털 배움터. https://www.xn—2z1bw8k1pjz5ccumkb.kr/c/inner/153.do\n신경민. (2023. 7. 11). 건강한 청년들이 전동 휠체어에. 이유를 봤더니. MBC 뉴스투데이. https://imnews.imbc.com/replay/2023/nwtoday/article/6502075_36207.html\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).\nBuolamwini, J. A. (2017). Gender shades: intersectional phenotypic and demographic evaluation of face datasets and gender classifiers (Doctoral dissertation, Massachusetts Institute of Technology).\nCampanella, T. J. (2017. 7. 10). Robert Moses and his racist parkway, explained. Bloomberg. https://www.bloomberg.com/news/articles/2017-07-09/robert-moses-and-his-racist-parkway-explained\nCoded Bias (2020). Netflix Series.\nCrawford, K. (2021). The atlas of AI: Power, politics, and the planetary costs of artificial intelligence. Yale University Press.\nChun, W. H. K., Fisher, A. W., & Keenan, T. (2004). New media, old media: A history and theory reader. Routledge.\nDastin, J. (2018. 10. 11). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. https://www.reuters.com/article/idUSKCN1MK0AG/\nFernanda Medina Macaya, J., Ben Dhaou, S., & Cunha, M. A. (2021, October). Gendering the Smart Cities: Addressing gender inequalities in urban spaces. In Proceedings of the 14th International Conference on Theory and Practice of Electronic Governance (pp. 398-405).\nGrother, P., Ngan, M., & Hanaoka, K. (2019). Face recognition vendor test (fvrt): Part 3, demographic effects. Gaithersburg, MD: National Institute of Standards and Technology.\nOECD (2019). Recommendation of the Council on Artificial Intelligence. OECD Legal Instruments, OECD/LEGAL/ 35 0449. Paris: OECD. https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449\nEuropean Commission High-Level Group on Artificial Intelligence. (2019). Ethics Guidelines for Trustworthy AI. 37 Brussels, Belgium: European Commission. https://ec.europa.eu/futurium/en/ai-alliance-consultation.1.html\nUNESCO. (2021). Recommendation on the ethics of AI. https://unesdoc.unesco.org/ark:/48223/pf0000381137\n\nScheuerman, M. K., Paul, J. M., & Brubaker, J. R. (2019). How computers see gender: An evaluation of gender classification in commercial facial analysis services. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW), 1-33.\nLight, J. S. (1999). When computers were women. Technology and culture, 40(3),  455-83.\nHayles, N. K. (2010). My mother was a computer: Digital subjects and literary texts. University of Chicago Press.\nThe National Women’s History Museum (2015). Women and computing. https://www.womenshistory.org/articles/women-and-computing\nWinner, L. (2017). Do artifacts have politics?. In Computer ethics (pp. 177-192). Routledge.\nUN News. (2023. 2. 10). More women and girls in science equals better science, UN chief declares. https://news.un.org/en/story/2023/02/1133367#:~:text=Just%20one%20in%20three%20researchers,five%20professionals%20is%20a%20woman.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>인공지능과 사회불평등</span>"
    ]
  },
  {
    "objectID": "labor.html",
    "href": "labor.html",
    "title": "6  인공지능과 노동",
    "section": "",
    "text": "6.1 서론\n1800년대 초공장 자동화가 노동을 대체할거라고 생각하여 기계를 파괴한 러다이트(Luddites) 운동을 시작으로 기술 혁신은 역사적으로 근로자들 사이에 일자리 대체에 대한 불안을 불러일으켜 왔다. 로봇 등 일자리 자동화가 인간 노동을 대체할 것이라는 두려움은 일부 현실이 되기도 하고 기술 혁신으로 인해 다른 부문의 노동 수요를 자극하여 새로운 일자리를 만들어내기도 했다. 그렇다면 인공지능(최근 생성형 인공지능으로 인해 촉발된) 기술 혁신은 노동 시장에 어떤 영향을 미칠까?\n노동 시장에 대한 혁신의 효과를 정확하게 측정하는 것은 매우 복잡한 문제이다. 제2차 세계대전 이후 미국은 40년 동안 혁신으로 인해 노동 수요가 강화되었고, 이후 40년 동안 성장이 둔화되었다. 전기, 내연 기관과 같은 2차 산업 혁명의 주요 발명품은 복제할 수 없을 만큼 엄청난 경제적 영향을 미쳤다. 연구에 따르면 제품 혁신(새롭거나 향상된 제품/서비스)은 프로세스 혁신(제품/서비스 생성 방식의 개선)보다 생산성과 경제적 성과를 더 높이는 경향이 있다. 노동 시장을 연구하는 학자들에 따르면 노동력을 강화(labor-augmenting)거나 인간이 수행하지 않거나 수행할 수 없는 작업을 수행하는 데 초점을 맞춘 기술 혁신은 결국 총 노동 수요를 자극하여 더 많은 일자리의 창출로 이어진다고 주장한다. 이러한 혁신은 프로세스 혁신보다 제품 혁신과 더욱 밀접하게 연계된다고 보았다. 반대로, 인공지능과 같은 기존 인간의 작업 수행에 초점을 맞춘 노동 자동화 기술은 프로세스 혁신과 더 유사하며 총 노동 수요를 감소시킬 가능성이 더 높다.\n하지만, 혁신을 순전히 제품이나 프로세스, 노동 증대 또는 노동 자동화로 분류하는 것은 복잡한 문제를 너무 단순하게 푸는 것과 같다. 예를 들어, 자동차 자체는 제품 혁신일 뿐만 아니라 운송이라는 과정에 대한 프로세스 혁신이기도 하기 때문이다. 혁신이 노동에 미치는 영향은 대체(자동화를 통해 노동 수요 감소), 회복(새로운 작업 창출 및 그에 따른 노동 수요 창출), 생산성 효과로 분류된다. 노동 수요에 대한 혁신의 전반적인 효과는 생산성 효과의 규모에 따라 결정되는데, 이는 높은 임금의 노동을 혁신이 대체할 때 가장 강력하다고 할 수 있다.\n인류는 현재 인공지능(AI)이라는 거대한 혁명의 중심에 서 있다. 이 혁명은 단순히 기술 발전을 넘어 우리의 일상생활, 직업의 본질, 심지어 우리가 자신과 세계를 인식하는 방식까지 근본적으로 변화시키고 있다. 따라서 인공지능이 노동 시장에 미치는 효과는 개인의 생계, 기업의 운영 방식, 국가 경제의 구조에 이르기까지 매우 광범위할 수 밖에 없다. 이 변화를 이해하는 것은 단순한 호기심, 즉 ’인공지능이 어떤 직업을 대체할 수 있을까?’에 대한 답을 찾는 것을 넘어, 우리 모두에게 미래 사회를 준비하는 실질적인 필요성이 되다. 본 챕터에서는 ‘인공지능과 노동’이라는 주제를 통해 인공지능의 발전이 사회에 미치는 영향을 깊이 있게 탐구하고자 한다.\n먼저 인공지능에 대한 기본적인 이해부터 시작해보려고 한다. 인공지능은 컴퓨터나 기계가 인간의 지능적인 행동을 모방하고, 복잡한 문제를 해결하며, 의사결정을 내릴 수 있는 고도의 기술이다. ****이 기술은 이미 여러분의 일상 속 깊숙이 자리 잡고 있다. Siri와 Bixby와 같은 스마트폰의 개인 비서 기능부터 의료 분야의 진단 시스템, OTT의 맞춤형 콘텐츠 추천, 금융 서비스의 자동화된 고객 상담까지 다양한 형태로 우리 삶에 영향을 미치고 있다.\n하지만 인공지능이 인류에 가장 큰 영향을 주는 영역은 노동 시장일 것이다. 일부는 인공지능을 일자리를 파괴하는 위협으로 보지만, 다른 이들은 기존의 일을 혁신하고 새로운 기회를 창출하는 동력으로 여기곤 한다. 실제로 인공지능은 특정 작업을 자동화함으로써 일부 직업을 대체할 수 있으나, 동시에 새로운 직업을 창출하고, 기존 직업의 효율성을 높이며, 전례 없는 혁신을 가져올 수 있다.\n그러나 인공지능의 발전이 가져오는 변화는 단순히 기술적인 측면에 그치지 않는다. 우리는 어떤 일자리가 사라지고, 어떤 일자리가 변화할지, 그리고 새로운 일자리는 어떻게 생겨날지를 예측하고 대비해야 한다. 이 과정에서 정부, 기업, 교육 기관, 개인 등 모든 이해관계자의 역할이 매우 중요하다. 정부는 적절한 정책과 규제를 통해 인공지능의 긍정적인 측면을 촉진하고 부정적인 영향을 최소화해야 하며, 기업은 직원들의 재교육과 재취업 지원 프로그램을 제공하여 변화하는 시장 요구에 부응해야 한다. 또한, 교육 기관은 학생들에게 미래의 노동 시장에 필요한 기술과 지식을 제공하여 이러한 변화에 대비할 수 있도록 해야 한다. 마지막으로 우리 모두 개인 차원에서 이러한 변화를 단순히 관찰자의 입장에서 보는 것이 아니라, 적극적으로 이해하고, 준비하며, 올바른 방향으로 이끌어 갈 필요가 있다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "labor.html#서론",
    "href": "labor.html#서론",
    "title": "6  인공지능과 노동",
    "section": "",
    "text": "예를 들어, 의료 분야에서는 인공지능이 다양한 방식으로 활용되고 있다.인공지능은 환자의 위치를 정확하게 파악하고 CT 이미지를 재구성하는 데 도움을 주어 방사선과학과의 효율성을 향상시키거나, 심장 기능을 평가하는 데 사용되는 초음파 측정의 복잡성을 줄이는 데 도움을 주고 있다.1\n\n\n제조업에서는 인공지능이 생산 과정을 최적화하고 불량률을 줄이는 데 기여하고 있다.예를 들어, 지멘스(Siemens)는 인공지능과 기계 학습을 활용하여 산업 자동화와 데이터 분석을 개선하고 있고, IBM은 클라우드, 인공지능, 기계 학습 도구를 제공하여 생산 시간과 비용을 줄이는 데 도움을 주고 있다.2",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "labor.html#인공지능의-진보와-일자리-자동화",
    "href": "labor.html#인공지능의-진보와-일자리-자동화",
    "title": "6  인공지능과 노동",
    "section": "6.2 인공지능의 진보와 일자리 자동화",
    "text": "6.2 인공지능의 진보와 일자리 자동화\n\n6.2.1 인공지능 기술의 급격한 발전과 현재의 다양한 적용\n많은 사람들이 처음으로 인공지능이 대단하다고 느끼는 순간은 대체로 인공지능이 체스나 바둑과 같은 게임에서 인간 챔피언을 이기는 장면이었을 것이다. 예를 들어, 1997년 IBM의 ’딥 블루’가 세계 체스 챔피언 가리 카스파로프를 이겼고, 2016년에는 Google의 ’알파고’가 바둑의 세계 챔피언 이세돌을 이겼다. 물론 이세돌 9단은 알파고와의 4번째 대국에서 ’신의 한수’를 만들어 승리했지만, 이러한 순간들은 인공지능이 단순한 계산을 넘어 인간의 전략적 사고를 모방하고, 때로는 뛰어넘을 수 있는 능력을 가지고 있음을 전 세계에 알리는 신호탄이 되었다.3\n그러나 실제로 인공지능은 체스나 바둑과 같은 게임에서의 승리에 그치지 않고, 이미 우리 일상생활에 깊숙이 침투해 있다. 예를 들어, 음성 인식 기술을 통해 우리는 스마트폰이나 가정용 스마트 스피커에게 명령을 내릴 수 있다. Siri, Alexa, Google Assistant와 같은 AI 비서들은 단순한 명령 수행에서부터 일정 관리, 뉴스 업데이트 제공, 심지어 감성적인 대화에 이르기까지 다양한 작업을 수행하고 있다. 이러한 기술들은 우리의 생활 방식을 변화시키며, 더 편리하고 연결된 삶을 가능하게 한다.\n의료 분야에서 AI의 역할은 더욱 중요해지고 있다. AI는 환자 데이터를 분석하여 질병을 진단하고, 적절한 치료 방법을 제안하며, 심지어 새로운 약물을 개발하는 데 기여하고 있다. 예를 들어, 구글의 DeepMind는 ‘알파폴드’ 프로그램을 통해 단백질의 3D 구조를 예측하는 데 혁명적인 진전을 이루었다. 이러한 발전은 맞춤형 의학과 치료법의 개발을 촉진시켜, 더 효과적이고 개인화된 의료 서비스를 제공할 수 있게 한다.4\n자동차 산업에서는 자율 주행 차량의 등장이 가까운 미래의 현실로 다가오고 있다. Tesla, Waymo, Uber와 같은 회사들은 이미 도로에서 자율 주행 테스트를 진행하고 있다. 이 기술이 완전히 성숙되고 널리 보급되면, 교통 사고의 감소, 교통 체증의 해소, 운전자의 편의 증진 등 많은 긍정적인 변화를 가져올 것으로 기대된다. 자율 주행 차량은 또한 이동성에 제한이 있는 사람들에게 새로운 기회를 제공하고, 물류 및 배송 산업에도 혁신을 가져올 것이다.\n\n\n6.2.2 인공지능이 일자리와 직업 구조에 미치는 광범위한 영향\n일부 전문가들은 인공지능을 ’일자리 파괴자’로 보는데, 특히 반복적이고 예측 가능한 작업을 수행하는 직업들이 위험에 처해 있다고 지적한다. 예를 들어, 제조업에서는 로봇과 AI 시스템이 인간 노동자를 대체하여 더 빠르고 정확하게 작업을 수행할 수 있는데, 이러한 자동화는 생산성을 크게 향상시킬 수 있지만, 동시에 기존의 일자리를 줄일 수 있는 이중적인 특성을 가지고 있다. 고객 서비스 분야에서는 AI 챗봇이 기본적인 문의 사항을 처리하고, 효율적인 서비스를 제공하면서, 전통적인 콜센터 직원의 역할을 변화시키고 있다.5\n반면, 다른 전문가들은 인공지능을 ’기회의 창출자’로 보고, AI가 담당할 수 있는 반복적인 작업들을 자동화함으로써, 인간 노동자들이 더 창의적이고 전략적인 작업에 집중할 수 있게 될 것이라고 주장한다. 예를 들어, 프롬프트 엔지니어, 인공지능을 활용한 콘텐츠 기획자와 같은 새로운 직업들이 등장하고 있다. 이러한 직업들은 기술 발전의 이점을 활용하며, 경제와 사회에 새로운 가치를 창출한다. 또한, 의료 분야에서 AI가 의사와 간호사의 진단과 치료 결정을 보조함으로써, 의료 서비스의 질을 향상시키고, 환자에게 더 나은 치료를 제공할 수 있게 되었다. 이러한 변화는 기존의 직업을 더 효율적이고 효과적으로 만들며, 새로운 전문 분야와 기회를 창출할 것이다.6\n\n\n6.2.3 자동화가 가능한 일자리는 인공지능으로 인해 대체 가능하다\n일자리 자동화는 인공지능(AI)과 로봇공학의 발전으로 인해 특정 작업이 기계에 의해 수행되는 현상을 말한다. 이 현상은 제조업을 시작으로 의료, 금융, 서비스업에 이르기까지 다양한 분야에서 점점 더 두드러지게 나타나고 있다. 자동화의 가장 대표적인 예로는 자동차 제조업에서의 로봇 팔 사용을 들 수 있다. 이 로봇들은 사람보다 훨씬 빠르고 정확하게 차량을 조립하고, 무거운 부품을 들어 옮기며, 위험한 작업을 수행하여 인간 노동자의 부상 위험을 줄인다. 그러나 이러한 기계의 도입은 전통적인 조립 라인에서의 수많은 일자리를 줄이는 결과를 가져왔다. 사람들은 이제 더 복잡하고 창의적인 업무에 집중할 수 있게 되었지만, 동시에 많은 사람들이 기존의 일자리를 잃게 된 것이다.\n서비스업에서의 자동화는 주로 ’소프트웨어 로봇’이나 AI를 통해 이루어지고 있습니다. 예를 들어, 은행업계에서는 전통적인 창구 직원의 역할이 ATM과 온라인 뱅킹 시스템에 의해 대체되고 있다. 이러한 기계는 24시간 동안 고객의 요구에 응답할 수 있으며, 오류의 가능성을 줄이면서 많은 루틴 업무를 처리할 수 있다. 더 나아가, AI 기반의 챗봇은 고객 문의에 대응하고, 간단한 금융 상담까지 제공할 수 있는데, 이는 고객 서비스의 효율성을 높이고, 인간 직원이 더 복잡하고 전략적인 작업에 집중할 수 있게 한다. 그러나 이러한 변화는 전통적인 고객 서비스 직종의 일자리 수를 줄이는데 큰 기여를 하고 있다.\n\n\n6.2.4 자동화로 인해 기존 인간의 역할이 바뀔 것이다\n한편, 기존의 자동화가 가능한 일부 직업들은 인공지능에 의해 대체되기보다는 역할이 변화하는 형태로 진화할 것이다. 전통적으로 회계사는 기본적인 계산과 데이터 입력과 같은 반복적인 작업을 수행해왔다. 하지만 이제 이러한 작업은 인공지능에 의해 자동가 가능하다. 그렇다고해서 회계사라는 직업이 사라지지는 않는다. 오히려 회계사로 하여금 더 복잡한 분석, 전략적 의사결정 지원, 고객 상담과 같은 더 높은 수준의 작업에 집중할 수 있는 기회를 제공할 것이다. 인공지능은 데이터를 빠르게 처리하고 데이터 베이스에 저장할 수 있는 능력을 가지고 있지만, 그 데이터 안에서 의미 있는 정보를 분석하고 이를 사용자(회계사)에게 제공하는 능력은 제한적이다. 회계 전문가들은 이러한 인공지능의 능력을 활용, 기업의 방대한 데이터를 분석하여 중요한 통찰력을 얻을 수 있으며, 이 정보를 기반으로 보다 전략적인 결정을 내릴 수 있다. 더 나아가, 회계 전문가들은 이 과정에서 얻어진 자료를 바탕으로 조직의 자문 역할을 강화할 수 있게 된다. 특히, 블록체인 기술과 일부 AI 도구를 활용함으로써, 회계 감사의 안전성과 보안성이 크게 향상될 것으로 기대된다. 이러한 기술의 도입은 회계 업무의 효율성과 정확성을 높이는 동시에 조직 전반의 신뢰성을 증진시킬 것이다.이처럼 자동화는 기존 직업의 역할을 변화시키고, 직업을 보다 전문화되고 창의적인 방향으로 발전시킬 수 있는 잠재력을 가지고 있다.7\n\n\n6.2.5 취약한 직업군과 견고한 직업군\n인공지능의 위협을 가장 많이 받는 직업들은 일반적으로 높은 수준의 반복적 작업을 포함하고 예측 가능한 환경에서 일하는 직업들이다. 이러한 직업군에는 제조업 노동자, 콜센터 직원, 일부 사무직, 그리고 운전 기반의 직업들이 포함된다. 이들 직업에서는 AI나 로봇이 인간보다 더 빠르고 정확하게 작업을 수행할 수 있으며, 피로나 오류의 위험이 없다는 장점이 있다. 따라서 이러한 직업들은 인공지능이 대중화될 수록 사라질 위험이 높은 취약한 직업군이라고 할 수 있다.\n반면, 창의성, 복잡한 사회적 상호작용, 고도의 비판적 사고를 요구하는 직업들은 AI가 쉽게 대체하기 어려운 영역이다. 예술가, 과학자, (직관을 통해) 전략적 결정을 내리는 관리자 등은 인간만의 독특한 능력을 필요로 하는 영역에서 활동하며, 이러한 직업들은 AI의 발전에도 불구하고 여전히 중요하고 가치 있는 역할을 수행할 것이다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "labor.html#생성형-인공지능과-일자리",
    "href": "labor.html#생성형-인공지능과-일자리",
    "title": "6  인공지능과 노동",
    "section": "6.3 생성형 인공지능과 일자리",
    "text": "6.3 생성형 인공지능과 일자리\n파운데이션 모델(Foundation model)을 사용하여 텍스트, 이미지, 코드와 같은 새로운 콘텐츠를 생성하는 기술인 생성형 인공지능은아직 초기 단계이지만 노동 시장에 미치는 잠재적 영향은 상당할 것이다. 1980년 이후 혁신은 저임금 및 중임금 일자리 자동화에 중점을 두어 상대적으로 낮은 비용 절감으로 인해 생산성 효과가 제한되었지만 생성형 인공지능으로 촉박된 노동 시장의 반응은 고임금 근로자에게 더 높은 위험을 초래하고 잠재적으로 상당한 비용 절감으로 이어져 결과적으로 더 강력한 생산성 효과와 총 노동 수요를 증가시킬 수 있다는 점에서 기존의 기술 혁신발 노동 시장의 변화와는 결이 다를 것으로 예상된다.\n생성형 인공지능은 일반적으로 노동 수요 증가 가능성이 낮은 프로세스 혁신으로 분류된다. 이는 상당한 생산성 효과를 가져옴과 동시에 많은 직업이 사라질 수 있음을 시사한다. 범용 기술인 생성형 인공지능은 코드 작성부터 사기 탐지(Fraud detection)까지 다양한 산업과 작업에 걸쳐 광범위한 영향을 미치며 생산성 향상을 이루어낼 것으로 기대되고 있다.\n이때, 인구 통계학적 변수와 국가마다 다르게 진화하는 인공지능에 대한 제도들은 생성형 인공지능이 노동 수요에 미치는 영향을 다르게 만들 수 있는 요소들이다. 노동력 부족은 자동화를 장려하며, 이는 노동력이 노령화되거나 감소하는 국가에서 생성형 인공지능의 채택과 응용이 더 많아지고 결과적으로 생산성 효과가 더 커질 수 있음을 시사한다. 그러나 근로자 보호와 노조의 힘이 강한 제도를 가진 국가(또는 지역)에서는 인공지능의 노동 대체 효과가 약해 직업적 업무가 보다 순차적으로 전환되고 단순히 업무를 자동화하는 대신 기존 고용된 일자리 노동자들의 생산성 향상을 위해 생성형 인공지능을 사용하는 데 중점을 둘 것이다. 노동자 보호가 강화되면 비용 절감 효과가 낮아 국가 전반적인 생산성 효과가 감소할 수 있지만, 노동 시장에서 보다 균형감 있고 지속 가능한 전환으로 이어질 수도 있다.\n생성형 인공지능은 노동 시장에서 불평등(Inequality)에 영향을 미칠 수 있다. 고소득 직종과 산업이 가장 위험에 처해 있기 때문에 이들 근로자를 대체하면 처음에는 불평등이 줄어드는 것 처럼 보일 수도 있을 것이다. 연구에 따르면 생성형 인공지능을 도입하면 상위 1%에 속하는 사람들은 큰 영향을 받지 않을 수도 있지만 상위 1%와 10% 사이의 소득 격차는 줄어들 수 있다고 한다. 고소득 기술을 AI로 대체하면 저임금 기술에 대한 수요와 임금이 증가하여 임금 격차가 더욱 줄어들기 때문이다. 그러나 이것이 보편적인 불평등 완화로 이어질지는 미지수이다. 생성형 인공지능으로 대체되는 고소득 직종이 많이 분포한 지역은 경제적, 사회적으로 어려움을 겪을 수 있기 때문이다.8",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "labor.html#인공지능이-불러온-사회-변화-트렌드",
    "href": "labor.html#인공지능이-불러온-사회-변화-트렌드",
    "title": "6  인공지능과 노동",
    "section": "6.4 인공지능이 불러온 사회 변화 트렌드",
    "text": "6.4 인공지능이 불러온 사회 변화 트렌드\n\n6.4.1 고용 없는 노동의 현실화\n현대 산업에서 자동화와 인공지능의 급속한 발전은 노동 시장에 본질적인 변화를 가져오고 있다. 기계와 소프트웨어의 작업 복잡성 증가로 인해 다양한 산업에서 여러 수준의 자동화가 도입되고 있으며, 이러한 인공지능으로 촉발된 디지털 전환이 고숙련 일자리 증가 및 저숙련 일자리 감소로 이어지는 숙련 편향적 기술 변화를 초래하고 있다(Balsmeier & Woerter, 2019). 국내에서도 약 55~57%의 일자리가 향후 컴퓨터로 대체될 가능성이 높은 것으로 나타나며, 일부 연구에서는 이 비율이 70.6%에 달한다고 보고되었다(김세움 2015; 박가열 외 2016)\n자동화, 로봇화, 컴퓨터화의 발전은 반복업무기반(routine-based) 직업의 인력 수요 감소를 예측하며, 특히 교통, 유통, 사무직 등이 디지털 전환에 따라 높은 위협을 받는 분야로 분석됩니다. 미국에서는 약 47%의 일자리가 고위험군에 속한다고 보고되었다(Frey & Osborne, 2017). 그러나 동시에 프로그래밍, 데이터 분석과 같은 비반복적 인지 기반 직무의 일자리는 증가할 것으로 예측된다(Acemoglu & Restrepo, 2018).\n인공지능은 또한 노동의 질적 측면에서도 변화를 가져온다. 특히 플랫폼 노동의 증가는 새로운 형태의 일자리와 노동거래 방식을 등장시켰으며, 이러한 경제는 네트워크를 통한 직접적인 일자리 중개 및 배정에 관여한다. 플랫폼 노동은 서비스 또는 가상재화 거래, 디지털 플랫폼을 통한 일거리 구함, 대가나 보수 중개, 그리고 특정인이 아닌 다수에게 열려 있는 일감 중개 등을 특징으로 한다. 이러한 경제는 코로나19의 영향으로 더욱 활성화되었으며, 특히 광의의 플랫폼 종사자 수가 2021년 220만 명에서 2022년 292만 명으로 증가하기도 하였다(고용노동부, 2022).\n플랫폼 노동의 확대는 고용 없는 노동의 장기적인 경제성장과 효율성 증가 잠재력을 가지고 있지만, 이는 또한 사회 안전망의 변화와 교육 체계에 대한 새로운 요구를 제기한다. 플랫폼 노동자에 대한 보호 제도 관련 논의가 발전했지만, 전통적 일자리에서 플랫폼 일자리로의 이동은 노동시장 내 인력 수급 및 운용 문제를 야기할 수 있다. 이러한 노동 시장의 편차와 불균형에 대응하기 위해서는 인간의 삶의 질을 향상시키는 지속 가능한 발전을 중심으로 한 정책적 방안이 필요하다(민순홍, 2023).\n또한, 알고리즘에 의한 노동자 통제와 관련한 논의도 중요하다. 최근 인공지능 플랫폼 노동에 일감을 배분하고 통제하는 데 인공지능 알고리즘을 적용하는 형태가 등장했기 때문이다. 이는 다이내믹 프라이싱(Dynamic pricing)을 통한 이윤 극대화 목적으로 활용되지만, 노동과 관련한 공정성 및 투명성 측면에서 문제가 제기되고 있다(장진희·노성철·현종화, 2022).\n결론적으로, 디지털 전환에 따른 다양한 노동 변화에 대응하기 위해서는 정부와 기업 간의 적절한 정책적 협력이 필수적이다. 기술 자동화가 대체하는 것은 ’직업’이 아니라 ’직무’임을 인식하고, 새롭게 등장하는 일자리에 대한 다방면의 고려가 필요하다. 또한, 플랫폼 노동과 관련하여 윤리적인 기술 활용과 적절한 규제의 중요성을 인식하고, 이에 대한 논의를 적극적으로 이어나가야 한다.\n\n\n6.4.2 인간노동 대체의 소비시대\n현대 소비자는 편의와 개인화된 서비스를 요구하며, 특히 코로나19와 인공지능의 발달로 인한 비대면화와 온라인화는 디지털 전환을 가속화하고 있다. 이는 전통적인 소비 패턴의 변화뿐만 아니라, 새로운 서비스와 비즈니스 모델의 등장을 촉발시켰다. 인공지능과 로봇은 판매부터 로지스틱까지 다양한 분야에서 활용되며, IoT·AI의 발달로 즉각적이고 개인화된 소비자 참여가 가능해졌다. 이에 따라 구매 채널 방식도 옴니채널이나 온디맨드 서비스 등으로 전환되는 추세다(한국마케팅연구원, 2022).\n유통업계는 AI를 활용한 ‘초개인화’ 서비스를 선보이며 소비자의 취향과 생각을 분석해 상품과 서비스를 제공하고 있다. 이는 ‘나만을’ 위한 맞춤 상품 소비를 가능하게 하며, 기업들은 구매 전환율을 높이고 차별화된 쇼핑 환경을 제공함으로써 시장에서 경쟁력을 확보하고자 한다. 이러한 디지털 전환은 노동과 소비시장의 구조 변화를 가져온다. 알고리즘 소비의 상대적 유익성, 소비자가 알고리즘 시장에서 가지는 지배력, 비용 절감이나 효용 비율이 소비자의 후생에 미치는 영향 등이 중요한 요소다. 알고리즘 도입이 증가할수록 과거의 노동력 투입이나 인지적 노력보다 더욱 적은 비용으로 빠르고 효과적인 삶의 수행이 가능해진다.\n인공지능 알고리즘은 경제활동에서의 비효율을 축소해 소비효용을 개선할 것으로 전망된다. 이는 정보를 낮은 비용으로 분석해 선택 다양성을 확보하고, 소비자 의사결정을 보완하며, 개인 특성에 따른 편향성을 감소시키는 등의 방식으로 이루어진다(Picht and Freund ,2018). 디지털 전환에 따른 소비 패턴의 재편은 개인적 측면뿐만 아니라 지역 및 사회 전반에 영향을 미칠 수 있다. 개인별 맞춤 가격 설정은 사업자에게는 새로운 기회를 창출할 수 있으나, 소비자 잉여의 축소와 같은 부정적 측면도 있다(허민영, 임병권, 2021). 기술 발전과 정보량 증가에 따라 알고리즘이 개인의 지불의사가격에 근접해 가격을 설정하는 사업모델이 확대될 것으로 예상된다.\n이러한 변화는 사회적 책임과 윤리를 중시하는 새로운 요구사항을 제기한다. 개인화된 가격으로 인한 소비자 이익 감소, 가격의 공정성과 시장의 신뢰, 기업의 개인정보 활용 범위와 윤리성 등 새로운 소비자 문제를 검토할 필요가 있다. 또한, 인공지능 알고리즘이 수집하는 개인정보와 그것의 활용에 대한 윤리적, 규범적, 철학적 고민이 필요하다.\n인공지능 시대의 소비 패턴 변화에 대응하기 위해서는 기업과 정부 간 협력이 필수적이며, 소비자 보호를 위한 적절한 규제가 중요하다. 이러한 미래의 지능정보사회에서 나타날 법적 이슈는 소비자의 안전을 보장할 개인정보 보호 강화와 새로운 산업에 대한 규제 완화 요구라는 상호 모순된 법제적 측면이 병존할 수 있다. 인공지능 알고리즘을 활용한 시장의 발전은 정보규율에 의해 결정되며, 개인정보의 수집 및 활용과 보호 사이에 존재하는 상충관계를 균형잡는 것이 중요하다.\n\n\n6.4.3 유목적 관계의 시대\n글로벌화와 디지털 기술 발전은 국가 간 경제 통합을 촉진하고 글로벌 네트워크를 가능하게 했다. 이는 다문화와 다양성을 중요한 가치로 부각시키며 사람들 사이의 연결을 강화했다. 최근 글로벌 대기업들은 다양성을 중요한 가치로 여기며 ’다양성 보고서’를 작성하는 등 인력 구성에 있어 연령, 성별, 민족, 성적 취향 등의 다양성을 존중하고 있다. 매킨지리포트에 따르면 다양성이 높은 기업은 그렇지 않은 기업보다 재무 수익이 높다고 한다(McKinsey, 2015). 이는 다양성이 기업 경쟁력으로 이어지기 때문이다.\n원격 노동과 프리랜서 경제는 노동 시장의 유연성을 증대시켰다. 교육과 업무 스킬의 전세계적 적용은 인력 이동을 촉진하고, 이민과 인력 이동은 새로운 경제 기회를 제공한다. 디지털 시대에는 컴퓨터의 등장으로 일하는 방식이 근본적으로 변화했으며, 직원들은 서로 다른 연관된 업무를 비동시적으로 일할 수 있게 되었다. 이는 근무의 유연성을 높이고 공간에 대한 종속성을 완화시켜 ‘디지털 노마드’ 현상을 가져왔다(정민재·이정교, 2018).\n가상 커뮤니티와 사회적 네트워킹은 협력과 공유 문화의 확산을 촉진하며 사람들의 삶의 질을 향상시켰다. 이는 모바일 기반의 전자상거래의 편재성, 편리성, 즉흥성, 경제성 등에서 확인할 수 있듯이 시간과 공간의 제약에서 벗어나 다양한 가치를 제공하는 구매채널로 자리 잡았다(김경희, 2018).\n유목적 관계의 시대는 국제 협력과 다자간 협상의 필요성을 강조하며, 정보 보안과 개인 정보 보호가 중요한 이슈가 되었다. 클라우드, AI 등의 기술 적용과 비대면 업무 등이 일상화되어가고 있지만, 사이버 위협도 빠르게 증가하고 있다. 예를 들어 줌바밍 해킹 사건처럼 화상 회의 중에 해커가 침투하여 내부 정보를 빼돌리거나 불법 게시물을 올리는 사건이 문제가 되었다. 이러한 취약성은 회사 내부의 중요 자료 유출에 대한 우려와 통제되지 않는 위험성을 초래한다. 따라서 디지털대전환시대의 소비 패턴 변화에 대응하기 위해서는 기업과 정부 간 협력이 필수적이며, 소비자 보호를 위해 적절한 수준의 규제가 중요하다. 이와 더불어 인공지능 알고리즘을 활용한 시장의 발전은 정보규율에 의해 결정되며, 개인정보의 수집 및 활용과 보호 사이에 존재하는 상충관계를 균형잡는 것이 중요하다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "labor.html#논의와-대응책",
    "href": "labor.html#논의와-대응책",
    "title": "6  인공지능과 노동",
    "section": "6.5 논의와 대응책",
    "text": "6.5 논의와 대응책\n\n6.5.1 다양한 관점에서의 논의\n인공지능(AI)과 노동에 대한 논의는 사회의 다양한 분야와 각자의 전문 분야에 따라 다른 관점을 제공한다. 경제학자들은 인공지능이 경제 성장과 생산성, 그리고 노동 시장에 미치는 영향을 분석한다. 이들은 AI가 일자리를 대체할 가능성, 새로운 일자리 창출, 임금 격차와 같은 경제적 파급 효과를 연구한다. 사회학자들은 사회 구조와 노동 시장에 대한 변화를 탐구하며, 인공지능이 사회적 계층, 교육, 불평등과 어떻게 상호작용하는지를 분석한다. 철학자와 윤리학자들은 인공지능의 발전이 인간의 존엄성, 자율성, 윤리적 선택에 미치는 영향을 논하며, AI의 책임과 도덕적 한계에 대해 논의한다. 기술 전문가들은 AI 기술의 발전 가능성과 한계를 탐색하고, 새로운 혁신과 기술적 도전을 이해하기 위해 노력한다. 이러한 다양한 관점의 논의는 인공지능이 노동에 미치는 복합적인 영향을 이해하고, 보다 폭넓고 균형 잡힌 시각을 갖는 데 중요하며, 모든 관점에서의 포괄적 논의가 필요하다.\n예를 들어, 경제학적 관점에서는 일자리 자동화가 생산성을 향상시킬 수 있지만, 동시에 불평등을 심화시킬 수 있다는 점을 지적한다. 이는 특히 저숙련 노동자나 반복적인 작업을 수행하는 사람들이 AI에 의해 가장 먼저 영향을 받을 수 있음을 의미한다. 사회학적 관점에서는 인공지능이 노동 시장에서 일부 집단에게 더 큰 영향을 미칠 수 있으며, 이는 사회적 긴장과 분열을 야기할 수 있음을 강조한다. 예를 들어, 특정 지역이나 산업에서의 일자리 감소는 지역 경제에 심각한 영향을 미칠 수 있으며, 이는 사회적 불안정성을 증가시킬 수 있다는 것이다.\n\n\n6.5.2 정부, 기업, 교육기관의 역할\n인공지능과 노동의 상호작용에 대응하기 위해서는 정부, 기업, 교육기관 등 모든 이해관계자의 역할과 책임이 매우 중요하다. 정부는 적절한 정책과 규제를 통해 인공지능의 긍정적인 측면을 촉진하고 부정적인 영향을 최소화해야 한다. 이는 국가 차원에서의 전략적 계획을 포함하며, 인공지능 관련 정책, 법률, 윤리 지침을 개발하는 것을 의미한다. 예를 들어, 정부는 재교육 및 재취업 프로그램을 지원하여 노동 시장의 변화에 대비할 수 있도록 해야 하며, 인공지능의 윤리적 사용을 위한 법적 틀을 마련해야 할 것이다. 또한, 정부는 사회 안전망을 강화하여 기술 변화로 인해 일자리를 잃은 사람들이 생계를 유지할 수 있도록 지원해야 한다.\n기업은 인공지능을 도입하면서 발생할 수 있는 윤리적, 사회적 문제를 고려해야 한다. 이는 기업의 사회적 책임(CSR) 활동의 일환으로, 직원들의 재교육과 재취업 지원 프로그램을 제공하는 것을 포함한다. 기업은 기술 발전에 따른 직원의 직업 안전과 복지를 보장하기 위한 노력이 필요하며, 이를 위해 직원들과의 소통과 협력을 강화해야 할 것이다. 또한, 기업은 인공지능의 윤리적 사용을 위한 내부 지침과 프로토콜을 개발하고, 이해관계자들과의 협력을 통해 지속 가능한 방식으로 기술을 채택해야 한다.\n교육기관은 학생들에게 미래의 노동 시장에 필요한 기술과 지식을 제공하는 중요한 역할을 한다. 이는 단순히 기술적 기술을 가르치는 것뿐만 아니라, 비판적 사고, 창의성, 사회적 기술과 같은 보편적 기술을 강조하는 것을 포함한다. (이 책이 그러한 역할을 하기 바란다). 교육기관은 평생 학습의 중요성을 강조하고, 학생들에게 유연한 학습 기회를 제공함으로써 빠르게 변화하는 노동 시장에 대응할 수 있게 해야 한다. 이는 전통적인 교육 시스템을 넘어서 온라인 학습, 직업 훈련 프로그램, 계속 교육 과정 등 다양한 형태의 교육을 포함할 수 있다.\n\n\n6.5.3 인공지능과 노동의 미래에 대한 심층적 전망\n인공지능(AI)과 노동의 상호작용은 앞으로 수십 년 동안 우리 사회와 경제에 광범위하고 깊이 있는 영향을 미칠 것이다. 이 변화의 파도는 일부 직업을 사라지게 하거나 근본적으로 변화시킬 것이며, 동시에 새로운 직업과 기회를 창출할 것으로 보인다. 이러한 변화는 우리에게 도전을 제시할 뿐만 아니라, 적절하게 대응한다면 무한한 가능성을 열어줄 기회로도 작용할 수 있다. 즉, 인공지능의 발전은 생산성 향상, 혁신 촉진, 삶의 질 개선과 같은 긍정적인 잠재력을 가지고 있는 동시에 불평등의 심화, 직업 안정성의 감소, 윤리적 및 사회적 문제와 같은 도전도 함께 제기하고 있다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "labor.html#더-읽을-거리와-생각해-볼-문제",
    "href": "labor.html#더-읽을-거리와-생각해-볼-문제",
    "title": "6  인공지능과 노동",
    "section": "6.6 더 읽을 거리와 생각해 볼 문제",
    "text": "6.6 더 읽을 거리와 생각해 볼 문제\n\n6.6.1 더 읽을 거리: 2023년 미국 작가 조합 파업\n\n2023년 5월 2일부터 9월 27일까지 148일 동안 진행된 미국 작가 조합 파업은 인공지능과 노동의 관계에 대한 중요한 시사점을 제공한다. 파업의 주요 쟁점은 인공지능의 발전으로 인해 작가들의 일자리가 위협받을 수 있다는 우려였다. 작가 조합은 인공지능을 활용한 콘텐츠 제작을 규제하고, 작가들의 최저임금을 인상하는 등의 요구를 했다.\n파업은 148일 만에 잠정 합의로 종결되었다. 합의안에는 인공지능을 활용한 콘텐츠 제작에 대한 규제 강화, 작가들의 최저임금 인상, 작가들의 단체교섭권 강화 등이 포함되었다.\n이 파업은 인공지능이 노동시장에 미치는 영향에 대한 노동자들의 우려를 반영한 것이다. 인공지능은 노동의 생산성을 높이고 새로운 일자리를 창출하기도 하지만, 기존 일자리를 대체하기도 한다. 따라서 인공지능과 노동의 관계를 조화롭게 발전시키기 위해서는 인공지능의 부정적 영향에 대한 대책을 마련하는 것이 필요하다.\n2023년 미국 작가 조합(WGA) 파업은 디지털 시대의 복잡한 노동 문제를 상징하는 중요한 사건이다. 이 파업은 OTT 시장의 확장과 작가들의 업무량 증가에도 불구하고 임금이 줄어들었다는 불만에서 시작되었다. 이와 관련하여, 작가들은 재방료 기준 확립, 원고료와 출연료 인상, AI를 통한 인력 감축 철회 등을 요구했다. 특히, AI 기술의 발전이 각본 작업에 미치는 영향과 OTT 드라마의 재방료 문제는 논쟁의 핵심이었다.\n\n\n\n6.6.2 인공지능과 노동의 쟁점\n인공지능과 노동의 관계에 대한 쟁점은 크게 두 가지로 나눌 수 있다.\n\n첫 번째 쟁점은 인공지능이 노동시장에 미치는 영향에 대한 것이다. 인공지능이 노동의 생산성을 높이고 새로운 일자리를 창출하기도 하지만, 기존 일자리를 대체하기도 한다. 따라서 인공지능이 노동시장에 미치는 영향에 대한 분석은 매우 중요하다.\n두 번째 쟁점은 인공지능과 노동의 관계를 조화롭게 발전시키기 위한 방안에 대한 것이다. 인공지능과 노동의 관계를 조화롭게 발전시키기 위해서는 인공지능의 부정적 영향에 대한 대책을 마련하는 것이 필요하다. 또한, 인공지능을 활용한 새로운 일자리를 창출하기 위한 노력도 필요하다.\n\n\n\n6.6.3 생각해 볼 문제: 2023년 미국 작가 조합 파업과 관련하여\n기술 발전과 창작 환경: 2023년 미국 작가 조합 파업에서 작가들은 AI의 사용을 제한하고, OTT 드라마의 재방료 문제를 제기했다. 이러한 요구는 기술 발전이 창작 환경에 어떤 영향을 미치는지에 대한 중요한 질문을 제기한다.\n\n기술의 발전이 창작자의 권리와 수익을 어떻게 변화시키고 있는지, 그리고 이에 대한 공정한 해결책은 무엇이라고 생각하는가?\n\n\n6.6.3.1 디지털 시대의 재방료와 로열티\n\nOTT 시대의 등장으로 전통적인 재방송 개념이 사라졌고, 작가들의 수익 구조에 변화가 생겼다. 이러한 변화 속에서 작가와 창작자들의 재방료와 로열티를 어떻게 보호하고 보장해야 할까?\n또한, OTT 플랫폼이 작품별 조회수와 같은 데이터를 공개하지 않는 것에 대해 어떻게 생각하는가? 노동 운동과 기술 변화에 대한 대응: 미국 작가 조합의 파업은 기술 변화에 대응하는 노동 운동의 중요한 사례이다. 미래에 기술의 변화가 더욱 가속화될 것으로 예상될 때,\n노동 운동이나 기타 사회적 운동이 이러한 변화에 어떻게 효과적으로 대응해야 한다고 생각하는가?\n또한, 노동자의 권리 보호를 위해 어떤 전략이 필요할까?\n\n\n\n6.6.3.2 Food for thoughts\n\n2023년 미국 작가 조합 파업의 결과는 인공지능과 노동의 관계에 어떤 영향을 미쳤을까?\n인공지능이 노동시장에 미치는 영향에 대해 긍정적, 부정적 측면을 각각 생각해 보자.\n인공지능과 노동의 관계를 조화롭게 발전시키기 위한 방안을 제시해 보자.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "labor.html#부록",
    "href": "labor.html#부록",
    "title": "6  인공지능과 노동",
    "section": "6.7 7. 부록",
    "text": "6.7 7. 부록\n본 챕터는 아래 노션 페이지에서 확인 가능하며, 계속해서 내용이 수정, 보완될 예정입니다. 내용과 문법에 오류가 있으면 코멘트 달아주시고, 아래 메일 주소로 보내주시면 감사하겠습니다.\nhttps://www.notion.so/cjleeskku/32a45c406c9d4390b90df16fe2a14641?pvs=4\n메일 주소: changjunlee@skku.edu",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "labor.html#참고문헌",
    "href": "labor.html#참고문헌",
    "title": "6  인공지능과 노동",
    "section": "6.8 8. 참고문헌",
    "text": "6.8 8. 참고문헌\n길은선·송영진·신위뢰 (2019). &lt;고용 없는 성장의 특성 및 산업별 분석&gt; (연구보고서 2019-913). 산업연구원.\n김세움 (2015). &lt;기술진보에 따른 노동시장 변화와 대응&gt;. 한국노동연구원.\n민순홍 (2023). &lt;플랫폼 노동 선택의 결정 요인과 플랫폼 종사자의 직업 이동 경로 분석&gt; (연구자료 2023-01). 산업연구원.\n박가열‧천영민‧홍성민‧손양수 (2016). &lt;기술변화에 따른 일자리 영향 연구&gt;. 한국고용정보원.\n이금노 (2018). &lt;인공지능 알고리즘 기반 경제에서의 소비자문제 연구&gt; (정책연구 18-17). 한국소비자원.\n이문호 (2020). 4차 산업혁명을 둘러싼 쟁점들 -’노동사회학적 관점’에서-. 노동연구, 40, 47-86.\n장진희·노성철·현종화 (2022). &lt;플랫폼노동의 알고리즘 현황과 대응방안 - 알고리즘의 공정성과 투명성, 노동자 통제를 중심으로&gt; (연구총서 2022-08). 한국노총중앙연구원.\n최병록 (2017). &lt;4차 산업혁명시대의 소비자이슈와 소비자정책&gt;. 한국기술혁신학회 추계학술대회.\n한국마케팅연구원 (2022). AI 기술과 초개인화 서비스. &lt;마케팅 2022&gt;, 56권 8호, 26-36.\n허민영·임병권 (2021). &lt;코로나 이후 디지털 전환 가속화에 따른 소비자정책 방향 연구&gt; (정책연구 21-01). 한국소비자원.\nAcemoglu, D., & P. Restrepo. (2018). The race between man and machine: Implications of technology for growth, factor shares, and employment. American Economic Review, 108(6), 1488-1542.\nBalsmeier, B., & M. Woerter. (2019). Is this time different? How digitalization influences job creation and destruction. Research Policy, 48(8), 103765. https://doi.org/10.1016/j.respol.2019.03.010.\nCharles, K.K., E. Hurst, & M.J. Notowidigdo (2013). Manufacturing decline, housing booms, and non-employment, Technical Report, NBER Working Paper, 18949, National Bureau of Economic Research.\nFrey, C.B. and M.A. Osborne (2017), “The future of employment: How susceptible are jobs to computerisation?,” Technological Forecasting & Social Change, 114, 254-280.\nGal, M., & Elkin-Koren, N. (2017). Algorithmic Contracts.Harvard Journal of Law and Technology,30, 309.\nPicht, P. G., & Freund, B. (2018). Competition (law) in the era of algorithms.Max Planck Institute for Innovation & Competition Research Paper, (18-10).",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "labor.html#footnotes",
    "href": "labor.html#footnotes",
    "title": "6  인공지능과 노동",
    "section": "",
    "text": "10 real-world examples of AI in healthcare↩︎\n11 AI in Manufacturing Examples to Know | Built In↩︎\nHow IBM’s Deep Blue Beat World Champion Chess Player Garry Kasparov Google’s AlphaGo wins final Go game against Lee Sedol↩︎\n딥마인드, 단백질 생성 AI ‘알파폴드’ 최신 버전 공개↩︎\nAI in Manufacturing: How It’s Used and Why It’s Important for Future Factories The Top 5 Benefits of Using Chatbots in Customer Service Teams↩︎\n10 new jobs created with AI in the workplace↩︎\n[Biz Focus] AI가 회계사 대체한다고?…오히려 귀한몸 된다 - 매일경제↩︎\nGenerative AI and the labor market: A case for techno-optimism↩︎",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>인공지능과 노동</span>"
    ]
  },
  {
    "objectID": "copy.html",
    "href": "copy.html",
    "title": "7  인공지능과 저작권 -무엇을 보호해야 하는가",
    "section": "",
    "text": "7.1 인공지능 시대, 저작권의 본질부터 고민해야 한다\n인공지능 시대에 저작권은 무엇을 보호해야 하는가? 인공지능이 초래하는 변화는 기존의 변화와는 차원이 다른 근본적 변화라고들 한다. 변화의 시기일수록 본질을 파고들어야 한다. 인공지능 시대에 저작권은 무엇을 보호해야 하는지, 이 어려운 질문에 답하려면, 저작권이 무엇인지 ’저작권의 본질’에 대한 고민부터 선행되어야 할 것이다.\n저작권이라는 개념은 언제부터 생겼을까? 구석기 시대에 동굴에 벽화를 그리던 사람들도 ‘이 그림은 내 그림’ 이라는 인식이 있었을까? 그리스 철학자들은 ’지식’을 인간의 것이 아니라 신의 계시나 선물로 생각하여 소유나 거래의 대상으로 보지 않았다고 한다. 소크라테스나 플라톤은 교사나 정치인이지 ’작가’는 아니었다는 것이다. 이러한 관점에서 ’저작권’이라는 개념은 인쇄술이 보급되면서 시작되었다고 보는 견해가 있다. 출판업자라는 이익집단이 생기면서 그 권리를 보호하기 위해 저작권이라는 개념이 등장하였다는 것, 즉 당초 출판업자 보호로 생긴 ’저작권’의 개념이 입법을 통해 ’저작자의 저작물 보호’로 발전해왔다는 것이다. 우리가 여기서 기억할 부분은, 저작권의 개념 역시 고정불변의 것이 아니라 시대와 역사에 따라 변화하는 개념이라는 점이다. 인공지능이 저작물에 대해 근본적 변화를 가져오는 기술일까? 저작권 개념 역시 근본적으로 그 개념부터 재정의해야 할까?\n이 문제에 대해 답하기 앞서 저작권의 사전적 의미부터 찾아보자. ‘저작권’에서 ‘저작’을 국어사전에서 찾으면 ‘예술이나 학문에 관한 책이나 작품 따위를 지음. 또는 그 책이나 작품’이라고 정의되어 있다. 프랑스에서 저작권을 ‘droit d’auteur’, 독일은 ‘Urheberrecht’라는 용어를 쓰는데, 이는 우리나라와 마찬가지로 ‘저작자의 권리’, 즉 author’s right에 좀 더 방점이 찍힌 개념이라고 할 수 있을 것이다. 반면 미국에서 말하는 ‘copyright’은 ‘copy’에 대한 권리, 즉 재산권이 좀 더 강조된 개념으로 이해된다. 물론 오늘날 저작권은 ‘저작자의 권리’와 ‘재산권’ 모두를 아우르는 개념이다. 저작권은 ’저작인격권’과 ’저작재산권’을 포함하는 개념으로 이해된다. 그러나 저작권에서도 저작인격권과 저작재산권 중 어떠한 권리에 더 비중을 둘 것인지에 따라 인공지능 시대 저작권 보호방향에 관하여도 입장 차이가 발생할 수 있지 않을까?",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>인공지능과 저작권 -무엇을 보호해야 하는가</span>"
    ]
  },
  {
    "objectID": "copy.html#기초-개념-이해하기-저작권-무엇이-보호되는가",
    "href": "copy.html#기초-개념-이해하기-저작권-무엇이-보호되는가",
    "title": "7  인공지능과 저작권 -무엇을 보호해야 하는가",
    "section": "7.2 기초 개념 이해하기: 저작권, 무엇이 보호되는가?",
    "text": "7.2 기초 개념 이해하기: 저작권, 무엇이 보호되는가?\n\n7.2.1 저작권이란 무엇인가?\n대한민국 저작권법은 1957년 1월 제정되었다. 당시 저작권법 제1조는 “본법은 학문적 또는 예술적 저작물의 저작자를 보호하여 민족문화의 향상발전을 도모함을 목적으로 한다”고 저작권법의 목적을 밝혔다. 지금은 어떨까? 2023년 5월 현재 시행 중인 저작권법 제1조는 “이 법은 저작자의 권리와 이에 인접하는 권리를 보호하고 저작물의 공정한 이용을 도모함으로써 문화 및 관련 산업의 향상발전에 이바지함을 목적으로 한다”고 저작권법의 목적을 밝히고 있다. 저작권법 제정와 비교하면, ’저작물의 공정한 이용을 도모’하면서도 ’산업의 향상 발전’을 명시적 목표로 삼고 있다는 점이 눈에 띈다. 저작물을 널리 공중이 이용하게 하는 일과 저작자의 권리를 보호하는 일은 늘 긴장관계에 있게 마련이다. 저작권법은 저작자의 권리를 보호하면서 저작물이 널리 이용되게 하고, 저작권 관련 산업 뿐 아니라 저작물을 자유로이 이용하여 산업 발전에 기여하는 일 역시 저작권법의 입법 목적으로 삼고 있다. 저작물에 대한 접근을 지나치게 통제하면 과학기술 발전이나 문화 증진이 저해될 우려가 있고, 재산권적 성격과 저작물의 산업적 성격을 강조하면, 저작자와 저작물 이용자보다 그 매개자가 이득을 보게 될 수도 있다. 저작권법의 목적은 모두 놓칠 수 없는 중요한 목적이지만, 저작권법이 왜 필요하고 무엇을 보호해야 할지, 각자 한 번씩 생각해보자\n\n\n7.2.2 저작권보호의 근거\n저작권보호의 근거는 크게 ‘권리보호의 관점’과 ‘동기 부여의 관점’에서 구분하여 설명되기도 한다. 권리보호의 관점 역시 두 가지 갈래로 나뉠 수 있다. 첫째로, 작가의 창작 노력에서 저작권의 인정 근거를 찾는 견해는, ‘sweat of brow’, 즉 인간의 노력에 대한 보상으로서 저작권을 보호해야 한다고 본다. 이러한 견해를 가진 대표적 사상가는 존 로크이다. 다른 갈래로, 저작물을 인격의 연장선으로 보아 보호하는 관점이 있다. 헤겔로 대표되는 이러한 견해는, 이성에 기초한 지적생산물은 자아의 표현이고 그렇기에 보호될 필요가 있다고 본다. 다만 인격의 연장으로 보는 견해는, 기업의 업무상저작물을 보호하는 문제를 설명하기 어려운 측면이 있다. 한편, 동기부여의 관점에서 저작권을 접근하는 입장에서는, 창작자가 창작물을 세상에 공표하게 하는 인센티브가 필요하므로 저작권을 보호해야 한다고 본다. 저작권 보호 근거에서 무엇을 더 중요하게 보는지에 따라 AI 저작물의 보호에 대하여도 관점이 달라질 수 있을 것이다. 인간의 지시로 AI가 저작물을 만들어 내는 방식으로 창작 과정이 바뀔 경우, 인간의 노력에 대한 보상으로서 저작권을 보호해야 한다는 견해가 여전히 유지될 수 있을 것인가? 창작물을 세상에 공표하게 하는 인센티브로서 저작권을 보호해야 한다면, AI를 이용한 저작물도 보호하는 것이 타당한가? 저작권보호의 근본적인 근거가 무엇인지 고민해보고, 인공지능 시대에 저작권보호의 근거가 그대로 유지될 수 있을지 각자 생각해보자\n\n\n7.2.3 저작물이란 무엇인가\n저작권법은 저작물을 ’인간의 사상 또는 감정을 표현한 창작물’로 정의한다(제2조). 즉, ① 인간의 사상과 감정을 표현한 ②창작물이어야 한다. 두 가지 요소로 구분하여 저작물의 개념을 살펴보자. 먼저 ’인간의 사상과 감정을 표현’한 것이어야 한다. 저작권은 idea가 아니라 표현을 보호한다. 피카소의 입체주의라는 그림의 사상이 있다고 하자. 그러나 ’입체주의’는 그 자체로 저작권으로 보호되지 않는다. 그에 기반한 그림에 저작권이 발생하는 것이다. 표현을 보호하는 것이므로 단순한 사실관계의 나열도 원칙적으로는 저작물이 되기 어렵다. 다만 사실을 어떻게 배열하는지에 창작성이 인정되어 저작물로 인정될 수 있을 뿐이다. 예를 들어 뉴스기사나 요리법, 설명서는 단순한 사실관계만 나열하였다면 설령 그러한 사실 자체를 모으는데 상당한 노력을 기울였다고 하더라도 저작권 보호 대상은 되지 않는다. 저작물은 단순히 인간의 노력에 대한 보상으로서만 보호되는 것이 아닌 셈이다. 또한 저작물은 ’창작성’이 있어야 한다. 이 때 창작성은 어느 정도 갖추어져야 할까? 대법원은 “창작성이란 완전한 의미의 독창성을 말하는 것은 아니며 단지 어떠한 작품이 남의 것을 단순히 모방한 것이 아니고 작자 자신의 독자적인 사상 또는 감정의 표현을 담고 있음을 의미할 뿐이어서 이러한 요건을 충족하기 위하여는 단지 저작물에 그 저작자 나름대로의 정신적 노력의 소산으로서의 특성이 부여되어 있고 다른 저작자의 기존의 작품과 구별할 수 있을 정도이면 충분하다(대법원 2012다28745)”고 기준을 제시한 바 있다. 즉, 독창성이 있어야 하지만, 창작성의 기준은 높은 수준이 아닌 낮은 수준이면 족하다. 창작적 가치가 높은지 낮은지에 따라 저작물 보호 여부가 결정되는 것은 아니라는 의미이다. 이러한 창작성의 판단 기준은 앞으로도 계속 유효할까? 인간이 창작물을 직접 만들어내던 과거와 달리 인공지능 시대에는 극히 최소한의 노력으로 저작물이 대량 생산될 가능성이 더욱 높아졌다. 이런 시대적, 기술적 환경 속에서도, ’나름의 정신적 노력의 소산’이 부여되고 ’기존 작품과 구별되는 정도’면 저작물로 지위를 부여하기 충분하다고 볼 수 있을까? 각자 고민을 해보자.\n\n\n7.2.4 저작자란 누구인가\n저작권법은 저작자를 ‘저작물을 창작한 자’로 정의한다(제2조 제2호). 창작을 제안하고, 동기를 부여하고, 자료 연구를 지시하고 장면을 제안하고 감수하여 의견을 제시하여 저작물이 완성되는데 기여하였다고 하더라도, 실제로 표현해서 창작한 사람이 아니면 저작자나 공동저작자의 지위가 인정되지 않는 셈이다. 쉽게 말해서, 편집자가 소설을 읽고 ‘이런 방향으로 써보시면 어떤지?’ 의견을 제안했다고 해도, 편집자가 컴퓨터 자판기로 직접 해당 소설의 부분을 집필한 것이 아닌 이상, 편집자는 소설의 저작자 지위가 부여되지 않는다. ’누가 실제로 표현하여 창작했는가’는 인공지능시대에 더욱 중요한 쟁점이 될 것이다. 인간은 인공지능에게 ’명령’을 하고, 창작 행위 그 자체에 관여하지 않는다면, 이를 인간의 저작물이라고 볼 수 있을까? ’이러이러한 그림을 그려줘’라고 지시하여 인공지능이 그림을 완성하였다면, 현재의 ’창작자 원칙’에 따르면 인공지능이 만들어 낸 그림은 인간의 저작물이라고 볼 수 없을 것이다.\n그러나 ‘창작자 원칙’은 고정불변의 개념은 아니다. 예를 들어 저작권법은 ‘업무상저작물’ 개념을 두고 있다. 법인등의 명의로 공표되는 업무상저작물의 저작자는 계약 또는 근무 규칙 등에 다른 정함이 없는 때에는 그 법인이 저작자가 된다(제9조). 이러한 업무상저작물 개념은 일본 저작권법에서 유래하여 우리나라 저작권법에도 도입된 것인데, 그 결과 고용관계에서 창작된 저작물은 업무상저작물로 인정하되 종업원에게는 그에 상응하는 보상을 받을 수 있도록 제도가 설계되었다. 사용자와 피용자간 형평의 문제가 제기될 수도 있겠지만, 저작물 창작 과정에 대규모 자본이 투자되고 관여하는 당사자가 다수인 경우라면 투자자본의 회수나 저작물 이용의 편리성을 고려할 때 업무상저작물 개념이 필요한 것이다. 유발하리리는 &lt;사피엔스&gt;라는 ‘법인’이라는 개념이 인류의 독창적 개념이었다고 설명한 바 있다. ‘업무상저작물’도 ‘만들어진’ 개념인 것처럼, 기존의 저작물 개념으로는 인공지능 시대의 저작물을 정의하기 어렵다면, 새로운 저작물 개념을 고안해내야 할 수도 있을 것이다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>인공지능과 저작권 -무엇을 보호해야 하는가</span>"
    ]
  },
  {
    "objectID": "copy.html#기초-개념-이해하기-저작권-무엇이-보호되는가-1",
    "href": "copy.html#기초-개념-이해하기-저작권-무엇이-보호되는가-1",
    "title": "7  인공지능과 저작권 -무엇을 보호해야 하는가",
    "section": "7.3 기초 개념 이해하기: 저작권, 무엇이 보호되는가?",
    "text": "7.3 기초 개념 이해하기: 저작권, 무엇이 보호되는가?\n\n7.3.1 사진과 저작권\n사진이 처음 등장했을 때, 미술평론가는 사진 기술을 “예술의 가장 치명적인 적”으로 비난했다고 한다. 사진이 회화를 대체할 수 있을지, 사진도 회화처럼 예술로 볼 수 있을지 논의들이 있었을 것이다. 인공지능의 등장 역시 예술의 치명적인 적으로 평가될까? 사진이 등장하였을 때 저작권 논쟁을 통하여, 당시의 관점과 판단이 오늘날 어떤 함의를 줄 수 있을지 생각해보자.\n오스카와일드를 찍은 사진이 저작물로 보호될 수 있을까? 1880년대에 논란이 있었다. 이른바 Burrow-Giles v. Sarony 사건에서 “사진은 예술이라기보다 기계적 과정일 뿐이며 작가의 아이디어를 구현할 수 없다”는 주장에 대해 법원은 해당 사진은 “미국 뉴욕이라는 공간에 찾아오게 해서 사진기 앞에서 자세를 취하게 했고, 의상, 주변 휘장과 악세서리를 선별 배치했으며, 조명과 그림자를 조정하고 노출을 조절한 상태에서 촬영을 해서 얻은 결과물”이고, “창작성 요소를 고안한 상태에서 사진을 찍었으므로 저작권자”라고 판단했다. 오늘날 사진과 관련하여서는 피사체의 선정, 구도의 설정, 빛의 방향과 양의 조절, 카메라 각도의 설정, 셔터의 속도, 셔터찬스의 포착, 기타 촬영방법, 현상 및 인화 등의 과정에서 촬영자의 개성과 창조성이 인정될 경우 저작권법에 의하여 보호되는 저작물에 해당된다는 것이 판례의 입장이다. 인공지능을 이용하여 완성된 창작물인 경우는 어떨까? 어떠한 명령어를 입력하여 만들어졌는지에 창작자의 개성과 창조성이 인정되므로 저작물에 해당한다고 볼 수 있을까? 아니면 기계적 과정일 뿐이므로 작가의 아이디어가 구현된 별도의 창작물은 아니라고 보아야 할까?\n\n\n7.3.2 컴퓨터프로그램과 저작권\n’기능적 표현물’에 해당한다고 하여 저작물로 보호되지 못하는 것은 아니다. 예를 들어 컴퓨터프로그램의 경우에도, 1900년대 초에는 저작권 보호 대상인지 논란이 있었다. 컴퓨터프로그램은 기계 제어가 목적인 기능적 표현물에 해당하고 예술적, 지적 성질이 부족하다는 것이었다. 그러나 오늘날 컴퓨터프로그램은 저작물로 보호된다.\n\n\n7.3.3 저작권법의 유연성\n저작권법은 여러 법 중에서 기술적 변화에 맞추어 유연하게 변모해왔다. 예를 들어 소재를 체계적으로 배열 또는 구성한 편집물로서 개별적으로 그 소재에 접근하거나 소재를 검색할 수 있도록 하는 ’데이터베이스’의 경우에도, 2003년 저작권법을 개정하면서 데이터베이스제작자의 권리를 보호하는 내용이 포함되었다. 현행 저작권법에 관련 규정이 없다고 하여 보호대상이 될 수 없는 것은 아니다. 저작권법은 기술 발전을 따라 가며 그 규율 및 보호 범위를 확장해 온 유연한 법인 만큼, 인공지능이 관여한 저작물이 늘어나는 상황에 맞추어 현행 저작권법을 개정해야 할 필요성이 대두된다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>인공지능과 저작권 -무엇을 보호해야 하는가</span>"
    ]
  },
  {
    "objectID": "copy.html#인공지능과-저작권법의-쟁점",
    "href": "copy.html#인공지능과-저작권법의-쟁점",
    "title": "7  인공지능과 저작권 -무엇을 보호해야 하는가",
    "section": "7.4 인공지능과 저작권법의 쟁점",
    "text": "7.4 인공지능과 저작권법의 쟁점\n\n7.4.1 창작의 주체로서 인공지능\n미국 콜로라도 주박람회 미술전에서 인공지능으로 그린 그림이 2022년 디지털 아트 부문 1위를 차지해 화제가 되었다. 작품의 작가는 미드저니(Midjourney)를 이용해 제작된 그 그림을 그리고, ’미드저니를 이용한 제이슨 앨런(Jason M. Allen via Midjourney)으로 분명히 밝혔고, 디지털아트 부문 규칙상 ’디지털 기술을 창작 또는 프리젠테이션 과정의 일부로 사용하는 예술 행위’는 허용되므로, 1위를 차지한 것이 문제가 없다는 입장이었다. 원하는 그림 얻기 위해 정확한 문구 입력해야 하고 이것이 인간 고유의 영역에 해당하므로, 해당 그림은 ’인간’의 예술작품으로 볼 수 있을까?\n미국의 AI 개발자는 2020년 AI가 발명한 특허라며 특허출원을 하였고, 특허청이 자연인으로 특허를 출원하라고 보정요구를 하였으나 이를 받아들이지 않았다. 특허청이 특허출원무효처분을 하자, 특허출원무효처분취소청구의 소를 제기하였고, 최근 1심 법원은 ’인공지능’은 출원서의 발명자로 허용되지 않는다고 판단하였다(서울행정법원 2022구합89524판결). 특허를 받은 수 있는 자는 ’발명을 한 사람 또는 그 승계인’으로 ’자연인’이어야 하므로, 현행 특허법령상 발명자로 인공지능만을 표시하는 것은 허용되지 않는다는 취지이다. 이에 대해 원고측은 ’출원인을 사람으로만 한정하는 것은 기술발전에 부합하지 않는다. 법률공백은 법의 취지를 고려하여 해석하여야 하고, 발명으로 가치가 있는지 실체적 판단을 해달라’는 취지로 주장하였다. 즉, 인공지능을 발명자로 표시할 수 있도록 허용하는 것이 발명을 장려하고 그 이용을 도모함으로써 기술발전을 촉진하여 산업발전에 이바지하고자 하는 특허법 본래의 목적과 취지에 더 부합한다는 것이 원고측의 주장이었다. 그러나 법원은 “인공지능이 발명자로 표시될 수 있다 하더라도 그로 인하여 인공지능이나 인공지능의 개발자가 더 적극적으로 발명을 할 유인이 발생한다고 볼 만한 합리적 근거는 부족한 반면, 인공지능을 발명자로 인정할 경우 향후 인간 지성의 위축을 초래하여 미래 인간의 혁신에 부정적 영향을 미칠 우려, 연구 집약적인 산업 자체가 붕괴될 우려, 발명이나 그 결과물과 관련된 법적 분쟁이 발생할 경우 인공지능의 개발자인 인간이 책임을 회피함으로써 책임 소재가 불분명해질 우려 등이 엄존하고, 소수 거대 기업 등이 강력한 인공지능을 독점함으로써 특허법이 소수의 권익만을 보호하는 수단으로 전락할 위험성도 있다”면서 인공지능을 발명자로 인정하는 것이 우리 사회의 기술 및 산업발전의 도모에 궁극적으로 기여할 것이라고 단정하기 어렵다고 판단하였다. 이 사안은 특허법과 특허출원인이 인공지능이 될 수 있는지에 관한 사건이기는 하지만, 저작권 역시 현행법상으로는 ’저작물을 창작한 자’로서 업무상저작물의 경우를 제외하면 자연인인 인간이 그 주체가 된다고 볼 수 있으므로, 인공지능이 저작물등록의 주체가 될 수 있는지와 관련하여 참고가 될 수 있을 것이다. 참고로 이 사건 원고는 대한민국 뿐 아니라 미국, 영국, 독일 등 총 16개국 특허관청에도 특허출원서를 제출하였으나 남아프리카공화국을 제외한 국가들은 모두 적격 요건 위반을 이유로 특허거절결정을 하였다.\n저작권의 경우도 특허출원과 크게 다르지 않은 결론이 현재까지 내려지고 있다. 미국 저작권청은 저작물이 인간에 의해 창작된 원본 저작물만 등록대상으로 한다’는 점을 명시하고 있고, 미국에서는 인공지능으로 생성한 예술작품의 저작물 등록을 저작권청이 거부하자 제기된 소송에서 법원이 미국 저작권청 결정에 손을 들어주는 판결을 내렸다. 하웰 판사는 “생성형 AI가 부상하면서 저작권 보호 자격을 얻기 위해 AI 프로그램에 얼마나 많은 인간의 개입이 필요한지, 그리고 저작권이 있는 기존 저작물에서 학습된 내용을 바탕으로 만들어진 AI 예술작품의 독창성을 어떻게 평가할 수 있는지 등에 대한 어려운 질문이 이어지게 될 것”이라면서도, 이번 사태는 탈러가 예술작품을 만드는데 사실상 아무런 역할도 하지 않았기 때문에 복잡한 문제가 아니었다는 취지로 판시하였다. 결과적으로 이 사건은 ‘인공지능으로 생성한 작품은 저작물 등록 대상이 될 수 없다’고 판시하기는 하였으나, ‘인간의 개입과 독창성의 기준을 어떻게 판단해야 할지’에 대해 아직 판례가 확립되어 있는 단계가 아니며, 앞으로 보다 많은 논의들이 이루어질 것으로 예상된다. 사진이 처음 등장하였을 때, 인간이 피사체와 관련한 장면과 조명을 설정하고 카메라의 매개변수를 조정하는 등 이미지를 만드는 이상 카메라로 찍힌 사진에 대해 저작권이 인정될 수 있다고 본 것처럼, AI가 전적으로 관여한(entirely controlled) 작품에는 인간의 개입이 없어 저작권이 인정될 수 없더라도, 인간이 ‘어느 정도’ 개입한 경우 저작물로 보호될 수 있는 것인지, 그러한 개입이 무엇을 의미하는지 아직 논의가 진행 중이다.\n\n\n7.4.2 인공지능 콘텐츠 창작과정의 쟁점\n현재 우리나라에도 인공지능의 저작권 이슈와 관련하여 저작권법 개정안들이 발의되어 있다. 인공지능 저작물의 저작자는 창작 기여도 등을 감안하여 별도 규정에 따라 정하도록 한다거나, 인공지능 저작물의 지적재산권의 존속기간은 더 짧게 규정한다거나, 인공지능 저작물의 저작자는 해당 저작물을 등록하면서 반드시 인공지능에 의하여 제작된 것임을 표기하도록 의무화한다거나 하는 등의 방안에 대해 논의가 진행 중인 상황이다. 이와 관련하여서는 현재 인공지능을 통한 창작물에도 권리를 부여하여 법적으로 보호해야 한다는 주장과, 인공지능을 통한 창작이 활성화되면 오히려 인간이 창작한 창작물 가치가 저하될 수 있으므로 오히려 인공지능 창작물은 보호받지 않는 저작물로 명시해야 한다는 상반된 주장이 각각 제기되고 있는 상황이기도 하다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>인공지능과 저작권 -무엇을 보호해야 하는가</span>"
    ]
  },
  {
    "objectID": "copy.html#생각해-볼-문제",
    "href": "copy.html#생각해-볼-문제",
    "title": "7  인공지능과 저작권 -무엇을 보호해야 하는가",
    "section": "7.5 생각해 볼 문제:",
    "text": "7.5 생각해 볼 문제:\n\n창작행위의 본질은 무엇일까? 예술작품 중에서도 반드시 ’인간의 손’으로 표현되어야 할 장르가 존재할까? 저작자의 조건은 무엇일까?\n\n\n가수 조영남씨가 이른바 ‘화투’ 그림을 직접 그리지 않았으면서 직접 그린 것처럼 전시 후 판매하여 피해자들의 돈을 갈취하였다는 이유로, ‘사기’ 혐의로 기소된 일이 있었다. 당시 검찰은 “예술작품은 작가의 머리에서 구상이 되고, 작가의 손에 의해서 표현이 되어서, 작가의 서명으로 아우라가 완성된다고 볼 수 있고, 특히 회화의 경우 개인의 숙련도, 붓 터치, 색채감 등 작가의 개성이 화풍으로 드러나 뚜렷이 표현되는 특징을 지니고 있으므로, 그림 구매자 입장에서는 누가 직접 그렸는가 하는 사실은 구매를 결정하는 판단의 기초가 되는 중요한 사실이므로, 누가 직접 그림을 그렸는지 고지할 의무가 있다”는 전제 하에, 조영남씨가 작업을 지시하여 완성된 그림을 건네받아 경미한 작업만 추가하고 서명을 하였음에도 이러한 작업방식을 알리지 않고 직접 그린 작품인 것처럼 전시하여 판매하였다며 사기로 기소하였다.\n이 사건은 1심에서 유죄, 2심에서 무죄, 대법원에서 최종적으로 무죄가 선고되었다. 법원이 무죄를 선고한 이유는 미술작품의 거래에서 창작과정을 알려주는 것, 특히 작가가 조수의 도움을 받았는지 등 다른 관여자가 있음을 알려주는 것이 관행이라는 것 및 미술작품을 구매한 사람이 이러한 사정에 관한 고지를 받았더라면 거래에 임하지 아니하였을 것이라는 관계가 인정되어야 하는데, 피해자들의 구매 동기 등 제반 사정에 비추어 검사가 제출한 증거만으로는 피해자들이 미술작품을 피고인의 친작으로 착오한 상태에서 구매한 것이라고 단정하기 어렵다는 것이었다. 저작권법위반으로 기소되지 않았기 때문에, 이 사건에서 법원이 저작자가 누구인지 판단하지는 않았다. 대법원은 “미술저작물을 창작하는 여러 단계의 과정에서 작가의 사상이나 감정이 어느 단계에서 어떤 형태와 방법으로 외부에 나타났다고 볼 것인지는 용이한 일이 아니다. 본래 이를 따지는 일은 비평과 담론으로 다루어야 할 미학적 문제이기 때문이다. 그러므로 이에 관한 논란은 미학적인 평가 또는 작가에 대한 윤리적 평가에 관한 문제로 보아 예술 영역에서의 비평과 담론을 통해 자율적으로 해결하는 것이 사회적으로 바람직하고, 이에 대한 사법 판단은 그 논란이 법적 분쟁으로 비화하여 저작권문제가 정면으로 쟁점이 된 경우로 제한되어야 한다”고 하여 저작자 판단을 유보하였다(대법원 2018도13696판결). 그러나 ‘저작자 대작’ 논란은, 앞으로 인공지능이 관여한 저작물에서 계속 문제가 될 것이다. 미술작품에서 보조자를 사용하는 제작방식이 존재하더라도 특정한 장르인 회화에서도 이러한 제작방식이 적합하다고 할 수 있을지, 이러한 제작방식이 관행에 해당하는지, 일반인이 이를 용인할 수 있는지는 법률적 판단의 범주에 속하지 않는다고 하더라도, ’인공지능’을 ’보조자’라고 본다면, 인공지능이 제작한 미술작품에서 저작자를 누구로 볼 것인가? 인공지능이 제작에 관여하였다면 그 사실을 고지할 의무가 있는가? 와 같은 구체적인 문제에서부터, 예술이란 무엇인가? 창작행위의 본질은 무엇인가? 아이디어는 어디까지 보호되는가? 저작권제도의 본질은 무엇인가? 법률로 예술을 평가할 수 있는가?와 같은 질문을 우리에게 던지고 있다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>인공지능과 저작권 -무엇을 보호해야 하는가</span>"
    ]
  },
  {
    "objectID": "copy.html#참고-문헌",
    "href": "copy.html#참고-문헌",
    "title": "7  인공지능과 저작권 -무엇을 보호해야 하는가",
    "section": "7.6 참고 문헌",
    "text": "7.6 참고 문헌\n&lt;인공지능법 총론&gt;인하대학교 법학연구소 AI데이터법 센터, 세창출판사(2023.6.)\n&lt;인공지능 창작과 저작권&gt; 조연하, 박영사(2023.1.)\n&lt;온라인 주석서&gt; 저작권법, 온주편집위원회, 집필대표 윤종수, 로앤비",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>인공지능과 저작권 -무엇을 보호해야 하는가</span>"
    ]
  },
  {
    "objectID": "psych.html",
    "href": "psych.html",
    "title": "8  사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로",
    "section": "",
    "text": "8.1 인공지능의 진화와 이용자의 평가",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로</span>"
    ]
  },
  {
    "objectID": "psych.html#인공지능의-진화와-이용자의-평가",
    "href": "psych.html#인공지능의-진화와-이용자의-평가",
    "title": "8  사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로",
    "section": "",
    "text": "8.1.1 인공지능의 가상인간화\n인공지능의 발전으로 컴퓨터 프로그램은 인간보다 더 빠르고 정확하게 임무를 수행한다. 오픈에이아이(OpenAI)의 챗지피티(ChatGPT)는 실수 없이 엑셀 자료를 정리하거나 블로그에 올릴 글을 작성하고, 마이크로소프트(Microsoft)의 코파일럿(Copilot)은 발표용 슬라이드를 자동으로 구성하거나 긴 문서를 요약해 준다. 이러한 높은 수준의 인공지능은 종종 외형과 소통 방식에서까지 인간과 유사하다. 컴퓨터 그래픽 기술로 구현한 캐릭터의 얼굴 표정과 동작은 실제 인간만큼 정교하고 사실적이며, 인공지능 언어 모델을 활용한 채팅 프로그램의 이용자는 대화하고 있는 상대방이 진짜 사람인지 컴퓨터 프로그램인지 구분하기 어려울 정도이다. 기술 진보로 발전된 인공지능의 작업 수행 능력과 인간으로서의 외형, 언어적 표현은 컴퓨터 프로그램이 점차 인간이 오랜기간 해 왔던 일들 인간이 지금까지 맡아왔던 역할을 대체하게 될 것으로 예상된다.\n컴퓨터 프로그램이 가상인간 화 함에 따라 이들을 대하는 이용자들의 평가와 판단도 전통적인 프로그램에 대한 그것들과는 크게 달라 질 것이다. 이전 프로그램이나 기기들을 평가할 때 이용자들은 새롭게 등장한 기술이 이전보다 정보처리 속도가 더 빠르고 저장용량이 더 커져서, 혹은 화면 해상도가 더 높아지거나 가격이 낮아져 좋게 평가하였다. 그러나 인간의 모습으로 인간의 일을 수행하는 가상인간에 대해서는 이러한 수치적 평가는 크게 중요하지 않을 것 같다. 가상인간에 대한 평가는, 실제 인간에 대한 것과 비슷하게, 정량적이기 보다 정성적이고 업무의 정확성과 신속성 보다는 사회적 기대에 어긋나지 않는지, 역할을 적절히 수행하는지에 관한 것이다. 이는 가상인간에게 우리 사회가 공유하는 도덕과 윤리에 부합하는지에 따지는 것이다. 이 글에서는 이 처럼 기술이 발전으로 인공지능이 가상인간으로 등장할 때, 이용자들이 기술을 평가하는 방식이 기술적인(descriptive) 가치 중립적 판단에서 도덕성과 윤리성을 논하는 가치 개입적 판단으로 변하고 있다는 관찰에서 출발한다.\n가상인간에 대한 도덕적 평가와 가치 개입적 판단에 대해 논할 때, 가상인간이 활용될 맥락을 조금 더 구체적으로 이야기 해 보는 것이 요점을 명확히 하는데 도움이 될 것으로 보인다. 아직 가상인간이 널리 활용되지 않고 있기에 우리가 할 수 있는 것은 기껏해야 예측이지만 말이다. 가상인간의 활용 맥락을 예측하는 것은 가상인간이 대체하게 될 기존의 인간 관계를 검토하는 것에서 출발할 수 있을 것이다. 그 이름에서 알 수 있듯이, 가상인간은 내부적으로는 인간의 인지와 감정을 모델링하고 외형적으로는 사람의 얼굴과 신체를 묘사할 것이기에, 이들은 이전에 사람들이 해 오던 일들과 역할들을 맡게 될 것이다. 영화나 소설과 같은 창작물에서는 이미 가상인간은 애인이나 자녀와 같은 핵심적인 인간 관계의 대상으로 고려된다. 이들 작품에서 가상인간은 종종 실제 인간보다 더 호감을 주는 외모와 성격, 실제 인간보다 더 주어진 관계 속에서 기대되는 역할을 충실히 수행한다. 이들에 대한 인간의 반응은 실제 사람 대상보다 더욱 진심이다. 조금 더 현실적으로 우리는 소셜미디어에서 가상인간이 올린 사진을 통해 그들의 일상을 엿본다. 가상의 인플루언서를 팔로우 하는 이용자들은 댓글이나 좋아요를 통해 반응을 보인다. 이미지를 공유하고 댓글을 통해 소통한다는 점에서 실제 인간 사용자들의 관계 속에서 하는 활동과 큰 차이가 없다.\n그러나 이들 작품들에서는 결국 가상인간과의 관계에서 인간들이 겪는 윤리적, 도덕적 문제들을 부각시킨다. 이용자가 대상과 맺는 관계의 속성은 유지하며 그 내용만 생체물에서 인공물로 대체할 때, 기존에 존재하지 않았던 가치 판단의 문제들이 발생하기 때문이다. 가상인간이 기존 인간 역할을 수행하고 사회적 관계에서 실제 인간을 대체할 때 이용자는 가상인간에 대해 어떤 가치 판단을 하며, 우리 사회는 어떤 도덕적, 윤리적 문제를 겪게 될까?\n이에 대한 가장 간편한 답은 인공지능이 인간 관계를 대체하게 될 때, 그 역할의 유사성으로 인해 인간에 대해 갖고 있었던 도덕적 권한과 의무를 가상인간에게 그대로 부여하고, 이를 기준으로 가상 인간을 평가하리라 예상하는 것이다. 아이를 입양 하는 경우를 예를 들어보자. 입양된 아이는 혈연적 관계가 아닌 사회적 계약에 의해 형성된 관계이다. 그러나 여전히 혈연 자식이 갖는 권리을 부여 받고, 혈연 자식과 동일한 수준의 책임을 갖는다. 또한 입양아의 부모들에게는 그들이 실제로 낳은 아이를 보살피는 것과 같은 수준의 노력과 관심으로 입양아를 돌볼 의무를 지운다. 인공지능에 의해 구현된 가상인간 아이에게도 같은 논리를 적용한다면, 우리는 가상인간 아이와 그리고 그 가상인간 아이를 돌보고 아이와 상호작용 하는 다른 사람들을 도덕적으로 평가할 때, 인간 아이의 기준에 준하여 할 수 있다.\n가상인간에 대한 도덕적판단의 또 다른 극단은 가상인간을 인간의 역할을 수행하고 있지만 인간이 아닌 하나의 제품이나 서비스로 인식하는 것이다. 이 경우 대상에 대한 도덕적 판단은 내려지지 않고, 기능이 기대했던 방식으로 작동하는지 만이 관심의 대상이다. 자식의 오랜 기능이었던 부모 보양의 의무는 요양보험과 같은 제도로 대체되고 이 제도 자체에 대한 도덕적 판단을 내리지 않는다. 해당 제도가 기대했던 기능을 잘 수행하는지, 다른 부작용은 없는지 따지게 된다. 다만, 그 제도를 기획하고 시행한 인간에 대해 판단 내릴 수는 있겠지만 말이다. 결국은 그러나 이러한 사례들은 현실에서 발견되기에는 극단적이고, 실제 가상인간에 대한 도덕적 평가는 그 중간 어디일 것이다. 가상인간이 실제 인간의 역할 대체할 때, 해당 가상인간에 대한 가치 평가와 도덕적 판단은 실제 인간의 온전히 같지도, 기계가 단순한 기능만 수행할 때의 판단하는 경우와 같지도 않을 것이다.\n\n\n8.1.2 가상인간 도덕성 평가의 복잡성\n대상의 가치를 평가하고 도덕적 관점에서 판단 한다고 함음, 특정한 규범적 기준으로 대상 옳은지, 그른지를 평가하는 것이다. 이러한 평가는 인공지능이 가상인간으로 구현되기 이전, 단순한 정보 처리기 수준이었을 때도 존재하였다. 비록 그것이 아주 단순해서 그러한 판단에 대한 이견이 없었기 때문에 주목받지 못했을 뿐이다. 즉 우리가 사용하는 기계는 의도대로 작동하고 계산이 정확하면 옳은 것이고 개인적인 이용 목적에 부합하여 유용하게 활용할 수 있으면 좋은 것이었다. 그러나 인공지능의 능력이 더욱 높아지고, 사회적으로 복합한 역할을 하는 가상인간이 되었을 때, 우리가 내리는 가치판단은 예전의 그것 만큼 단순하지 않다.\n가상인간 도덕성 평가가 복합한 이유의 일부는 그것이 개인적이 아닌, 조직적, 사회적 단위로 이루어 진다는 점에 있다. 단순한 계산기 수준의 서비스에서와는 다르게, 가상인간에 대한 가치 판단은 개인을 넘어서 내가 몸담는 조직 수준에서, 내가 속한 사회적 수준에서 이루어 진다. 즉 나한테는 좋은 것이지만 나의 가족이나 내가 속한 조직에게는 나쁜 것이라면, 해당 서비스는 나쁜 것으로 판단한다. 이러한 집단 수준의 평가는 종종 상충될 수 있는 다양한 기준들을 포함한다. 인공지능을 활용하면서 효율성은 높일 수 있지만, 공정성을 저해한다면, 이들의 상충되는 가치로 인해 합의된 평가 기준을 확립하기 어렵다.\n가상인간에 대한 가치 판단이 어려운 또 다른 이유는 그것에 대한 판단 기준이 유동적이라는 것이다. 해당 기술을 바라보는 사람마다 그것을 해석하는 방식이 다르다. 어떤 사람에게 눈 앞에 존재하는 사람 형태의 대상은 단순한 연산을 수행하는 프로그램일 뿐이지만, 다른 사람에게는 그 대상이 연약한 모습을 띤 어린아이거나 사랑스러운 연인이다. 이렇게 가상인간을 어떻게 보느냐에 따른 인식 상의 차이는 무엇이 바람직한 것이고 무엇이 바람직하지 않은지의 도덕적 평가에 차이를 유발한다. 가상인간이 이런 것도 해 주는구나 하고 고마워 하고 감동 받을 수도 있고, 기계가 단순히 작업하는 것을 당연하다고 판단할 수 있다.\n가상인간에 대한 도덕적 판단에서의 또 다른 특성은 그 판단의 대상이 가상인간이 되기도 하지만, 이와 상호작용하는 이용자가 되기도 한다는 점에 있다. 내가 제품이나 서비스를 이용하는 방식이 종종 다른 사람들이 평가하고 판단하여, 도덕적으로 적절하고 바람직하다고 칭찬할 수도 있고, 부도덕하다고 비난할 수도 있다.\n본 글에서는 이러한 가상인간에 대한 가치 평가과 도덕적 판단을 다루고자 한다. 가상인간과 사용자가 어떤 조건에 있을 때 도덕적 판단을 내리는지, 어떤 방식으로 해당 평가가 이루어 지는지 살펴보고자 한다. 이를 위해 가치 평가와 도덕적 판단을 사회인지(social cognition)적 관점에서 문제를 풀어보고자 한다. 사회인지는 인간의 인식, 기억, 추론 등의 인지과정을 인간 대상의 자극물에 적용하는 인지심리학의 한 분야이다. 특히 비인간 대상에 대한 인간 지식의 활용이라는 의인화 과정을 기반으로 설명할 것이다. 본 글은 다음과 같이 구성되어 있다. 2절에서는 사회인지 이론으로 에플리의 의인화 이론을 소개하고, 3절에서는 의인화된 인공지능을 가치평가의 사례로서 다룬다. 4절에서는 사회인지 과정을 넘어서 가상인간의 도덕적 판단에 영향을 미치는 요인들을 개관한다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로</span>"
    ]
  },
  {
    "objectID": "psych.html#의인화-사회적-인공지능에-대한-이용자-반응",
    "href": "psych.html#의인화-사회적-인공지능에-대한-이용자-반응",
    "title": "8  사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로",
    "section": "8.2 의인화: 사회적 인공지능에 대한 이용자 반응",
    "text": "8.2 의인화: 사회적 인공지능에 대한 이용자 반응\n인간과 유사한 모습을 하고 있거나 인간의 행동을 하고 있는 비인간 대상에 대해 이용자는 인간의 모습과 행위를 떠올리고, 그러한 인간에게 적용했던 의미를 그대로 부여한다. 이를 에플리(Epley et al., 2007)는 의인화(anthropomorphism)로 설명하였다. 의인화는 콘텐츠 창작자들이 대상의 특성을 과장하여 눈코입을 그리거나 사람과 같은 심적인 상태를 명시적으로 표현하는 것으로 이해되지만, 여기서는 일종의 인지과정으로 인간과 관련된 지식을 인공지능이나 가상인간과 같은 비인간 대상에 적용하는 것으로 이해할 수 있다. 의인화는 보다 일반적인 인지과정인 지식활성화(knowledge activation)의 한 종류이다. 비인간 대상에 의해 인간의 지식이 활성화 되고, 관련되어 있는 개념과 지식들이 함께 활성화 하여 비인간 대상을 이해하는 데에 활용된다. 지식 활성화는 커뮤니케이션 분야의 미디어 효과 연구에서 많이 활용되는 점화(priming)와도 유사한 개념으로, 어떤 자극으로 인해 기억 속에 저장되어 있던 특정한 지식, 개념, 관념이 활성화 되어 이후 인지과정에 영향을 주는 것을 의미한다(Roskos-Ewoldsen et al., 2009).\n인간은 외부의 자극을 처리할 때 단순이 논에 보이는 것 만을 인식하는 것이 아니라, 대상의 눈에 보이지 않는 특성을 파악하거나, 대상에 어떤 행동을 취해야 할 지, 어떤 방식으로 대해야 할 지 등이 결정해야 한다. 대상이 인간과 유사할 때 바로 의인화가 발생한다. 대상이 형태적으로, 행동적으로, 관계적으로 인간과 유사하다면 이는 인간 지식을 활성화 시키고 이 활성화 된 지식을 활용하여 대상을 파악하고 대상에 대해 행동하는 방식을 결정한다.\n\n8.2.1 의인화 과정의 인지적 특성\n의인화와 같은 지식활성화 과정의 특징은 자동적(automatic)이라는 것이다(Bargh, 1989). 인지과정의 자동성은 다양한 수준을 갖는데, 구체적으로는 지식활성화의 효과가 의식하지 못하고(unaware), 노력이 필요 없어(effortless), 의도가 필요 없고(unintentional), 조절불가능(uncontrollable) 하다. 의인화의 종류에 따라 전부, 혹은 일부 그런 특성을 갖는다.\n의인화가 자동적인 인지과정이라는 것은 다음의 그림에서도 알 수 있다. 아래 나무 조각에서 대부분의 사람들은 눈과 콧구멍, 입으로 구성된 사람의 얼굴을 볼 것이다. 의인화 이론에 따르면 우리는 동그란 형태에 윗 부분에 수평을 맞춰 존재하는 두개의 작은 구멍과 그것 보다 더 작은 두개의 구멍이 중간에 위치하고, 큰 구멍이 아랫 부분에 위치하는 형태로는 사람의 얼굴을 가장 많이 봐 왔기 때문에 해당 자극에 대해 사람의 얼굴 지식을 활성화 시키고 이를 토대로 나무조각을 사람 얼굴로 해석하는 것이다. 특히 아래의 그림에서는 동그란 눈과 입을 보고 놀란 표정의 얼굴을 떠올릴 것이다.\n이 그림에서 사람의 얼굴을 보는 것은 노력이나 의도가 필요 없고, 사람의 얼굴이 아닌 다른 존재를 보려고 조절하는 것도 쉽지 않다. 다만, 표면의 질감이나 그 묘사의 세부적인 부분에서 진짜 사람이 아니라 사람 얼굴과 닮았고, 나는 이것이 나무인 것을 알지만 사람 얼굴이 보힌다고 인식할 수 있을 것이다.\n\n\n\n&lt;그림 1&gt; 사람의 얼굴을 떠올리게 하는 나무 조각, 출처: Rennie (2015)에서 재인용\n\n\n의인화는 형태를 인식 할 때 뿐만 아니라, 완결성 있는 이야기를 구성해 나갈 때도 활용된다. 하이더와 짐멜(Heider & Simmel, 1944)의 실험에서 참여자들은 짧은 영상을 보고 그들이 본 것을 기술하였다(&lt;그림 2&gt;). 실험에 참여한 사람들은 움직이는 삼각형, 원형의 도형들의 움직임에 의미를 부여하고 의도와 감정을 읽어 내었으며 성격을 부여하여 해당 도형들을 파악하였다. 배경지식이나 맥락을 공유하지 않은 다른 사람에게 특정 대상을 쉽게 설명하기 위해서는 인간 지식을 활용하는 것은 매우 효과적일 것이다.\n\n\n\n&lt;그림 2&gt; 사람의 감정과 성격을 얼굴을 떠올리게 하는 실험 자극물, 출처: Heider & Simmel (1944)\n\n\n인간에 대한 지식을 활용하여 비인간의 행위를 이해하고 어떻게 비인간과 상호작용 해야 하는지 힌트를 얻는 것은 인간-컴퓨터 상호작용 분야에서도 활발히 연구되었다. 내스와 그의 동료들은 일련의 연구를 통해 컴퓨터에 대한 이용자의 반응이 사람에 대한 반응과 다르지 않다는 점을 발견하였다(Nass et al., 1994; Nass & Moon, 2000). 특히 내스가 사용하였던 컴퓨터는 교사라는 사회적 역할을 수행하며 음성언어로 응답하는 특징이 있었는데, 이는 그 당시 사용되던 컴퓨터와는 다소 거리가 있는 것으로, 이용자들이 사회적 규범을 활용할 수 있도록 고안된 것이었다. 내스는 실험을 통해 이용자들이 인간과의 상호작용에 적용하는 다양한 사회적 규범을 컴퓨터에도 적용한다는 점을 보였다. 예를들어 이용자들은 다른 사람을 직접적으로 부정적으로 평가한다는 것을 꺼린다는 공손함의 규범을 적용하였는데, 이를 보이기 위해 내스는 이용자들이 컴퓨터를 이용하여 학습을 하게 하고 학습한 컴퓨터를 평가하는데 같은 컴퓨터를 사용하거나 다른 컴퓨터를 사용할 때 평가가 다름을 보여 공손삼의 규범이 컴퓨터와의 상호작용에도 적용됨을 보였다. 이러한 결과도 의인화 과정으로 설명할 수 있다. 음성언어의 사용이나 선생이라는 사회적 역할을 인지하였을 때 이용자는 그에 부합하는 사회적 규범을 떠 올렸을 것이고 이에 부합하도록 컴퓨터를 평가하거나 행동하였을 것이다. 이 과정은 역시 이용자들이 의식적으로 노력하지 않은 자동적인 반응이다.\n\n\n8.2.2 사회인지 과정으로서 의인화\n에플리에 따르면, 의인화는 늘상 발생하는 자연스러운 인지과정이다. 세상에 존재하는 다양한 대상 중에 인간에 관련된 지식이, 특히 내가 인간으로서 경험하는 지식이 가장 접근 가능한(accessbile) 지식이기 때문이다. 우리는 인간으로서, 인간에 대한 지식, 특히 외부 조건과 내부 상태에 대한 조건화 과정을 지속적으로 경험하여 왔다. 추운 곳에 나가 있으면 춥워서 고통스럽고, 신체활동에 적절한 온도의 실내로 들어오면 따듯함과 아늑함을 느낀다. 오랫동안 먹지 않으면 허기를 느끼고, 배가 고플 때 맛있는 것 먹으면 포만감에 기분이 좋아진다. 간절히 원하던 것을 이루면 성취감에 행복감을 느끼며, 오랫동안 관계맺었던 소중한 사람을 잃으면 상실감으로 마음 아프다. 이러한 개인의 경험 지식은 항상 우리가 직접 경험하는 지식이기에 쉽게 접근가능하고, 이 때문에 다른 사람이 비슷한 조건에 있을 때 그 사람의 내적 상태를 쉽게, 종종 자동적으로 추론할 수 있다. 더 나아가 그 대상이 인간이 아니더라고 외형적으로 인간과 비슷할 때, 혹은 수행하는 일에서 인간의 역할을 떠올리게 할 때, 인간의 지식을 활용하여 대상을 인식한다. 이러한 인간 관련 지식은 해당 지식을 비인간 대상에 적용하여 의인화 할 때 필수적인 요소이다.\n인간 지식이라는 인지적 차원의 요소 외에 의인화에 영향을 미치는 동기(motivation) 차원의 요인이 존재한다. 에플리는 의인화를 촉진시키는 동기 요인으로 사회 동기(sociality motivation)와 효능 동기(effectance motivation)를 제안하였다. 사회 동기는 인간은 기본적으로 타인과 관계 맺고 타인과 함께 있는 것에 대한 욕구가 있음을 가정한다. 이에 오랜 기간 동안 혼자 있거나, 가까운 사람들의 죽음이나 헤어짐 등 관계의 상실을 경험한 사람일 수록 사회 동기가 높고 이를 충족하기 위한 방안으로 비인간 대상을 의인화 한다. 이 전 연구들은 배우자의 죽음 등과 같은 관계의 상실의 경우, 반려견에 대한 의지가 높아 진다든지, 다른 비인간 대상들로부터 인간 관계를 충족시킨다는 연구를 보고하였다.\n또 다른 동기 차원의 요인은 효능 동기이다. 효능 동기 요인이 가정하는 바는, 인간은 기본적으로 세상에 존재하는 외부 것들에 대해 이해하려고 하고, 통제하려는 욕구이다. 외부 사물에 대한 이해가 충분하고 세상을 통제할 수 있다는 자신감이 있을 때, 인간은 만족을 느끼는데, 그렇지 못한 경우, 비인간 사물을 의인화 하여 이해하고 상호작용을 통해 통제할 수 있다는 느낌을 받으면서 만족감을 얻는다.\n\n\n8.2.3 의인화와 비인간 대상에 대한 마음인식\n의인화는 인지과정으로 눈으로 쉽게 관찰될 수 없지만, 종종 관찰되는 의인화의 지표는 대상의 마음을 인식하는 것이다. 그래이와 동료들은 사람들이 다른 사람이나 비인간 대상의 내적 상태를 인식할 때 크게 경험(experience)과 작인(agency)의 두 마음 상태가 구분됨을 보였다(Gray et al., 2007). 경험은 기쁨과 즐거움, 슬픔과 같은 감정이나 쾌락이나 육체적 고통을 감정을 의미하고, 작인은 행동을 계획하거나 목표를 수립하는 등의 능력과 관련되어 있다. 이들 마음 상태에 대한 인식은 인식 대상의 성별이나 나이, 직업 등에 따라 달라진다. 예를 들어 그래이의 연구에서 연구 참여자들은 남성에 비해 여성에 대해 높은 경험과 낮은 수준의 작인을 인식하였으며, 강아지의 경우 장난감 로봇에 비해 높은 감정을 가졌지만, 작인의 경우 로봇이 강아지 보다 높은 것으로 인식하였다. 이 두 비인간 대상으로부터 마음을 인식하는 것은 대상에 대한 도덕적 판단의 대상이 된다. 반복적으로 움직이는 기계적 움직임에 도덕적 판단을 내리지 않는다. 행위로 발생하는 결과에 대한 도덕적 책임을 묻고, 외부의 행위로 인해 혜택을 받거나 피해를 받을 때 도덕적 권리를 부여받는다.\n\n\n\n&lt;그림 3&gt; 마음인식과 도덕적 판단으로 이어지는 의인화 과정",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로</span>"
    ]
  },
  {
    "objectID": "psych.html#가상인간-이루다에-대한-도덕적-튜링-테스트",
    "href": "psych.html#가상인간-이루다에-대한-도덕적-튜링-테스트",
    "title": "8  사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로",
    "section": "8.3 가상인간 이루다에 대한 도덕적 튜링 테스트",
    "text": "8.3 가상인간 이루다에 대한 도덕적 튜링 테스트\n이루다는 스캐터랩(Scatter Lab)에서 2020년 11월 오픈한 인공지능 기반의 채팅서비스로 이용자와 채팅이라는 대화 수단으로 상호작용을 한다. 기존의 챗봇이 정보 검색이나 단순한 질의-응답을 이루는 목적 지향형 챗봇을 넘어서 인간 사이의 일상적인 대화가 가능한 오픈도메인 챗봇으로 개발되었다. 이에 우리가 일상적으로 대화하는 주제를 자유롭게 선택하여 상호작용할 수 있다. 특히 이루다는 나이와 성별, 인종 등의 인간적 특성을 부여받았다는 점에서 가상인간으로 볼 수 있다. 2024년 1월 현재 홈페이지에 공개된 정보에 따르면 이루다의 나이는 22세이고, MBTI는 ENFP, 취미는 날씨 좋은 날 산책하기, 친구들이랑 수다떨기, 인스타그램 구경하기로, 24년에 20대를 보내고 있을 법한 특징을 지니고 있다. 이루다는 소셜미디어 계정을 통해 개인의 취미와 좋아하는 것을 간접적으로 노출하며 정체성을 쌓아가고 있다. 특히 LOL 게임의 결과에 반응하는 글을 올려 컴퓨터 게임에 관심이 있음을 알렸으며, 첫눈을 기념한 포스팅이나 수능 날 기념 포스팅 등의 시의성 있는 포스팅을 올려 외부 세계와 현실감 있는 소통을 진행중이다. 이용자들은 이미지에 코멘트를 날리거나 공감을 표현하는 등, 일반적인 인간 이용자에 대한 반응과 유사한 반응을 보이기도 하고, 외형적으로 어색한 부분을 언급하기도 한다.\n[특정한 관계 속에서 이루어지는 것은 아니지만, 특정한 인구통계학적 집단에 소속된다는 이유로, 공개적인 이루다는 대중적으로 소개된 가상인간이라는 점에서 우리가 가상인간에 대해 어떤 판단과 평가를 이룰지 보여주는 중요한 사례이다. 흥미로운 점은 출시 즉시 이루다는 사회적 평가의 대상이 되었고, 이용자들은 이루다의 다양한 면들을 관찰하며 논란을 생산하였다는 점이다. 여러 논란 중에 흥미로운 점은 도덕성을 판단하기 위해 민감한 질문을 지속적으로 물어보고 이를 온라인 커뮤니티에 공유함으로써 논란을 키웠다는 점이다. 예를 들어 이런 저런 질문을 하였고, 초창기 이루다는 이런 저런 대답을 하였다. 이는\n\n\n\n&lt;그림 4&gt; 가상인간 이루다의 혐오표현 (출처: “20살 여성 AI 이루다, 지하철 임산부석에 ‘혐오스러움’”, 김금이, 2021.01.10, 매일경제, https://n.news.naver.com/mnews/article/009/0004730670?sid=102)\n\n\n주목할 만한 점은 이루다는 컴퓨터 프로그램으로 도덕적 판단의 대상이 될 필요가 없다는 것이다. 예를 들어 특정 소수 집단에 대한 소수 표현은 인터넷 검색을 통해 노출 될 수 있다. 우리가 검색을 통해 해당 표현을 보았다면, 글을 작성한 사람을 도덕적으로 비난하지, 해당 검색엔진을 비난하지 않는다. 그러나 언어로서 표현했다는 사실이 의인화 하게 하여 비난하도록 만들었다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로</span>"
    ]
  },
  {
    "objectID": "psych.html#나오며-가상인간에-대한-도덕성-평가",
    "href": "psych.html#나오며-가상인간에-대한-도덕성-평가",
    "title": "8  사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로",
    "section": "8.4 나오며: 가상인간에 대한 도덕성 평가",
    "text": "8.4 나오며: 가상인간에 대한 도덕성 평가\n인공지능의 발달로 인한 인간화는 얼마나 인간과 비슷한 지적 능력과 외형을 갖게 되는지에 대한 기술적 문제가 주목받아 왔지만, 더욱 인간과 유사해 질 수록 가상인간은 도덕적, 윤리적 판단의 대상이 될 것이다. 본 논의에서 살펴본 이루다의 사례에서 보듯이, 다양한 논란을 불러 일으킬 것이다. 본 논의에서는 이를 관통하는 중요 메커니즘으로 의인화를 살펴보았다. 앞으로 가상인간은 다양한 수준의 지적능력과 사회적 역할을 수행하며 소개 될 것이다. 이들에 대한 가치판단은 어떻게 될 것인지 보다 진전된 논의가 필요하다.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로</span>"
    ]
  },
  {
    "objectID": "psych.html#생각해-볼-문제",
    "href": "psych.html#생각해-볼-문제",
    "title": "8  사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로",
    "section": "8.5 생각해 볼 문제",
    "text": "8.5 생각해 볼 문제\n\n인간이 실수를 저지르고 이에 대해 도덕적 판단을 내릴 때, 종종 그 사람의 속 사정이 알려져 비난의 수위가 낮아지곤 한다. 범죄자의 어려웠던 집안 사정으로 인해 동정심이 유발되거나 폭력적인 가정에서 자란 흉악범에 비난을 가정형편으로 돌리는 것이 그 예이다. 가상인간에 대한 도덕적 비난이 있을 때 유사한 경우가 가능할까? 집안의 경제적 사정이나 불우했던 행했던 경우는 모두 인간으로서 경험하게 되는 사건이라는 점을 생각해 보자.\n가상인간이 가장 활발히 활용될 것으로 예측되는 분야 중 하나는 엔터테인먼트이다. 때로는 대중으로부터 사랑을 받기도 하고 외면을 받기도 하지만 최초의 가상인간 가수 아담부터 메이브까지 지속적으로 가상인간 연예인이 등장하고 있다. 오늘날 연예인에 대한 대중의 관심이 음악이나 연기 등과 같이 연출되는 것을 보는 것 외에, 브이로그나 라이브 방송과 같이 있는 그대로의 모습을 노출함으로써 더 큰 사랑을 받는 것으로 보인다. 가상인간의 경우 인간적인 모습을 보이기 어려운데, 이 문제를 어떻게 해결할 수 있을까?",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로</span>"
    ]
  },
  {
    "objectID": "psych.html#더-읽을-거리",
    "href": "psych.html#더-읽을-거리",
    "title": "8  사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로",
    "section": "8.6 더 읽을 거리",
    "text": "8.6 더 읽을 거리\n“사이버가수 ’아담’부터 가상인간 ’로지’까지, 어디까지 진화해 나갈까”, 윤휘종, 비즈니스포스트, 2021.11.21. https://www.businesspost.co.kr/BP?command=article_view&num=260590\n“가상 인간을 기업이 아닌 일반인들이 사용하기 시작했다. 크리에이터들은 가상인간을 어떻게 활용할까?”, 이승필, 브런치스토리, 2021.09.30. https://brunch.co.kr/(seungpillee/18?)\n“가상 아이돌, 영화에도 출연… 눈앞에 다가온 AI 영화-옴니버스 영화 ‘서울 도시 전설’ 제작보고회”, 서정민, 한겨레신문, 2023.11.02. https://www.hani.co.kr/arti/culture/culture_general/1114623.html\n##참고문헌\nBargh, J. A. (1989). Conditional automaticity: Varieties of automatic influence in social perception and cognition. In J. S. Uleman & J. A. Bargh (Eds.), Unintended thought (pp. 3–51). Guilford Press.\nEpley, N., Waytz, A., & Cacioppo, J. T. (2007). On seeing human: A three-factor theory of anthropomorphism. Psychological Review, 114(4), 864–886. https://doi.org/10.1037/0033-295X.114.4.864\nGray, H. M., Gray, K., & Wegner, D. M. (2007). Dimensions of mind perception. Science, 315(5812), 619–619. https://doi.org/10.1126/science.1134475\nHeider, F., & Simmel, M. (1944). An experimental study of apparent behavior. American Journal of Psychology, 57, 243–259. https://doi.org/10.2307/1416950\nNass, C., & Moon, Y. (2000). Machines and mindlessness: Social responses to computers. Journal of Social Issues, 56(1), 81–103. https://doi.org/10.1111/0022-4537.00153\nNass, C., Steuer, J., & Tauber, E. R. (1994). Computers are social actors. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 72–78. https://doi.org/10.1145/191666.191703\nRennie, B. (2015). Guest Editor’s Introduction: Religion, Art, and Cognition. Journal for the Study of Religion, Nature and Culture, 9(3), 251–258. https://doi.org/10.1558/jsrnc.v9i3.27055\nRoskos-Ewoldsen, D. R., Roskos-Ewoldsen, B., & Carpentier, F. D. (2009). Media priming: An updated synthesis. In J. Bryant & M. B. Oliver (Eds.), Media effects: Advances in theory and research (pp. 74–93). Routledge.",
    "crumbs": [
      "2부: 인공지능 윤리의 적용",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>사회적 인공지능의 도덕성 평가: 가상인간의 마음인식을 중심으로</span>"
    ]
  },
  {
    "objectID": "journalism.html",
    "href": "journalism.html",
    "title": "9  인간을 위한 인공지능의 기술적 탐구",
    "section": "",
    "text": "9.1 미디어 법학자들의 인공지능 기술 탐구\nAI 기술은 최근 몇 년간 “전통적인 미디어”에서 다양한 미디어 상품의 집합체로서 발전하고 있는 미디어 변화를 인식하게 한다. 다시 말해, ’미디어의 융합’은 모든 미디어 콘텐츠를 디지털화하여 품질 저하 없이 생산, 교환할 수 있는 기술적 가능성에서 비롯된다. 이는 미디어 콘텐츠 생산자(제공자) 간의 경쟁을 고려하고, 동시에 미성년자, 개인 권리 및 지적 재산권의 보호가 적절하게 보장되는 규제의 틀을 국가가 어떻게 만들 수 있는지에 대한 물음으로 확대된다.\n이런 점에서 독일의 미디어 법학자들은 새로운 기술이 가진 잠재력과 위험을 무엇보다 기본권 차원에서 다루고 있다. 독일의 기본법은 “언론의 자유와 방송 및 영화를 통한 보도의 자유를 보장”하면서, 방송과 영화의 자유와 함께 언론의 자유를 보장하고 있다. 독일연방헌법재판소는 출판물의 특정 형태와 무관하게 의사소통 과정에서 출판물이 관련되는 한, 언론의 자유를 보호한다(BVerGE 66, 134: 107, 280). 따라서 새로운 전자적 배포의 형태, 즉 로봇 저널리즘이나 AI 저널리즘도 기본법상 언론의 보호 대상에 포함된다(Weberling, 2018). 하지만 AI 자체는 인간도 아니고 법적 실체도 아니므로 기본권을 행사할 수 없다. 물론 AI 운영자는 기본권을 행사할 수 있으며, 직업의 자유나 커뮤니케이션 권리가 보장된다. 그러나 가장 중요한 것은 AI를 기반으로 하는 저널리즘 제공자가 언론의 자유에 의존할 수 있는지다. 왜냐하면, 기본법상의 기본권인 ’언론의 자유’는 언론 종사인 ’사람’을 위한 인권이기 때문이다.\n물론 AI 저널리즘은 저널리스트의 개입하에 콘텐츠가 제작된다. 거대 인공지능 모델이 사람의 도움 없이 초소형 AI를 만드는 시대가 왔고, 이제는 거대 모델이 알아서 소형 모델 개발부터 배포까지 하고 있다. 인공지능 과학자들에 따르면, 거대 AI 모델은 스스로 데이터 세트를 통해 사람의 동작 데이터까지도 수집하고 분석할 수 있다(Business Insider, 2023. 12.). 하지만 인간 저널리스트는 기사 작성과 콘텐츠 제작과정에서 모든 유형의 출판물에 중요한 요인이 무엇인지 선별하고 결정하고 있다. 따라서 편집자나 저널리스트가 AI 소프트웨어를 보조도구로써 사용하고, 이러한 보조도구에 의해 제작된 콘텐츠에 대한 윤리적, 법적 책임은 여전히 사람에게 부여된다. 아래 &lt;표 1&gt;은 독일의 미디어 법학자들이 AI 저널리즘과 관련해 논의하고 있는 주요 이슈들을 개괄적으로 나열하고 있다.\n&lt;표 1&gt;에서 살펴본 바와 같이 AI가 생성한 텍스트는 아직　저작권으로 보호되지 않는다. 왜냐하면, 저작권법은 인간의 창작 활동에 의한 저작물을 보호의 대상으로 하기 때문이다. AI에게 프롬프트를 사용해 텍스트를 생성해 달라고 요청한 사람도 자동으로 결과물의 작성자가 되는 것이 아니다. 이는 AI가 생성한 텍스트 및 이미지를 제한 없이 복사하고 다시 게시할 수 있음을 의미한다. 따라서 인공지능을 사용하여 맞춤형 콘텐츠를 생성하는 언론사나 광고 대행사에 문제가 될 수 있다. 이들은 자신의 창작물이 법적 결과 없이 다른 사람에 의해 복제될 수 있음을 예상해야 한다.\n최근 AI 기술을 이해하고, 적절한 법안을 제시하기 위한 법학자들의 노력 가운데 ’텍스트와 데이터 마이닝’에 대한 기술적 해명이 있다. 우선, 언어 중심의 생성형 AI는 거대언어모델(LLM, Large Language Model)을 통해 데이터를 학습한다. 다시 말해, 이미지나 영상 생성 AI도 그림과 비디오 등을 배우지만, 학습한 데이터는 학습이 끝난 뒤 거대언어모델 내부에 저장이 되거나 원본 파일이 따로 없다. 즉, 거대언어모델은 데이터를 갖고 머신러닝을 돌리면 수식이 저절로 생기는 방식이다. 따라서 데이터는 머신러닝에서 가중치를 최적화하는 재료로 사용되지만, 데이터가 최종 수식에 보존되지 않는다. 이런 특성 때문에 생성 AI 업계에서는 개발사가 소설, 기사, 만화 등 저작물을 법적 제약 없이 학습할 수 있다는 인식이 지배적이다. 지금의 저작권법에서는 원작을 저장하고 재현하는 행위를 저작권 위반으로 보는데, 생성 AI는 둘 다 해당 사항이 없다는 견해이다. 이런 점에서 뉴욕타임스와 오픈AI 간의 법적 갈등은 평행선을 달려왔다. 뉴욕타임스는 생성 AI 학습에 기사를 쓴 대가로 거액을 요구했지만, 오픈AI는 이런 저작물을 마음대로 학습할 수 있다고 주장해 왔다. 따라서 미국의 AI 지지자들은 저작권법 제107조에 따라, ’공정이용(fair use)’을 주장하는 반면, 유럽에서는 ’텍스트 및 데이터 마이닝’을 위해 저작권이 있는 작품의 특정 사용을 허용하는 DSM 지침2을 인용해 왔다.\n이런 상황에서 독일의 법학자 팀 도어니스(Tim W. Dornis)와 인공지능 과학자 세바스티안 스토버(Sebastian Stober)는 생성형 AI 모델의 기술을 살펴보면서, 이 기술의 훈련은 텍스트 및 데이터 마이닝의 경우가 아니라고 주장한다. 이들에 따르면, “훈련 데이터의 일부는 현재 생성모델(LLMs and latent diffusion models)에서 전체 또는 부분적으로 기억할 수 있기 때문에 최종 사용자가 적절한 프롬프트로 다시 생성하여 재생산할 수 있다”고 지적한다(Initiative Urheberricht, 2024). 따라서 이는 명백한 저작권 침해가 된다. 다시 말해 이 연구는 지금까지 지배적인 유럽의 법률적 견해, 즉 DSM 지침에 반대하는 이론적 근거가 된다.\n텍스트와 데이터 마이닝에 대한 기술적 해명에서 제시하는 주요 주장은 크게 세 가지로 구분된다. 첫째, 텍스트와 데이터 마이닝에 대한 예외는 생성형 AI 훈련에 적용되어서는 안 된다. 왜냐하면, 이 기술은 근본적으로 다르기 때문이다. 하나는 의미 정보(semantic information)을 처리하는 반면, 다른 하나는 구문 정보(syntactic information)를 추출한다. 둘째, 생성형 AI의 훈련 중에 발생하는 대규모 침해를 정당화할 적절한 저작권 예외 또는 제한은 없다. 예를 들면, 데이터 수집 중에 보호된 작품을 복사하는 것, AI 모델 내에서 전체 또는 부분적으로 복제하는 것, 챗GPT와 같은 AI 시스템의 최종 사용자가 시작한 훈련 데이터에서 작품을 재생산하는 것과 관련된다. 셋째, AI 훈련이 유럽 외부에서 이루어지더라도 개발자는 유럽의 저작권법을 완전히 피할 수 없다. 저작물이 AI 모델 내부에서 복제되는 경우, 유럽에서 모델을 공개하면 InfoSoc 지침 제3조3에 따라 ’공개할 권리’를 침해할 수 있다. 따라서 유럽에서 AI 서비스를 제공하는 것은 궁극적으로 개발자를 유럽 저작권법과 유럽 법원의 관할권에 종속시킨다. 이런 점에서 이 연구는 지금까지 우리가 지적재산의 도난을 다루고 있다는 사실을 기술적, 이론적으로 증명한 것이며, 인간의 창의성 보호와 AI 혁신 간의 더 나은 균형이 필요하다는 사실을 분명히 밝고 있다. 독일기자협회 미카 보이스터 (Mika Beuster) 회장은 “이 연구를 통해 빅테크 기업의 AI 훈련 관행이 불법이라는 점을 확인”하는 한편, “앞으로 깨끗한 ’그린(Green) AI’는 우리 사회의 가치와 규범을 반영하고 법적 위반을 사전에 방지해야 한다”고 주장했다(DJV, 2024. 9).",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>인간을 위한 인공지능의 기술적 탐구</span>"
    ]
  },
  {
    "objectID": "journalism.html#미디어-법학자들의-인공지능-기술-탐구",
    "href": "journalism.html#미디어-법학자들의-인공지능-기술-탐구",
    "title": "9  인간을 위한 인공지능의 기술적 탐구",
    "section": "",
    "text": "&lt;표 1&gt; 미디어 법학자들의 AI 저널리즘에 대한 논의와 견해\n\n\n\n\n\n\n\n\n\n주요 이슈\n관련 내용\n참고\n\n\n\n\n기본법\n·언론의 개념\n·의사 소통 과정에 출판물이 관련되어 있다면 언론의 개념으로 보호의 대상\n·인간의 개입 없이 독자적으로 저널리즘 작업을 수행할 수 없음\n\n\n\n·언론자유에 의미와 보호 범위\n·전자적 배포 형식의 로봇 저널리즘도 언론의 보호 대상이 됨\n\n\n\n\n\n·기본법상의 기본권인 ’언론의 자유’는 언론 종사인 ’사람’을 위한 인권임\n\n\n\n노동법\n·노동법과의 관련성\n·저널리즘 로봇은 자연인이 아니므로 노동법과는 무관함\n·노동권은 사람을 위한 인권임\n\n\n\n\n·보조적 도구가 저널리스트의 인격권을 침해하는지 법적 책임\n\n\n\n저작권법\n·저작물의 보호 대상\n·저작권법은 인간의 창작 활동에 의한 저작물을 보호의 대상으로 함\n·저널리즘에 사용되는 기계나 컴퓨터의 활용은 보조수단임\n\n\n\n\n·저널리즘에 사용되는 로봇은 보조수단이므로 저작권법상 저작자에서 제외됨\n\n\n\nEU\n·AI 개념 정의\n·로봇 저널리즘과 AI 저널리즘의 개념 구분\n·세계 첫 AI 법\n\n\nAI 법\n·규제의 범위\n·국가 개입의 정도\n\n\n\n\n\n·위험등급을 구분하고,\n\n\n\n\n\n라벨링의 필요성 제시\n\n\n\n\n\n·산업적 개발 필요",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>인간을 위한 인공지능의 기술적 탐구</span>"
    ]
  },
  {
    "objectID": "journalism.html#미디어-경제학자들의-인공지능-기술-탐구",
    "href": "journalism.html#미디어-경제학자들의-인공지능-기술-탐구",
    "title": "9  인간을 위한 인공지능의 기술적 탐구",
    "section": "9.2 미디어 경제학자들의 인공지능 기술 탐구",
    "text": "9.2 미디어 경제학자들의 인공지능 기술 탐구\n미디어 경제학에서는 디지털 미디어의 산업 구조와 자원을 근본적으로 변화시키고 판매 시장에 영향을 미치는 디지털 플랫폼에 주목한다(Caraway, 2023). 디지털 플랫폼은 다양한 행위자 간의 중개자 역할을 하는 플랫폼 경제를 의미한다. 플랫폼 경제에서 중요한 요소는 직간접적인 행태로 발생하는 네트워크 효과이다. 직접적인 네트워크 효과는 네이버와 같이 사용자가 많아질수록 사용자에 대한 플랫폼 가치가 높아질 때 발생한다. 간접적인 네트워크 효과는 관련된 사용자의 행위가 많을수록 이들의 활동이 플랫폼의 가치를 높일 때 발생한다.4 이러한 역동성은 비용 우위를 통해 지배적인 플랫폼이 등장하여 사용자 기반의 상당 부분을 통제하고 시장을 지배하는 독점으로 이어진다(Peitz, 2006; Fuchs, 2015; Yun, 2024). 디지털 미디어 환경의 독점화는 시장 점유율이 소수의 대기업에 집중되는 것을 의미한다. 따라서 소수의 강력한 플레이어가 시장을 지배하고, 잠재적으로 경쟁 부족 및 독점이윤으로 이어질 수 있다(Litschka & Tschulik, 2019; Yun, 2024).\n오늘날 우리 생활에서 데이터가 차지하는 비중은 상당하다. 미디어 경제학에서는 디지털 미디어 환경을 고찰하면서 데이터의 특성과 데이터의 생산, 유통 및 소비 환경에 주목한다. 이때 데이터는 지식과 정보가 포함된 결과물로서 디지털 미디어 환경에서 한번 생산되고 나면 매우 쉽고, 간단히 전달되는 재화(good)이다. 이처럼 한번 생산된 데이터는 정치, 사회, 경제, 문화 등 관련 영역에서 주체들이 고유한 활동을 하면서 발생시키는 결과물이며, 동시에 다른 주체들에게 필요한 (리)소스가 된다. 따라서 ’시장’에서는 지식과 정보가 포함된 데이터가 ’상품’으로 생산, 유통 및 소비되는 반면, ’공적 영역’에서는 공개와 협력 및 교류를 통해 전체 사회를 위한 데이터 ’재화’가 생산, 분배되고 있다. 이런 점에서 미디어 경제학, 특히 미디어 정치경제학5에서는 디지털 미디어 상품의 가치와 가격에 대한 분석을 통해 디지털 미디어 환경에서 한번 생산된 데이터가 상품화, 독점화되는 시장의 구조를 검토하는 한편, 시장이 아닌 공공 영역에서 한번 생산된 데이터가 공공재화로 생산, 분배되고 있는 데이터 저널리즘의 사례들을 제시하고 있다(Fuchs, 2015; Yun, 2024). 다시 말해, 미디어 정치경제학에서는 디지털 미디어 상품의 가치와 가격에 대한 분석을 통해, 데이터가 디지털 환경에서 추가 생산비용 없이 재생산된다는 점에서 데이터의 경제적 가치를 고찰하고, 이를 통해 데이터가 상업적인 방식이 아닌 공공재화로서 생산, 분배될 수 있는 가능성을 주장하고 있다.\n한편, 지식과 정보가 포함된 데이터 분석은 데이터와 저널리즘이 어떻게 관련되는지 데이터 저널리즘의 형태로 확대, 연결된다. 이는 빅데이터, 알고리즘, 또는 AI 기술이 컴퓨터공학이나 정보관리 시스템, 그리고 도서관학 연구자들의 연구 대상을 넘어 미디어 커뮤니케이션 연구 일반에서 주요하게 고찰하고 있는 연구 대상, 즉 데이터 저널리즘, 컴퓨테이션 저널리즘 및 AI 저널리즘을 말한다. 예를 들면, 뉴스가 하나의 데이터로 생산, 전달되는 데이터 저널리즘의 생태계에서 시장의 지배력이 막강한 빅테크 플랫폼 기업은 이미 뉴스 데이터를 상업적인 알고리즘으로 설계, 운영하면서 독점이윤을 획득하고 있다. 하지만 다른 한편에서는 높은 사용가치가 있는 뉴스 정보를 생산하기 위해 일정 부분 투여됐던 최소한의 생산비용이나 노동비용조차 제대로 회수하지 못하는 경우가 많다. 간단히 말해, 한번 생산된 뉴스 데이터는 인터넷 공간에서 기술적인 차단이나 법률적인 개입, 예를 들어 저작권이나 불법이라는 인식이 없다면 누구나 쉽고, 간단히 사용할 수 있는 공공재화가 된다. 하지만 인터넷과 디지털 기술이 발전하면서 뉴스 콘텐츠와 같은 생산물을 사적으로 소유하고, 지배하기 위해 기술적 차단 방식이 개발되고, 강력한 법률적 개입이 증가하고 있다. 물론 뉴스 콘텐츠와 같은 데이터를 생산하려면, 어느 정도의 생산비용이 요구되고, 창작자를 위한 합당한 분배가 보장되어야 한다. 그런데 현실 세계에서 확인되는 미디어 시장의 상품화, 독점화 현상은 시장의 지배력을 지닌 빅테크 플랫폼 기업이나 대형 언론사 또는 관련 시장에서 나름대로 크고, 작은 영향력을 행사하는 기업들이 본래의 생산비용이나 생산에 투여된 노동보다 높은 이윤을 획득하고 있다. 반면, 디지털 미디어 환경에서 아무런 영향력이 없거나 법적 권리를 주장하기 힘든 창작자나 영세 기업들은 최소한의 생산비용조차 회수하기 불가능한 것이 현실이다.\n따라서 미디어 정치경제학자들이 데이터 기반의 컴퓨테이션 저널리즘 또는 AI 저널리즘에 주목하는 이유는 미디어 기업이 보다 많은 작업에서 데이터를 활용하고 있고, 이러한 데이터를 더 많이 공유할수록 전체 사회 또는 인류에 더욱 많은 혁신과 복리가 증대될 것으로 예상하기 때문이다(Diakopoulos, 2016). 실례로 디지털 미디어 환경에서 데이터를 공유하는 것은 미디어 기업의 경영 측면에서도 새로운 수익원이 된다. 이는 리눅스 운영체제 관련 기업에서 오픈 데이터인 리눅스를 통해 수익성이 증가한 사실에서 확인된다. 오픈 데이터(소스)란 어떤 소프트웨어의 설계도라 할 수 있는 소스 코드와 디자인 문서, 그리고 제품의 콘텐츠 등을 누구나 사용할 수 있도록 공개하는 것을 말한다. 이렇게 공개된 소스 코드는 디지털 환경에서 누구나 접근할 수 있으며, 또 다른 사람이 공개된 소스 코드 위에 새로운 코드를 만들어 발전된 소프트웨어의 코드로 재공개된다.\n디지털 미디어 환경에서 데이터의 경제적 가치를 고찰하는 미디어 정치경제학자들은 상업화되고 독점화되는 인터넷 플랫폼이 우리 사회의 민주주의를 위협한다며 비판적 시각에서 공공서비스 인터넷과 공공서비스 알고리즘, 나아가 공공서비스 미디어의 가능성을 제시하고 있다(Fuchs & Unterberger, 2021). 공공서비스 알고리즘이란 공개와 협력 및 교류를 통해 전체 사회를 위한 알고리즘을 상품이 아닌 ’재화’로써 생산, 분배하는 것이다. 이를테면, 독일의 제2 공영방송 체데에프(ZDF)는 자사의 추천 알고리즘 방식을 YouTube나 Google의 상업적 알고리즘과 차별화하고 있다. 체데에프에서 추천 알고리즘을 개발하고 있는 안드레아스 그륀(Andreas Gruen)에 따르면, 체데에프의 알고리즘은 매우 유사한 콘텐츠의 수를 제한하고 몇 가지 프로그램의 동영상만 제공하지 않는다(Krei, 2022). “우리는 최신 선호도와 인기 선호도를 대응하기 위해 더 긴 시간과 더 많은 사용 데이터를 샘플링한다. 그리고 우리가 만든 알고리즘을 공개하고, 이를 통해 사람들에게 새로운 주제에 관심을 두게 만든다.” 이것이 공영방송 체데에프가 상업 미디어와 차별화하는 방식인데, 직접 알고리즘을 개발하고, 알고리즘의 운영 방식을 공개할 때 투명성을 유지하는 것이다. 즉, 데이터와 알고리즘이 공개되고, 서로 협력하여 교류할 때, 새로운 가치를 창출하게 된다는 미디어 정치경제학의 설명은 인류가 이룩한 과학기술의 성과는 소수 기업이 독점할 수 있는 배타적인 성과가 아니다. 이들의 주장은 데이터와 알고리즘이 지닌 객관적인 기술적 특성에서 기인하며, 시장이 아닌 공적 영역에서 더욱 실현 가능하다는 사실로 제시되고 있다. 다음 &lt;표 2&gt;는 미디어 경제학에서 데이터와 알고리즘 그리고 AI 기술을 설명하는 주요 내용을 개괄하고 있다.\n\n&lt;표 2&gt; 미디어 경제학에서 데이터, 알고리즘 및 AI를 설명하는 내용\n\n\n\n\n\n\n\n\n\n\n기술적 활용\n데이터의 특성과 활용\n저널리즘의 활용\n참고\n\n\n\n\n데이터\n·빅데이터\n·지식과 정보가 포함\n·뉴스 콘텐츠는 하나의 데이터이며 소스이자 코드\n·공공데이터\n\n\n\n·오픈 데이터\n·한번 생산하고 나면 쉽고, 간단히 전송\n·데이터 저널리즘\n·상업데이터\n\n\n\n\n·저장이 가능함\n·빅데이터 활용(생산, 유통, 판매): 데이터 시각화\n·사적 소유 가능\n\n\n\n\n\n·뉴스 데이터 기반의 플랫폼 서비스 증가(포털, 블로그, SNS, 검색엔진)\n\n\n\n알고리즘\n·콘텐츠 기반 필터링\n·데이터 기반의 추천 알고리즘\n·필터버블: 편향성\n·공공 알고리즘\n\n\n\n·협업 필터링\n·선별된 데이터 서비스\n·개인화 서비스\n·협력, 공개, 투명\n\n\n\n\n\n·편향성, 차별성 우려\n·신뢰, 만족\n\n\n\n\n\n\n·양극화 우려\n\n\n생성 AI\n·머신러닝\n·데이터는 주요한 원료\n·AI 저널리즘\n·사적 소유 불가\n\n\n\n·텍스트 & 데이터 마이닝\n·사용(최적화) 후 원재료는 불필요\n·로봇 저널리즘\n·공정 이용\n\n\n\n·거대언어모델(LLM)\n\n·뉴스 데이터 기반의 빅테크 기업의 개인화 서비스 극대화\n·개인화,독점화 고조",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>인간을 위한 인공지능의 기술적 탐구</span>"
    ]
  },
  {
    "objectID": "journalism.html#독일-저널리스트들의-인공지능-기술-탐구",
    "href": "journalism.html#독일-저널리스트들의-인공지능-기술-탐구",
    "title": "9  인간을 위한 인공지능의 기술적 탐구",
    "section": "9.3 독일 저널리스트들의 인공지능 기술 탐구",
    "text": "9.3 독일 저널리스트들의 인공지능 기술 탐구\n저널리즘 분야에서 인공지능을 활용하는 것은 의견과 여론형성, 나아가 국가와 사회에 영향을 미칠 수 있는 잠재력이 있다. 또한, 언론인의 업무에 심각한 영향을 미칠 것이다. 따라서 저널리즘에 인공지능을 활용하려면 자유민주주의와 저널리즘 직업을 위태롭게 하지 않도록 적절한 가이드라인이 절대적으로 필요하다(DJV, 2024.4). 이러한 지침, 즉 객관적인 원칙은 도로 위의 자율주행차로부터 사람들을 보호해야 하는 것처럼, 저널리즘 분야에서도 인공지능의 무분별한 사용을 막을 수 있게 한다.\n독일기자협회(DJV)는 AI 저널리즘은 한 신문사에 국한된 문제가 아니라 소통과 의견형성에 신기술이 활용되는 문제로서 국가와 사회에 영향을 미칠 수 있다는 견해를 밝힌다. 무엇보다 AI 응용 프로그램은 윤리 및 가치 체계와 거리가 멀게 작동하므로 언론사와 저널리스트가 항상 가지고 있던 비판적 감시 기능을 수행할 수 없으며, 이익을 추구하는 미디어 기업은 저널리즘적 품질보다 AI의 경제적 잠재력을 더 높이 평가할 수 있다는 게 협회의 입장이다. 나아가 협회는 악셀 슈프링거사(Axel Springer)와 같은 거대 언론사가 오픈AI사와 긴밀한 협력 관계에서 언론 활동을 할 때, 저널리스트를 위한 AI 수익의 공정한 분배를 요구한 바 있다.6 이는 2024년 1월, 협회 성명서에서 확인되는데, “악셀 슈프링거가 판매하는 것은 저자의 지적 작품이기 때문에 언론인들은 적절한 몫을 받아야 한다”는 입장을 분명히 했다(DJV, 2024. 1.). 아래 &lt;표 3&gt;은 저널리즘 분야에서 인공지능 활용에 대한 독일기자협회의 공식적인 입장을 개괄적으로 나열하고 있다.\n한편, 독일의 제1공영방송 아아르데(ARD) 산하의 바이에른 공영방송(BR)은 AI 프로그램 개발을 위해 10가지 원칙을 제시한 바 있다(BR, 2024.7.). 여기에서는 AI 활용의 윤리적 문제가 포함되는데, 투명성과 자원에 대한 책임감, 협력과 토론이 수반되는 평가, 그리고 알고리즘 편향에 대응하고 사회적 다양성을 반영하기 위한 노력이 강조된다. 바이에른 공영방송은 더 나은 저널리즘을 수행하기 위해 인공지능의 잠재력 이점과 위험을 고려하면서, 인간과 인공지능 사이의 건설적인 상호작용의 가능성을 모색하고 있다. 이때 새로운 기술이 가져다줄 투명성과 다양성, 그리고 지역성은 기자와 사용자를 위한 이익과 가치가 된다.\n바이에른 공영방송은 기술이 그 자체로 끝나는 것이 아니라 더 나은 저널리즘을 수행하는 데 도움이 되는 도구로 사용됨을 강조한다. “우리는 바이에른 공영방송과 사용자에게 실질적인 부가가치를 제공하는 기술만을 사용한다”는 목적을 분명히 밝히고 있다. 이때 새로운 기술은 인공지능과 기타 모든 형태의 자동화 기술까지 확대, 적용된다. 그리고 이와 같은 새로운 기술의 적용은 다음 &lt;표 4&gt;와 같은 윤리 지침을 토대로 운영된다.\n\n&lt;표 3&gt; AI 저널리즘에 대한 독일기자협회의 입장(2024년 4월 기준)\n\n\n\n\n\n\n\n주요 입장\n내용 요약\n참고\n\n\n\n\n인간의 성과를 대체할 수 없음\n·미디어 회사는 언론인의 일자리를 대처하기 위한 목적으로 인공지능을 사용할 수 없음\n새로운 기술은 인간을 위해 개발함\n\n\n\n·인공지능과 사람 간의 상호작용은 여전히 중요함\n\n\n\n\n·완전히 자동화된 메시지 생성과 배포는 피하고, 콘텐츠의 책임은 사람에 있음\n\n\n\n콘텐츠에 대한 책임\n·자동화 및 AI 사용을 통해 생성된 게시물이 인간 기자의 참여나 개입 없이 게시되면 안 됨\n기계가 아닌 사람에게 책임이 있음\n\n\n\n·편집팀은 저널리즘적 책임을 짐\n\n\n\n\n·언론사는 자사의 AI 행동 강령을 준수하고 불만 사항을 담당하는 책임자를 임명해야 함\n\n\n\n데이터 자료의 책임 있는 처리\n·편집팀은 데이터에 관한 책임과 정확성을 보장함\n상업용 빅테크기업과 독립성 보장함\n\n\n\n·데이터의 불완전성, 왜곡 및 기타 오류는 즉시 수정되고, 개인 데이터는 관련 보호법을 준수해야 함\n\n\n\n\n·미디어 회사는 자체적으로 데이터를 구축하고, 당국과 정부 기관의 공개데이터 프로젝트를 지원해야 함\n\n\n\n투명성 및 라벨링\n·인공지능의 생성 콘텐츠는 표시해야 함\n입법부에 라벨링 법제화를 요구함\n\n\n\n·자동화 기술은 일부 공개되어야 함\n\n\n\n\n·저작자 보상을 위해 데이터 출처를 명시함\n\n\n\n책임 있는 개인화\n·필터버블을 피함\n사회적 다양성과 투명성을 보장함\n\n\n\n·편집팀이 선별한 콘텐츠 제공을 보장함\n\n\n\n\n·사용된 알고리즘을 사용자에게 공개함\n\n\n\n\n·사용자는 알고리즘 기준을 변경하고, 개인화된 배포를 선택, 취소할 수 있어야 함\n\n\n\n인증된 AI 시스템 사용\n·각 언론사에서 활용되는 AI시스템에 대한 인증 개발을 지지하고, 지원함\n저널리즘의 품질 개선과 표준화\n\n\n\n·품질, 균형, 차별 금지, 데이터보호 및 보안이 특정 표준을 충족해야 함\n\n\n\n지속적인 검토\n·미디어 기업은 저널리즘 분야에서 인공지능 활용과 영향을 지속해서 검토해야 함\n언론사 및 미디어 기업의 사회적 책임 강화\n\n\n평생 교육\n·새로운 기술에 대한 언론인 훈련과 교육 제공\n저널리스트를 위한 기술 활용\n\n\n\n·인공지능에 대한 인식과 활용의 기회 확대\n\n\n\n적절한 보상\n·제작자인 언론인에게 적절한 보상이 필요함\n저널리스트를 위한 보상\n\n\n\n·언론인의 AI 시스템 개발과 사용을 위해, 보상을 의무화하도록 입법부에 요구하고 있음\n\n\n\n\n(자료출처: 독일기자협회 홈페이지, https://www.djv.de/medienpolitik/kuenstliche-intelligenz/)\n한편, 독일의 제1공영방송 아아르데(ARD) 산하의 바이에른 공영방송(BR)은 AI 프로그램 개발을 위해 10가지 원칙을 제시한 바 있다(BR, 2024.7.). 여기에서는 AI 활용의 윤리적 문제가 포함되는데, 투명성과 자원에 대한 책임감, 협력과 토론이 수반되는 평가, 그리고 알고리즘 편향에 대응하고 사회적 다양성을 반영하기 위한 노력이 강조된다. 바이에른 공영방송은 더 나은 저널리즘을 수행하기 위해 인공지능의 잠재력 이점과 위험을 고려하면서, 인간과 인공지능 사이의 건설적인 상호작용의 가능성을 모색하고 있다. 이때 새로운 기술이 가져다줄 투명성과 다양성, 그리고 지역성은 기자와 사용자를 위한 이익과 가치가 된다.\n바이에른 공영방송은 기술이 그 자체로 끝나는 것이 아니라 더 나은 저널리즘을 수행하는 데 도움이 되는 도구로 사용됨을 강조한다. “우리는 바이에른 공영방송과 사용자에게 실질적인 부가가치를 제공하는 기술만을 사용한다”는 목적을 분명히 밝히고 있다. 이때 새로운 기술은 인공지능과 기타 모든 형태의 자동화 기술까지 확대, 적용된다. 그리고 이와 같은 새로운 기술의 적용은 다음 &lt;표 4&gt;와 같은 윤리 지침을 토대로 운영된다.\n\n&lt;표 4&gt; 바이에른 공영방송의 AI 및 자동화 처리에 대한 윤리 지침(2024년 7월 버전)\n\n\n\n\n\n\n\n윤리 지침\n주요 내용\n참고\n\n\n\n\n1. 사용자를 위한 부가가치\n·AI 시스템은 작업 효율을 위해 사용하고, 기여자가 위임한 리소스를 책임감 있게 사용함\nAI 도입과 사용 목적\n\n\n2. AI에 대한 정확한 설명\n·인공 생명체에 대한 오해를 줄이고, 기술의 기능을 허위적으로 묘사하는 것을 피함\n활용 기술의 범주 구분\n\n\n3. 편집 통제 및 투명성\n·생성된 콘텐츠 책임은 사람과 편집팀에 있음\n책임과 의무\n\n\n\n·사용하는 기술, 허용 가능한 위험과 제한, 데이터의 역할 및 담당 편집팀에 대한 공개\n\n\n\n\n·생성된 콘텐츠에 표시(라벨링)하고, 접근 방식을 게시함\n\n\n\n4. 영향 평가\n·AI 사용의 효과와 부작용을 사전에 평가함\n관리 가능한 시스템 사용\n\n\n\n·언론 기준과 해당 법률에 부합하는지 평가함\n\n\n\n\n·가능한 작업에 접근하도록 공개하고, 오픈소스 소프트웨어를 사용함\n\n\n\n\n·타사 소프트웨어 사용 시 윤리적 고려 사항과 영향 평가를 통합함\n\n\n\n5. 다양성과 지역성\n·콘텐츠에 대한 더 큰 포용과 장벽 없는 접근의 기회로 AI를 활용함\n공영방송의 역할 확대\n\n\n\n·잠재적이고 차별적인 고정관념을 고려하면서 데이터를 다루고, 지역성 강화를 위해 노력함\n\n\n\n6. 의식적인 데이터 문화\n·신뢰 가능한 데이터를 평가, 사용하며, 데이터 보호 규정의 틀에서 개발, 활용됨을 직원들에게 교육함\n데이터 관리\n\n\n7. 책임 있는 개인화\n·개인화는 사회적 교류와 결속력을 훼손하지 않고, 원치 않는 필터버블 효과를 방지함\n상업 미디어 기업과 차별화함\n\n\n\n·공공 서비스 추천 알고리즘 개발에 전념함\n\n\n\n8. 학습 문화\n·제품과 정책 개발을 위해 파일럿 프로젝트와 프로토타입을 통한 학습 환경 보장\n개발, 교육\n\n\n\n·언론인, 개발자, AI 전문가와 경영진으로 구성된 연구팀에서 정기적인 경험과 윤리적 경계선을 모색함\n\n\n\n9. 교류 및 파트너십\n·대학과 협력하고, 과학연구소 및 AI 윤리 전문가와의 교류를 모색함\n교류, 홍보\n\n\n\n·과학기술 및 지역 신생 기업과 협력하고, 전문 지식을 활용해서 프로젝트로 연결, 홍보함\n\n\n\n\n(자료출처: 바이에른 공영방송(BR) 홈페이지, https://www.br.de/extra/ai-automation-lab/ki-ethik-100.html)\n위에서 살펴본 바이에른 공영방송의 윤리 지침은 2024년 7월 개정된 내용으로서, 2020년 11월에 작성되었던 첫 번째 버전에서 몇몇 내용을 수정, 보완한 것이다. 대표적으로 위에서 살펴본 지침서 2번에 해당하는 AI에 대한 분명한 개념 정의이다. 지침서에 따르면,\n\n“AI(인공지능)라는 용어는 상황에 따라 매우 다른 의미가 있을 수 있다. AI는 지능적인 행동과 유사한 컴퓨터의 기능 집합으로 이해될 수 있습니다.”지능형”이 무엇을 의미하는지, 어떤 기술이 사용되는지는 명시되어 있지 않다. 실제적인 이유로 우리는 특정 작업을 수행하기 위해 훈련이 필요한 컴퓨터 시스템으로 정의를 제한한다. “머신러닝” 또는 독일어로 “기계 학습”이라고 한다”(BR, 2024. 7.).\n\n바이에른 공영방송과 달리 베를린-브란데부르크 지역에 있는 공영방송 에르베베(rbb: Rundfunk Berlin-Brandenburg)에서는 인공지능을 다루기 위한 원칙을 비교적 간단, 명료하게 제시하고 있다(2024년 7월 버전 기준). 에르베베에 따르면, AI를 사용하는 목적은 자사의 공적 의무를 이행하기 위함인데, 이를 위해 저널리즘 연구의 가능성을 확대하고, 다양한 콘텐츠를 현대적이고 매력적인 방식으로 생산, 배포하며 동시에 행정 업무 프로세스를 더욱 효율적으로 만들기 위해 AI를 사용한다(rbb, 2024). 이때 저널리즘의 의무와 데이터 및 출처 보호는 가장 주요하게 고려되며, AI를 사용하기 전에 발생 가능한 위험들은 주의 깊게 검토되어야 한다. 한편, 모든 콘텐츠가 공개되기 전에 사람의 검토와 승인이 필요하며, 담당 편집팀이 모니터링하면서 저널리즘적 관리가 더욱 요구된다. 나아가 아직 라벨링과 관련하여 정확한 기준을 마련한 것은 아니지만, 에르베베는 인공지능 사용의 투명성은 자사에 대한 사용자의 신뢰에 연결되기 때문에 AI 기술을 통해 생성된 콘텐츠에는 사용자가 이해할 수 있는 표시(라벨링)를 분명히 할 계획이다. 이와 같은 향후 계획은 일상적인 편집과 운영 과정에서 인공지능 기술을 올바로 이해하고 투명하게 활용하기 위한 규칙, 즉 법적 틀에 관한 지속적인 논의로 확대될 전망이다.\n지금까지 살펴본 바와 같이 독일의 언론 종사자들은 AI와 관련된 가이드라인을 만들기 위해 다음과 같은 내용에 주목하고 있다.\n\n하나, 저널리즘에 인공지능을 활용하려면 적절한 지침, 즉 가이드라인이 필요한데, 무엇보다 투명성의 의무가 강조된다. 여기서 투명성의 의무는 다시 두 가지 측면에서 확인된다.: 첫째, AI를 사용해서 저널리즘 콘텐츠를 생산할 때, 생산자와 이용자는 이를 인식할 필요가 있고, 둘째, AI가 어떤 데이터 소스를 사용했는지, 어떤 자료를 사용해서 훈련했는지 명확해야 공개할 필요가 있다.\n둘, 작성된 콘텐츠가 AI 교육을 목적으로 사용될 경우라도 원 작성자와 소통을 하는 한편, 적절한 보상을 할 수 있도록 해야 한다.\n셋, 저널리즘 분야에서 인공지능의 활용은 품질, 균형, 차별 금지, 데이터 및 출처 보호, 그리고 저작권 및 보안 측면에서 최소한의 기준을 충족하는 사회적으로 인증된 AI 시스템이 사용되어야 한다.\n넷, 인공지능의 활용은 무엇보다 자체적으로 제어가 가능한 기술을 저널리즘 분야에서 활용하고, 유럽연합 기반의 자체적인 인프라를 보유하는 것이 바람직하다.\n다섯, 나아가 신속하고 단호한 규제가 AI 저널리즘 분야에 필요하다. 실례로 디지털 단일시장 저작권지침(DSM-RL: Digital Single Market Copyright Directive)이 제정된 후 유럽의회에서 채택되기까지 10년 이상이 걸렸고, 그 이후 국내법으로 전환되는데 별도의 시간이 소요되었다. 따라서 AI를 규제하기 위해서는 시기적절한 규제 정책이 필요하다.\n\n독일의 저널리즘과 독일 사회는 AI 기술을 최대한 신속하고 현명하게, 그리고 건전하게 사용하기 위한 원칙을 요구하고 있다. 이러한 원칙은 입법부와 행정부가 제안하는 법률안과 규제 정책, 나아가 앞에서 살펴본 바와 같이 언론사 내부에서 설정, 제시하는 가이드라인을 통해 구체화한다. AI 기술이 저널리즘과 민주주의를 발전시킬지 아니면 파괴할지 ​​여부는 이제 입법부의 손에 달려 있다(DJV, 2024. 3).",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>인간을 위한 인공지능의 기술적 탐구</span>"
    ]
  },
  {
    "objectID": "journalism.html#마치며-인간을-위한-인공지능-기술",
    "href": "journalism.html#마치며-인간을-위한-인공지능-기술",
    "title": "9  인간을 위한 인공지능의 기술적 탐구",
    "section": "9.4 마치며: 인간을 위한 인공지능 기술",
    "text": "9.4 마치며: 인간을 위한 인공지능 기술\n우리 사회가 어떤 AI 윤리를 마련할 것인지 이제 공은 정치인들의 법정에 있다. 정치인들은 기업이 아닌 사람에게 필요한 결론을 도출하고, 언론인과 다른 창작자들의 희생을 끝내야 할 것이다. 물론 정치인들은 급변하는 AI 기술에 대해 전체 시민들이 무엇을 인식하고, 어떤 문제를 제기하며, 시민들이 어떤 미래를 꿈꾸는지 상호작용하며 필요한 결론을 도출해 갈 것이다. 따라서 인간에게 필요한 정치적 결론은 역설적으로 인간이 현실 세계를 분명히 인식하고, 자신에게 필요한 사안을 정치적으로 요구할 때 실현할 수 있다. 이런 점에서 빠르게 발전하는 AI 기술을 공정하게 사용하려면, 무엇보다 AI 기술에 관한 객관적인 이해와 탐구가 필요하다. 지금까지 살펴본 바와 같이 다양한 관점과 학제 간의 연구들은 AI 기술이 기업이 아닌 사람, 즉 인간을 위한 인공지능 기술이라는 대원칙을 제시하고 있다.",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>인간을 위한 인공지능의 기술적 탐구</span>"
    ]
  },
  {
    "objectID": "journalism.html#더-생각해볼-문제",
    "href": "journalism.html#더-생각해볼-문제",
    "title": "9  인간을 위한 인공지능의 기술적 탐구",
    "section": "9.5 더 생각해볼 문제",
    "text": "9.5 더 생각해볼 문제\n\nAI 기술이 보편화하는 오늘날 인공지능 윤리와 도덕적 가치가 논의되는 이유는 무엇인가?\n인공지능 윤리와 도덕 외에 AI 기술을 공정하게 사용하려는 우리의 원칙은 무엇인가?\n미디어 커뮤니케이션 연구 일반, 특히 저널리즘 연구에서 ‘공정’과 ‘공익’, 그리고 ’공공’에 관한 개념 정의는 AI 기술의 ’공정’과 ’공익’으로 어떻게 확대, 적용될 수 있는가?\nAI 기술의 불공정성이 상업화된 생산방식에서 기인한다면, 비상업화된 생산방식에서 AI 기술은 자본이 아닌 인간을 위해 사용될 수 있는가?",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>인간을 위한 인공지능의 기술적 탐구</span>"
    ]
  },
  {
    "objectID": "journalism.html#더-읽을거리",
    "href": "journalism.html#더-읽을거리",
    "title": "9  인간을 위한 인공지능의 기술적 탐구",
    "section": "9.6 더 읽을거리",
    "text": "9.6 더 읽을거리\n김태균, 권영전, 성서호, 박주현 (2023). &lt;챗GPT와 생성 AI 전망&gt;. 커뮤니케이션북스\n로버트 W. 맥체스니. 전규찬 옮김 (2014). &lt;디지털 디스커넥트: 자본주의는 어떻게 인터넷을 민주주의의 적으로 만들고 있는가&gt;. 삼천리.\n이재신 (2022). &lt;인공지능 알고리즘과 다양성 그리고 편향&gt;. 커뮤니케이션북스\nBettig, Ronald (1996). Copyrighting Culture: The Political Economy of Intellectual Property. Taylor & Francis.\nYun, Jang-Ryol (2024). The Value and Price of Digital Media Commodities. Media, Culture & Society, 46(2), 219-234. https://doi.org/10.1177/01634437231188464",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>인간을 위한 인공지능의 기술적 탐구</span>"
    ]
  },
  {
    "objectID": "journalism.html#참고문헌",
    "href": "journalism.html#참고문헌",
    "title": "9  인간을 위한 인공지능의 기술적 탐구",
    "section": "9.7 참고문헌",
    "text": "9.7 참고문헌\nBayerischer Rundfunk (2024. 7. 12.). Umgang mit Kuenstlicher Intelligenz: Unsere KI-Richtlinien im Bayerischen Rundfunk. https://www.br.de/extra/ai-automation-lab/ki-ethik-100.html\nBusiness Insider (2023. 12. 17.). Large AI models can now create smaller AI tools without humans and train them like a ‘big brother,’ scientists say https://www.businessinsider.com/large-models-can-create-new-smaller-ai-tools-scientists-2023-12\nCaraway, Brett (2023). Digital Media Economics: A Critical Introduction. SAGE Publications.\nCoddington, Mark (2015). Clarifying Journalism’s Quantitative Turn. Digital Journalism, 3(3), 331-348.\nDahm, M. H. & Twesten, N. (2023). Der Artificial Intelligence Act als neuer Maßstab für künstliche Intelligenz. Das Spannungsfeld zwischen Regulatorik und Unternehmen. Wiesbaden. https://doi.org/10.1007/978-3-658-42132-8\nDeutscher Journalisten-Verband (2024). Positionspapier bezueglich des Einsatzes Kuenstlicher Intelligenz im Journalismus.\nDeutscher Journalisten-Verband (2024. 1. 3.). KI-Erlöse fair verteilen https://www.djv.de/news/pressemitteilungen/press-detail/ki-erloese-fair-verteilen/\nDeutscher Journalisten-Verband (2024. 4. 24). DJV fordert klare Regeln https://www.djv.de/news/pressemitteilungen/press-detail/djv-fordert-klare-regeln/\nDeutscher Journalisten-Verband (2024. 8. 28). Inhalte nicht an KI verschenken https://www.djv.de/news/pressemitteilungen/press-detail/inhalte-nicht-an-ki-verschenken/\nDeutscher Journalisten-Verband (2024. 9. 30). DJV fordert Green-KI https://www.djv.de/news/pressemitteilungen/press-detail/djv-fordert-green-ki/\nDornis, Tim W. & Sebastian Stober (2024). Urheberrecht und Training generativer KI-Modelle: technologische und juristische Grundlagen.\nDiakopoulos, Nicholas (2016). Computational journalism and the emergence of news platforms. In B. Franklin & S. Eldridge II (Eds.), The Routledge companion to digital journalism studies (pp. 176–184). Abingdon: Routledge.\nFuchs, Christian (2015). Reading Marx in the Information Age. Routledge.\nFuchs, Christian and Klaus Unterberger (2021). The Public Service Media and Public Service Internet Manifesto. University of Westminster Press. pp. 7-18.\nHagendorff, Thilo (2020). The Ethics of AI Ethics: An Evaluation of Guidelines. Minds and Machines. 30, 99-120.\nInitiative Urheberrecht (2024. 10. 1.). Interdisziplinäre Studie belegt Urheberrechtsverletzungen beim Training generativer KI https://urheber.info/diskurs/interdisziplinare-studie-belegt-art-und-umfang-der-urheberrechtsverletzungen-beim-training-generativer-ki\nKrei, Alexander (2022). DWDL.de-Interview mit Eckart Gaddum und Andreas Gruen: Den öffentlich-rechtlichen Auftrag in Algorithmen übersetzen. https://www.dwdl.de/interviews/88806/den_oeffentlichrechtlichen_auftrag_in_algorithmen_uebersetzen/?utm_source=&utm_medium=&utm_campaign=&utm_term=\nLitschka, Michael & Sebastian Tschulik (2019). Der Social Choice der Selbstregulierung–ein vertragstheoretischer Versuch in Zeiten ökonomisierter und mediatisierter conditio humana. Der Mensch im digitalen Zeitalter: Zum Zusammenhang von Ökonomisierung, Digitalisierung und Mediatisierung, 87-102\nLitschka, Michael & Tassilo Pellegrini (2019). Considerations on the Governance of Open Data – an Institutional Economic Perspective. International Journal of Intellectual Property Management, 9(3-4), 247-263.\nPeitz, Martin (2006). Marktplätze und indirekte Netzwerkeffekte. Perspektiven der Wirtschaftspolitik, 7(3), 317-333.\nRundfunk Berlin-Brandenburg (2024). RBB KI GRUNDSAETZE: GRUNDSAETZE ZUM UMGANG MIT KUENSTLICHER INTELLIGENZ IM RBB.\nWeberling, Johnnes (2018). Medienrechtliche Bedingungen und Grenzen des Roboterjournalismus, NJW 2018, 735-739.\nYun, Jang-Ryol (2024). The Value and Price of Digital Media Commodities. Media, Culture & Society, 46(2), 219-234. https://doi.org/10.1177/01634437231188464",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>인간을 위한 인공지능의 기술적 탐구</span>"
    ]
  },
  {
    "objectID": "journalism.html#footnotes",
    "href": "journalism.html#footnotes",
    "title": "9  인간을 위한 인공지능의 기술적 탐구",
    "section": "",
    "text": "Duden online, https://www.duden.de/rechtschreibung/Ethos↩︎\nDSM 지침이란 유럽의회가 2019년 의결한 ’유럽연합 디지털 단일시장의 저작권 및 저작인접권 지침’을 말한다. 동 지침 제4조(1)항에 따르면, 저작권자가 명시적으로 제한하지 않는 한 텍스트 및 데이터 마이닝 목적으로 합법적으로 접근 가능한 작품의 복제 및 추출을 허용할 의무를 규정하고 있다. 이러한 복제는 텍스트 및 데이터 마이닝 목적으로 필요한 기간 보관될 수 있다. 그런데 2024년 9월, 베를린에서 발표된 인공지능 과학자와 법학자 간의 공동 연구에 따르면, 생성형 AI 모델 훈련이 현행 저작권법과 호환되지 않는다는 결론에 도달하고 있다(Tim W. Dornis & Sebastian Stober, 2024).↩︎\nInfoSoc Dirctive는 정보 사회에서 저작권 및 관련 권리의 특정 측면의 조화에 관한 지침이다. (2001년 5월 22일, 2001/29/EC) 동 지침 제3조, 대중에 대한 전달권에 따르면, 회원국은 저작자에게 유선 또는 무선 수단을 통해 자신의 작품을 대중에게 전달하는 것을 허가하거나 금지하는 독점적 권리를 부여해야 하며, 여기에는 대중이 개별적으로 원하는 장소에서 원하는 시간에 작품에 접근할 수 있도록 한다.↩︎\n미디어 경제학에서는 디지털 미디어 환경에서 생산과 소비의 활동을 동시에 하는 플랫폼 경제를 연구하면서, 디지털 노동과 착취의 구조를 설명하고 있다(Fuchs, 2015). 이러한 현상과 구조는 접근 방식의 따라, 네트워크 효과(Caraway, 2023), 독점이윤(Yun, 2024)으로 설명한다.↩︎\n이 글에서 말하는 미디어 정치경제학이란 독일어권의 비판적 정치경제학의 미디어와 커뮤니케이션 연구(Die Kritische Politische Oekonomie der Medien und der Kommunikation)와 영미권의 미디어 정치경제학(Political Economy of the Media)을 모두 아우른다. 이들은 주요하게 맑스주의 정치경제학의 기본 개념들을 수용하고, 비판적인 관점에서 미디어 커뮤니케이션 연구를 진행하고 있다.↩︎\nAxel Springer (2023. 9. 6.). Axel Springer startet bei BILD deutsches ChatGPT-Angebot Hey_ https://www.axelspringer.com/de/ax-press-release/axel-springer-startet-bei-bild-deutsches-chatgpt-angebot-hey_↩︎",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>인간을 위한 인공지능의 기술적 탐구</span>"
    ]
  },
  {
    "objectID": "ad.html",
    "href": "ad.html",
    "title": "10  인공지능과 광고윤리",
    "section": "",
    "text": "10.1 서론\n광고 산업의 변화 속도는 기술 개발의 속도와 비례한다고 볼 수 있다. 새로운 혁신적인 기술이 소개되면 그 기술은 광고와 접목되어 새로운 유형의 광고로 소비자를 만나고는 했다. 3D 광고, AR/VR 광고, 메타버스 광고 등이 그 예들이다. 광고 마케팅 산업에서 인공지능(AI)는 단순한 유행어가 아닌 업계를 재편하는 혁신적인 도구로 현재 활발하게 사용되고 있다. 즉, 생성형 AI는 현재 미래의 광고산업의 혁신과 변화를 이끌 게임 체인저로 인식되고 있다. Chat GPT-4, 미드저니(Midjourney), 소라(Sora), 스테이블 디퓨전과 같은 생성형 AI 기술은 광고주가 고객에게 도달하고 참여하는 방식을 총체적으로 혁신할 수 있는 잠재력을 제시하고 있다 (유승철, 2023).\n예를 들어, 2023년 6월, 오픈AI가 공개한 동영상 생성 AI ’소라(Sora)’는 큰 반향을 일으켰다 (안옥희, 2024). 간단한 명령어만 입력하면 촬영이나 녹음 없이 AI만으로 고화질 영상을 손쉽게 제작할 수 있었기 때문이다. AI가 만든 영상은 실제 촬영한 것인지, CG로 만든 것인지, 가짜인지 구분하기 어려울 정도로 정밀하고 생생했다. 생성형 AI가 여러 산업에 빠르게 도입되면서 광고업계에서도 AI 활용이 늘고 있다. 문장 작성뿐 아니라 이미지, 영상, 음성 등 창의적인 분야에서도 AI 기술이 확산되며 기업들은 전담 조직을 만들고 자체 플랫폼을 개발하는 등 AI 사업에 박차를 가하고 있다. AI 역량 강화를 위한 세미나와 사내 자체 연구활동 역시 활발하게 이루어지고 있다.\n최근 발표된 블룸버그 인텔리전스(Bloomberg Intelligence)에 따르면, 생성형 AI 시장은 2032년까지 1조 3천억 달러에 이를 것으로 예상된다 (한정호, 2024). 이는 2022년 400억 달러에서 시작해 연평균 성장률(CAGR)이 약 42%에 달하는 급격한 성장이다. 이러한 성장은 AI 인프라 투자, 특히 대형 언어 모델(LLM)과 디지털 광고, 특화된 AI 소프트웨어 및 서비스의 수요 증가가 주요 요인으로 작용한다. 특히 아마존 웹 서비스(AWS), 마이크로소프트, 구글, 엔비디아와 같은 대형 기술 기업들이 이 성장을 주도할 것으로 예상되며, AI 모델 훈련을 위한 인프라 수요는 2032년까지 2,470억 달러에 이를 것으로 보인다. 또한 디지털 광고 시장에서만 매년 약 1,920억 달러의 AI 관련 지출이 발생할 것으로 전망된다. 이 보고서는 생성형 AI가 생명과학, 교육 등 다양한 산업을 혁신할 잠재력이 있다고 강조하며, 전통적인 기술 기업들이 AI의 급속한 성장에 적응해야 할 필요성을 지적한다.\n생성형 AI는 디지털 광고 분야에서도 큰 변화를 가져오고 있다. 생성형 AI는 2032년까지 디지털 광고 시장에서 약 1,925억 달러의 지출을 유발할 것으로 예상된다. 이는 AI 기술이 광고 제작, 콘텐츠 개인화, 광고 캠페인 최적화 등 여러 영역에서 혁신을 이끌기 때문이다. 디지털 광고에서 생성형 AI는 자동화된 콘텐츠 생성과 개인화를 주도한다. AI는 소비자 데이터를 분석해 맞춤형 광고를 생성하고, 이를 통해 광고주는 소비자의 선호와 행동에 맞춘 광고를 실시간으로 제공하여 광고 효율성을 크게 높일 수 있다. 예를 들어, AI는 광고 카피, 이미지, 동영상 등을 생성하고 각 사용자에게 최적화된 형태로 변형하여 전달할 수 있다. 또한, AI는 광고 성과 분석과 캠페인 최적화에도 활용된다. AI 기반 분석 도구는 실시간으로 광고 성과를 모니터링하고, 데이터를 기반으로 즉각적인 전략 수정을 가능하게 한다. 이를 통해 광고주들은 적은 자원으로 더 큰 효과를 얻고, 변화하는 시장 환경에 신속히 대응할 수 있다. 결론적으로, 생성형 AI는 디지털 광고의 제작부터 배포, 성과 분석까지 전반적인 과정을 혁신하고 있으며, 이를 통해 기업들은 광고 캠페인의 효율성을 극대화할 수 있는 강력한 도구로 자리 잡고 있다.",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>인공지능과 광고윤리</span>"
    ]
  },
  {
    "objectID": "ad.html#서론",
    "href": "ad.html#서론",
    "title": "10  인공지능과 광고윤리",
    "section": "",
    "text": "그림 1 생성형 AI 시장 규모",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>인공지능과 광고윤리</span>"
    ]
  },
  {
    "objectID": "ad.html#본론",
    "href": "ad.html#본론",
    "title": "10  인공지능과 광고윤리",
    "section": "10.2 본론",
    "text": "10.2 본론\n\n10.2.1 국내 광고 대행사들의 생성형 AI를 통한 변화와 혁신\nHS애드는 최근 국내 광고 업계 최초로 통합 마케팅 AI 플랫폼 ’대시AI’를 상용화했다 (박용선, 2024). 대시AI는 마케팅 전략 수립부터 광고 제작, 성과 분석까지 생성형 AI를 통해 전체 과정을 지원하는 플랫폼이다. 기존에는 광고 제작이나 성과 측정 같은 부분적인 용도로만 AI가 활용되었으나, 대시AI는 이를 넘어 전반적인 마케팅 업무를 AI로 처리할 수 있다. HS애드는 지난해부터 LG AI연구원과 협력하여 다양한 고객사와 기술 검증을 진행하며 대시AI 개발을 추진해 왔다. 앞으로 HS애드는 대시AI를 통해 브랜드 인지부터 구매에 이르는 모든 고객 경험에서 마케팅 크리에이티브를 강화할 계획이며, 단기적으로는 디지털 광고 제작 과정의 80%를 AI로 처리하는 것을 목표로 하고 있다.\n\n\n\n그림 2 HS애드가 ’대시AI’를 활용해 제작한 이미지\n\n\n이노션은 2024년 3월 생성형 AI를 활용한 광고 제작 역량 강화를 위해 ’AI솔루션팀’을 신설했다. 이는 2023년 말에 출범한 ’생성형 AI 전담 태스크 포스(TF)’를 격상한 것이다. AI TF팀은 클라이언트 요구에 맞는 다양한 AI 기반 브랜디드 콘텐츠를 기획하고 관련 플랫폼 구축에 주력해 왔다. 2023년 11월 현대자동차 울산 전기차 전용 공장 착공식에서는 AI를 활용해 현대차그룹 창업자인 정주영 회장의 음성을 복원해 큰 관심을 받았다. 또한, 2024년 6월 공개된 현대자동차 트럭 브랜드의 디지털 광고 ’영원히 달리는 자동차’는 캐릭터와 배경 음악, 작사·작곡까지 모두 AI로만 제작되어 주목 받았다 (김성태, 2024).\n\n\n\n그림 3 이노션이 AI만을 활용해 제작한 현대자동차의 디지털 캠페인 ‘영원히 달리는 자동차’\n\n\n제일기획은 2023년 7월, AI만을 사용해 만든 삼성생명의 TV 광고를 선보이며 업계 최초로 AI 기반 광고 제작에 성공했다 (강은영, 2023). 이 광고는 챗GPT 출시 후 1년도 안 되어 AI를 광고에 도입한 사례로, 다채로운 표정과 세밀함을 확보하기 위해 3개월간 1만 장이 넘는 AI 생성 이미지를 사용했다. 특히, AI가 주로 서양인 이미지를 생성하는 한계를 극복하기 위해 한국인에 가까운 이미지를 만들기 위한 노력이 돋보였다. 제일기획은 디지털 테크 부서를 중심으로 AI 기술의 비즈니스 활용 가능성을 연구하며 다양한 AI 기반 광고 제작을 시도하고 있다.\n\n\n\n그림 4 100% AI 기반으로 제작된 삼성생명의 ‘좋은 소식의 시작’ 광고 캠페인\n\n\n대홍기획 역시 AI 활용에 적극적으로 나서고 있다. 2023년 12월 AI 기반 마케팅 시스템인 ’AI 랩’과 논슈팅 필름을 제작하는 ’AI 스튜디오’를 설립해 AI를 크리에이티브에 접목하고 있다 (정상봉, 2024). 2023년 롯데리아 광고에서 AI가 만든 음악을 선보인 데 이어, 햄버거 맛을 AI가 그린 그림으로 표현한 광고도 선보였다. 외국인에게 뇌파 탐지기를 사용해 느낀 맛을 AI로 시각화한 방식이다. 2024년 7월에는 마케팅 전 과정을 하나의 플랫폼에서 지원하는 올인원 AI 시스템 ’에임스’를 출시하며 AI, 빅데이터, 클라우드를 활용한 다양한 기능을 제공하고 있다.\n\n\n\n그림 5 대홍기획의 롯데리아 ‘K 버거, K 음악이 되다’ 광고 캠페인\n\n\n광고 업계에서는 AI가 효율성과 편리성을 가져오는 동시에 인간의 일자리를 위협할 수 있다는 우려도 존재한다. 그러나 창의적 사고와 감정 이해 등은 여전히 AI가 따라올 수 없는 인간 고유의 영역으로 남아 있다. 광고 업계는 AI를 인간의 창의성을 증폭시키는 도구로 보고 있으며, AI가 반복적인 업무를 대신하는 동안 인간은 더욱 창의적인 작업에 집중할 수 있다는 점에서 AI 활용의 중요성을 강조하고 있다. AI를 통해 광고 제작의 효율성을 높이고 비용과 시간을 절감하는 동시에, 인간의 크리에이티브 역량을 극대화하는 것이 핵심이다.\n\n\n10.2.2 광고 및 마케팅 산업에서 생성형 AI 활용의 장점\n생성형 AI의 주요 장점 중 하나는 하이퍼 타깃팅(hyper-targeting)을 통해 초개인화된 광고를 제공하는 능력이다 (유승철, 2023). AI는 소비자 행동, 취향, 관심사 등의 데이터를 분석하여 개별 고객에게 가장 적합한 광고를 제시할 수 있다. 강화 학습(reinforcement learning)을 통해 고객의 행동 패턴을 학습하고, 적절한 시점에 최적화된 광고를 제안함으로써 구매 전환율을 극대화할 수 있다. 예를 들어, 온라인 쇼핑몰에서 AI가 이전 구매 이력과 검색 기록을 분석해 고객이 관심을 가질만한 제품을 자동으로 추천하는 시스템을 생각할 수 있다. 이러한 방식은 단순한 타깃팅보다 훨씬 더 세밀한 개인 맞춤형 광고를 가능하게 하며, 고객과의 상호작용을 통해 신뢰와 충성도를 높이는 데 기여한다. 또한, 초개인화된 광고는 고객에게 맞춤형 경험을 제공함으로써, 광고의 반복 노출로 인한 피로감을 줄이고 긍정적인 브랜드 인식을 형성할 수 있다.\n생성형 AI는 블로그, 소셜 미디어, 광고 카피 등 다양한 콘텐츠를 신속하고 대규모로 생산할 수 있다 (유승철, 2023). 이는 마케팅 팀에게 전략적 작업에 더 많은 시간을 투자할 수 있는 기회를 제공하며, 동시에 고품질의 콘텐츠를 지속적으로 제공할 수 있다. 예를 들어, AI는 자동으로 최신 트렌드를 반영한 소셜 미디어 게시물을 작성하고, 고객에게 맞춤형 메시지를 전달하는 역할을 할 수 있다. 다만, 생성형 AI 사용 시 투명성 부족과 개인정보 보호 문제는 신뢰에 영향을 미칠 수 있다. 예를 들어, AI가 생성한 콘텐츠가 명확한 출처를 밝히지 않거나, 개인 데이터를 과도하게 수집하는 경우, 사용자들은 AI 기반 광고에 불신을 가질 수 있다. 또한 AI가 생성하는 저품질 콘텐츠가 검색 엔진에 노출되면, 검색 엔진 최적화(SEO) 전략에도 큰 도전 과제가 될 수 있다. 따라서 생성형 AI 활용 시에는 콘텐츠 품질 관리와 데이터 보호 방안이 함께 마련되어야 한다.\n생성형 AI는 실시간으로 고객 데이터를 분석하고, 개인화된 경험을 제공하는 데 탁월하다 (김벼리, 2024). 예를 들어, AI 기반 챗봇은 고객의 문의에 즉각적으로 대응하며, 24시간 개인 맞춤형 고객 지원을 제공할 수 있다. AI는 텍스트나 이미지 기반의 검색 기능을 결합해 온라인 쇼핑 경험을 혁신할 수 있으며, 이를 통해 고객의 요구를 보다 빠르게 해결하고, 기업에 대한 신뢰와 충성도를 높일 수 있다. 예를 들어, 사용자가 특정 상품을 찾을 때 AI가 실시간으로 관련 제품을 추천하고, 구매 결정을 도울 수 있다. G마켓은 AI 알고리즘으로 고객이 최근 구입했거나 살펴본 상품, 검색 빈도, 상품페이지 체류 시간 등을 분석해 개인 맞춤형 상품을 추천한다. 주요 행사에 이 기술을 적용한 결과 G마켓 모바일 애플리케이션(앱) 홈 화면에서 고객당 클릭 횟수는 40% 이상 늘었으며, 클릭한 전체 상품 수는 2배 이상 증가했다고 밝혔다 (김벼리, 2024). 그 외에도, 실시간 고객 지원 외에도 AI는 고객의 피드백을 실시간으로 분석해 제품이나 서비스 개선에 기여할 수 있다. 이로 인해 고객의 만족도를 높이고, 장기적인 관계 구축이 가능해진다.\n생성형 AI는 광고 캠페인의 다양한 과정을 자동화하여 비용을 절감하고 효율성을 극대화할 수 있다 (강태구, 2023). 예를 들어, 반복적인 작업인 광고 카피 작성, 이미지 생성, 그리고 타깃팅 전략 설계와 같은 과정을 AI가 처리함으로써 마케팅 팀은 더 창의적이고 전략적인 업무에 집중할 수 있다. AI는 이를 위해 수많은 데이터를 실시간으로 분석하여 최적화된 광고 메시지를 생성하고, 그 결과 더 나은 ROI를 제공할 수 있다. 실례로, 코카콜라는 AI를 활용하여 전 세계적으로 수많은 버전의 광고를 자동으로 생성하고, 각 지역의 소비자에게 맞춤형 메시지를 전달하는 캠페인을 진행한 바 있다. 이렇게 AI를 활용한 광고 자동화는 광고주에게 더 빠르고 비용 효율적인 방법을 제공하며, 기존보다 훨씬 적은 비용과 시간으로 캠페인을 운영할 수 있도록 돕는다. 또한, AI 기반 분석 도구는 광고 성과를 실시간으로 모니터링하여, 캠페인 중간에 즉각적인 피드백을 제공한다. 이를 통해 기업은 실시간으로 전략을 조정하고 최적화할 수 있어, 불필요한 자원의 낭비를 최소화한다. 예를 들어, 페이스북이나 구글 애드 플랫폼에서 AI를 통해 자동화된 A/B 테스트를 진행하면, 어떤 광고가 더 효과적인지 빠르게 파악해 캠페인 성과를 극대화할 수 있다.\nAI는 창의적인 광고 콘텐츠 제작을 새롭게 혁신하고 있다. 과거에는 많은 시간과 인력이 필요했던 이미지, 동영상, 음악 생성 작업을 AI가 자동으로 처리할 수 있다. 예를 들어, 마이크로소프트의 Azure AI는 기업들이 AI 기반 이미지 생성 툴을 활용하여 짧은 시간 내에 고품질의 광고 이미지를 만들 수 있도록 지원한다. 이러한 AI 기술을 통해 브랜드는 더욱 창의적이고 독창적인 방식으로 소비자에게 접근할 수 있으며, 이를 통해 광고 차별화를 이룰 수 있다. AI는 또한 글로벌 광고 캠페인에서 매우 유용하다. 다국어 지원 기능을 갖춘 생성형 AI는 각 지역의 언어와 문화에 맞춘 맞춤형 콘텐츠를 생성할 수 있다. 예를 들어, 유니레버는 AI를 통해 전 세계 여러 나라에서 진행된 캠페인에서 지역별로 최적화된 광고 메시지를 제작하여 효과적으로 타깃팅했다. 이는 각 나라의 문화적 차이를 반영한 광고로, 글로벌 기업들이 더욱 강력하게 시장에 다가갈 수 있는 전략적 도구가 된다. 이처럼 생성형 AI는 브랜드 메시지를 글로벌 시장에 맞춤형으로 전달함으로써 광고주들이 세계 각지의 소비자를 효율적으로 타깃팅할 수 있도록 돕는다. AI는 시장별로 다른 요구와 기대에 대응하며, 광고 캠페인이 현지화된 전략을 따를 수 있도록 한다.\n위에 언급한 생성형 AI 활용의 장점들 외에도 생성형 AI는 실시간 데이터를 기반으로 하여 변화하는 시장 상황에 빠르게 대응할 수 있다. 소비자 행동이 변화할 때, AI는 이를 즉시 감지하고 광고 전략을 즉각적으로 조정할 수 있어 경쟁 우위를 확보할 수 있다. 그리고 AI는 대규모 데이터를 분석해 소비자의 심리와 행동 패턴을 더 깊이 이해할 수 있다. 이를 통해 더 정교한 타깃팅이 가능해지고, 광고주가 고객의 요구와 기대에 더 정확하게 대응할 수 있다. 또한, AI는 예측 분석 기능을 통해 향후 소비자 트렌드를 예측하고, 미리 광고 전략을 준비할 수 있게 한다. 이를 통해 기업은 미래의 변화에 대해 선제적으로 대응할 수 있으며, 보다 예측 가능한 광고 캠페인을 운영할 수 있다. 이렇듯 생성형 AI는 마케팅과 광고 산업에 혁신적이고 효율적인 도구로 자리 잡으며, 기업들이 더욱 창의적이고 전략적으로 고객에게 접근할 수 있는 다양한 기회를 제공하고 있다.\n\n\n10.2.3 생성형 AI와 연관된 법적·윤리적 문제\n생성형 AI는 다양한 분야에서 혁신적인 변화를 가져오고 있지만, 동시에 법적·윤리적 문제들도 제기되고 있다. 첫째, 저작권 침해 문제는 심각하다. 생성형 AI는 대규모 데이터 세트를 학습하여 새로운 콘텐츠를 생성하는 방식으로 작동한다. 이때 학습에 사용된 데이터가 저작권이 있는 자료라면, AI가 생성한 콘텐츠가 저작권을 침해할 수 있다. 예를 들어, AI가 기존에 저작권이 있는 텍스트나 이미지를 학습하고 이를 기반으로 유사한 작품을 만들어낼 경우, 법적으로 저작권 침해로 간주될 가능성이 있다. 또한, 생성형 AI가 만든 콘텐츠의 저작권이 누구에게 귀속되는지도 논란이 있다. AI가 창작한 결과물의 저작권을 AI 개발자가 가져가는지, AI를 훈련시킨 사용자에게 귀속되는지, 또는 법적으로 저작권이 부여되지 않는지에 대한 명확한 규정이 아직 불확실하다.\n둘째, 데이터 사용 및 프라이버시 침해는 현재 진행형이다. 생성형 AI는 방대한 데이터를 활용해 학습을 진행하는데, 이 과정에서 개인 정보가 포함될 가능성이 있다. AI가 개인정보를 활용해 생성된 콘텐츠를 만들거나 이를 기반으로 광고 타겟팅을 할 경우, 개인 정보 보호법(GDPR 등)을 위반할 수 있다. 특히, 민감한 개인정보를 무단으로 사용하거나 동의 없이 데이터가 수집되는 경우 심각한 법적 문제가 발생할 수 있다. 예를 들어, AI 챗봇이 사용자와의 대화 내용을 학습 데이터로 활용할 때, 이 정보가 제대로 보호되지 않으면 프라이버시 침해 논란이 일어날 수 있다. 이와 관련해 데이터 수집과 사용에 대한 투명성을 보장하는 것이 중요하다.\n셋째, 생성된 콘텐츠의 신뢰성 및 가짜 정보가 큰 이슈로 떠오르고 있다. AI가 생성한 콘텐츠의 신뢰성은 중요한 윤리적 문제다. 생성형 AI는 매우 사실적으로 보이는 텍스트, 이미지, 동영상을 만들어낼 수 있지만, 이러한 콘텐츠가 진실이 아닐 경우 가짜 정보나 허위 뉴스로 악용될 가능성이 크다. AI가 생성한 이미지나 영상이 실제 상황을 왜곡하거나 조작된 정보를 퍼뜨릴 때, 그 피해는 심각할 수 있다. 실제로, AI를 활용한 딥페이크(Deepfake) 기술은 인물의 얼굴을 합성하여 가짜 영상을 만들어내는 데 사용되며, 정치적 선전, 명예 훼손, 사기 등 다양한 방식으로 악용될 수 있다. 이러한 가짜 정보는 사회적 혼란을 야기할 뿐만 아니라, 피해자의 명예를 훼손하거나 범죄에 악용될 수 있어 규제의 필요성이 제기되고 있다.\n넷째, 윤리적 편향 및 차별은 생성형 AI가 가지고 있는 가장 큰 내재적 문제점이라 할 수 있다. 생성형 AI는 학습 데이터에 따라 결과물을 생성하는데, 데이터 자체가 편향되어 있으면 AI도 편향된 결과를 만들어낼 수 있다. 예를 들어, 인종, 성별, 계층 등의 문제에서 AI가 특정 그룹을 차별하거나 왜곡된 관점을 반영하는 결과물을 만들 수 있다. 이 경우, AI는 무의식적으로 사회적 불평등을 강화하거나 잘못된 정보를 전달할 위험이 있다. 또한, AI의 편향된 학습으로 인해 특정 직업군이나 소비자 그룹을 차별하는 마케팅 또는 채용 방식이 발생할 수 있다. 이를 해결하기 위해 AI 알고리즘을 공정하게 설계하고, 편향된 데이터를 사전에 필터링하는 노력이 필요하다.\n마지막으로, 책임 소재 문제는 아직도 해결되지 않았다. 생성형 AI가 자율적으로 콘텐츠를 생성하거나 결정을 내릴 경우, 문제가 발생했을 때 책임 소재를 누구에게 물을 것인가에 대한 논란이 있다. AI가 잘못된 정보를 제공하거나 법적 문제를 일으켰을 때, 이를 개발한 기업, 데이터를 제공한 사용자, 또는 AI 시스템 그 자체가 책임을 져야 하는지에 대한 명확한 규정이 부족하다. 특히, 자율적인 의사결정을 내리는 AI가 잘못된 결과를 초래했을 경우, 법적 책임의 범위와 주체를 정의하는 것이 중요한 과제로 남아 있다.\n결론적으로, 생성형 AI는 엄청난 가능성을 가지고 있지만, 그와 함께 법적·윤리적 문제들도 다루어져야 한다. 저작권, 데이터 보호, 가짜 정보, 편향성, 그리고 책임 소재 등 여러 문제가 법적 규제와 사회적 합의를 필요로 하며, 이러한 문제들을 해결하기 위한 법적·윤리적 기준이 확립되어야 한다.\n\n\n10.2.4 생성형 AI 사용을 제한하는 회사들의 움직임\n인공지능(AI)의 영향력이 전 세계 산업에 걸쳐 확산되는 가운데, 광고 및 마케팅 분야에서도 AI의 사용을 제한하려는 움직임이 감지되고 있다. AI는 비용 절감과 작업 시간 단축은 물론, 빠르게 변화하는 시장 환경에서 신속하고 효율적인 결과물을 만들어내는 장점이 있지만, 편향성, 오류, 윤리적 문제 등 심각한 단점도 부각되고 있다. 이러한 이유로 일부 브랜드는 광고와 마케팅에서 AI 사용을 경계하며, ’No AI’를 외치는 사례가 늘어나고 있다.\n로레알(L’Oréal), 도브(Dove), 레고(LEGO), H&M, 띵스(Thinx) 등 다양한 산업의 대표 브랜드들이 AI 기술을 제한적으로 사용하거나 아예 배제하는 접근 방식을 취하고 있다 (김수경, 2024). 예를 들어, 로레알은 AI가 인간의 머리카락이나 피부톤을 묘사하는 작업에 AI를 사용하지 않겠다는 방침을 세우고 있으며, 상업적 목적에서는 AI 기술을 사용할 수 있지만 인간을 묘사하는 데에는 제한을 두고 있다. AI가 아이디어나 영감을 제공할 수는 있지만, 제품의 이점을 직접적으로 표현하는 데 사용되지 않도록 신중한 접근을 하고 있다.\n도브는 2024년 ‘더 코드(The Code)’ 캠페인에서 AI가 미적 기준을 왜곡하는 모습을 비판했다. AI에게 ’세계에서 가장 아름다운 여성’의 이미지를 생성하라는 요청에, 편협한 미적 기준에 따라 금발의 백인 여성 이미지를 만들어낸 것이다. 이 캠페인은 AI의 고정관념을 꼬집으며 ’아름다움’의 다양성을 강조했다. 레고는 지적재산권(IP)과 관련된 콘텐츠 제작에 AI를 사용하지 않겠다는 방침을 고수하고 있다. 제품 디자인과 개발에는 AI 기술을 테스트하고 있으나, 콘텐츠 제작에 있어서는 인간 예술가와의 협업을 지속하겠다는 입장을 밝혔다. 레고는 AI로 생성한 이미지가 비판을 받자, AI를 활용한 콘텐츠 제작을 철회하고 다시 인간 중심의 창작 방식을 강조했다.\n그리고 H&M 역시 광고 콘텐츠 제작에 AI를 사용하지 않고 있으며, 윤리적 가이드라인이 마련되기 전까지는 광고에서 AI 활용을 제한하겠다고 밝혔다. 다만, 고객들이 AI를 활용해 옷을 디자인할 수 있는 크리에이터 툴을 제공하는 등 AI 기술 자체에 대한 관심은 여전히 크다. 속옷 브랜드 띵스(Thinx)는 ‘겟 바디와이즈(Get BodyWise)’ 캠페인을 통해 AI가 여성의 건강 문제를 잘못 표현하는 방식을 비판했다. AI가 ’생리’에 대한 이미지를 생성했을 때, 부정적이고 왜곡된 이미지를 만들어냈고, 이에 띵스는 편견을 깨기 위한 올바른 교육의 중요성을 강조하는 메시지를 전했다.\n이렇듯 AI는 광고와 마케팅의 창작 방식과 접근법을 변화시키고 있지만, AI 사용에 있어 윤리적·법적 문제를 고려한 신중한 접근이 요구되고 있다. 로레알, 도브, 레고, H&M, 띵스와 같은 브랜드들은 이러한 움직임의 선두에서 AI의 위험성을 경계하고 있으며, 이를 통해 AI의 장점과 단점을 균형 있게 활용하는 방향을 모색하고 있다.\n\n\n10.2.5 미국 어린이 광고 심의 부서(CARU)의 AI로 생성된 어린이 광고에 대한 가이드라인 제정\nAI 기술이 광고 산업에 도입되면서, 특히 어린이 광고에서 AI의 사용을 둘러싼 윤리적 문제와 법적 규제가 주목 받고 있다(최영호, 2024). 광고주는 AI의 장점을 활용할 수 있지만, 특히 어린이와 같은 취약한 소비자 집단에게는 AI 사용이 신중하게 관리되어야 한다. 이는 어린이가 AI로 생성된 콘텐츠와 현실의 차이를 구별할 수 없기 때문이다. 미국의 BBB 내셔널 프로그램의 어린이 광고 심의 부서(CARU: Children’s Advertising Review Unit)는 AI를 활용한 광고와 데이터 수집에 대한 새로운 규정 준수 경고를 발표했다. CARU의 가이드라인은 광고주와 브랜드가 AI가 생성한 콘텐츠가 어린이에게 기만적이지 않고 윤리적으로 올바르게 제작되었는지 확인할 것을 요구한다. 또한, 어린이를 대상으로 하는 AI 기반 광고는 현실과 허구를 혼동하지 않도록 주의해야 하며, AI가 어린이의 개인정보를 수집할 때 어린이 온라인 개인정보 보호법(COPPA)를 준수해야한다.\nCARU의 경고는 딥페이크와 같은 AI 기반 기술이 광고에 사용될 때 특히 주의해야 한다고 강조한다. 어린이를 대상으로 하는 광고에서 AI가 실제 사람이나 상황을 시뮬레이션 하거나, AI 생성 캐릭터가 어린이와 소통하는 경우, 이를 통해 어린이를 오도하거나 기만할 위험이 있다는 것이다. 따라서 광고주는 이러한 요소를 사용할 때 투명성을 확보하고, 어린이가 현실과 가상을 명확히 구분할 수 있도록 해야 한다.\nCARU는 광고에 AI를 사용하는 브랜드는 다음 영역에서 어린이를 오도하거나 속일 수 있는 가능성에 특히 주의해야 한다고 경고하고 있다. - AI로 생성된 딥페이크, 실제 사람, 장소 또는 사물의 시뮬레이션을 포함한 시뮬레이션 요소, 광고 내 AI 기반 음성 복제 기술. - 제품 묘사(제품 또는 성능 특성을 나타내는 AI를 사용하여 생성되거나 강화된 카피, 사운드, 시각적 프레젠테이션을 포함한 제품 묘사). - 애니메이션 및 AI 생성 이미지와 같은 기술을 통해 어린이의 상상력을 부당하게 이용하거나, 달성할 수 없는 성과에 대한 기대감을 조성하거나, 현실과 공상을 구분하기 어려운 어린이의 특성을 악용할 수 있는 판타지. - 어린이가 실제 사람과 소통하고 있다고 믿을 수 있는 캐릭터 아바타나 시뮬레이션된 인플루언서를 생성하여 어린이와 직접적으로 소통하는 것.\n다른 나라에서도 AI로 생성된 어린이 광고에 대한 규제가 강화되고 있다. 유럽 연합(EU)의 경우, 디지털 서비스법(DSA: Digital Service Act)은 AI 기반 광고를 포함한 모든 디지털 콘텐츠가 어린이의 권리와 프라이버시를 보호하는 방식으로 제작되도록 요구한다. 예를 들어, 광고주가 AI를 통해 어린이 데이터를 수집할 경우, 부모의 동의를 명확히 요구하며, 어린이가 광고로 인해 부정적인 영향을 받지 않도록 보호 장치가 마련되어야 한다. 이와 같은 규제들은 AI가 어린이 광고에서 정확성과 투명성을 확보해야 하며, 어린이의 상상력을 과도하게 자극하거나 오도하지 않도록 하는 데 중점을 두고 있다. 광고주는 어린이의 특성을 악용하지 않도록 주의해야 하며, AI가 생성하는 콘텐츠가 윤리적으로 설계되도록 해야 한다.",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>인공지능과 광고윤리</span>"
    ]
  },
  {
    "objectID": "ad.html#결론",
    "href": "ad.html#결론",
    "title": "10  인공지능과 광고윤리",
    "section": "10.3 결론",
    "text": "10.3 결론\n생성형 AI는 광고 산업에 있어 미래를 이끄는 핵심 기술로 자리매김하고 있다. AI는 기존의 광고 제작 과정에서 경험하지 못했던 효율성과 창의성을 제공하며, 빠르게 변화하는 소비자 요구에 실시간으로 대응할 수 있게 한다. AI 기술은 데이터를 기반으로 고객의 행동과 선호도를 분석해 초개인화된 광고를 제작하고, 이를 통해 소비자와 브랜드 간의 더욱 깊은 상호작용을 가능하게 한다. 예를 들어, 광고 카피, 이미지, 동영상 등을 자동으로 생성해 각 사용자에게 맞춤형으로 전달하는 AI 시스템은 광고 캠페인의 효율성을 극대화하고, 비용을 절감할 수 있는 가능성을 보여준다. 이는 단순한 자동화 기술을 넘어, 광고 제작과 소비자의 참여 방식을 근본적으로 바꾸는 혁신적인 도구로서의 역할을 수행한다.\nAI는 또한 글로벌 광고 캠페인에서 매우 유용한 도구로 활용될 수 있다. 다국어 지원 기능을 갖춘 AI는 각국의 언어와 문화를 고려한 맞춤형 광고 콘텐츠를 생성해, 글로벌 소비자에게 더욱 적합한 메시지를 전달할 수 있다. 이는 다양한 국가와 시장을 타깃으로 하는 기업들이 효율적으로 현지화된 캠페인을 운영할 수 있도록 돕는다. 또한 AI는 실시간 데이터를 기반으로 소비자 트렌드를 분석하고, 이에 따라 광고 전략을 즉각적으로 수정하거나 최적화할 수 있어, 급변하는 시장 상황에서도 경쟁력을 유지할 수 있다.\n하지만 이러한 긍정적인 전망에도 불구하고, 생성형 AI가 광고 산업에서 윤리적 문제와 법적 도전에 직면하고 있는 것도 사실이다. 먼저, 저작권 문제는 매우 중요한 이슈로 다뤄지고 있다. AI가 생성한 콘텐츠가 기존 저작물을 모방하거나 그 기반을 둔 경우, 저작권 침해로 간주될 수 있다. AI가 학습하는 과정에서 저작권이 있는 자료를 사용하는 경우, AI가 만들어낸 결과물에 대한 법적 소유권을 명확히 하는 것도 문제로 남아 있다. 콘텐츠를 만든 AI가 아니라 이를 개발하고 사용한 기업과 창작자가 누구인지, 그리고 이들의 권리가 어떻게 보호되어야 하는지에 대한 논의가 필요하다. 또한, AI의 데이터 수집과 프라이버시 침해 문제는 여전히 큰 도전 과제이다. AI는 방대한 데이터를 기반으로 학습하고 광고를 제작하는데, 이 과정에서 개인정보가 포함될 가능성이 크다. 광고주가 소비자의 데이터를 지나치게 활용하거나, 동의 없이 데이터를 수집하는 경우 프라이버시 침해와 같은 법적 문제가 발생할 수 있다. 따라서 AI가 생성하는 콘텐츠와 광고가 윤리적으로 투명성을 보장하고, 데이터 사용이 합법적이며 사용자에게 적절한 보호 장치를 제공해야 한다는 규제가 필수적이다.\n더불어, AI는 학습 데이터를 기반으로 편향된 결과물을 생성할 위험이 있다. 만약 AI가 인종, 성별, 계층 등과 관련된 편향된 데이터를 학습하면, 생성된 광고 또한 특정 집단에 대한 편견을 반영할 수 있다. 이는 광고가 사회적 불평등을 강화하거나 왜곡된 정보를 전달할 가능성을 높인다. 따라서 AI 시스템의 편향성을 줄이기 위한 알고리즘 개선과 공정한 데이터 처리가 필요하다. AI는 수많은 데이터를 학습하지만, 그 데이터를 공정하게 처리하는 방법을 찾는 것이 앞으로 중요한 과제로 남을 것이다.\n마지막으로, 책임 소재의 문제는 생성형 AI가 자율적으로 의사결정을 내리고 콘텐츠를 생성하는 과정에서 발생할 수 있는 법적 도전이다. AI가 광고 제작 과정에서 문제를 일으켰을 때, 그 책임을 AI 자체에 물을 수 없는 상황에서, 이를 개발하고 운영한 기업, 데이터 제공자, 혹은 광고주가 그 법적 책임을 어떻게 분담할 것인지에 대한 명확한 기준이 마련되어야 한다. 이러한 책임 소재에 대한 명확한 법적 프레임워크가 확립되지 않는다면, AI 기반 광고는 법적 분쟁의 대상이 될 수 있다.\n결론적으로, 생성형 AI는 광고 산업의 미래를 근본적으로 변화시킬 잠재력을 가지고 있다. 효율성, 창의성, 그리고 초개인화된 광고 전략을 가능하게 하는 이 기술은 광고주들에게 새로운 기회를 제공하고 있으며, 광고 산업 전체에 혁신적인 도구로 자리잡고 있다. 하지만 AI가 가지고 있는 법적, 윤리적 문제들은 해결되지 않으면 안 되는 과제이다. 저작권 보호, 개인정보 보호, 편향성 제거, 그리고 책임 소재의 명확화와 같은 문제들을 해결하기 위한 사회적 합의와 법적 규제가 필요하다. 이를 통해 생성형 AI는 광고 산업의 미래를 더욱 긍정적이고 윤리적인 방향으로 이끌어 갈 수 있을 것이다.",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>인공지능과 광고윤리</span>"
    ]
  },
  {
    "objectID": "ad.html#더-읽을-거리",
    "href": "ad.html#더-읽을-거리",
    "title": "10  인공지능과 광고윤리",
    "section": "10.4 더 읽을 거리",
    "text": "10.4 더 읽을 거리\n김윤명 (2023). 생성형AI의 법과 윤리에 대한 문답. 박영사.\n유승철, 상윤모, 엄남현, 양승관 (2023). 인공지능 시대의 광고윤리. 학지사.\n한국광고학회 (2023). AI 기반 광고 전략. 온샘.\n한국광고홍보학회 (2023). 인간정서와 AI. 한울.",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>인공지능과 광고윤리</span>"
    ]
  },
  {
    "objectID": "ad.html#생각해-볼-문제",
    "href": "ad.html#생각해-볼-문제",
    "title": "10  인공지능과 광고윤리",
    "section": "10.5 생각해 볼 문제",
    "text": "10.5 생각해 볼 문제\n\nAI가 광고 산업에서 인간의 창의성을 대체할 수 있을까? AI가 광고 제작에서 점점 더 큰 역할을 맡고 있지만, 이러한 기술이 인간의 창의성과 감성을 대체할 수 있을까?\n초개인화된 광고는 개인정보 보호 측면에서 어떤 문제를 야기할 수 있을까? AI가 소비자의 데이터를 분석해 맞춤형 광고를 제공하는 과정에서 개인정보 침해 우려는 어떻게 해결될 수 있을까?\n생성형 AI가 만든 콘텐츠의 저작권은 누구에게 귀속되어야 할까? AI가 학습한 데이터가 저작권이 있는 자료라면, AI가 만든 콘텐츠의 소유권 문제는 어떻게 해결할 수 있을까?\nAI가 생성하는 가짜 정보나 허위 콘텐츠를 어떻게 방지할 수 있을까? AI 기술로 생성된 딥페이크나 허위 정보가 사회에 미치는 영향을 어떻게 줄일 수 있을까?\nAI로 생성된 광고에서 문제가 발생했을 때, 법적 책임은 누구에게 있을까? AI가 자율적으로 콘텐츠를 생성했을 때, 그에 대한 책임은 광고주, AI 개발자, 데이터 제공자 중 누구에게 있을까?",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>인공지능과 광고윤리</span>"
    ]
  },
  {
    "objectID": "ad.html#참고문헌",
    "href": "ad.html#참고문헌",
    "title": "10  인공지능과 광고윤리",
    "section": "10.6 참고문헌",
    "text": "10.6 참고문헌\n강은영 (2023). 삼성생명, 100% AI 기반 제작 ‘좋은 소식의 시작’ 광고 캠페인 공개. 브릿지경제.\n강태구 (2023). 광고와 마케팅, AI는 당장 필요한가. 반론보도.\n김벼리 (2024). 내 생각을 읽었니? ’AI 추천’에 꽂힌 유통업계. 헤럴드경제.\n김성태 (2024). “생성형 AI로만 제작”… 이노션, 현대차 신규 디지털 캠페인 공개. 마켓뉴스.\n김수경 (2024). “광고·마케팅에 AI 금지”… 로레알·도브·레고·H&M·띵스(Thinx)의 전략은?“. 뉴데일리경제.\n박용선 (2024). “디지털 광고 80% AI로 제작” HS애드, AI 플랫폼 국내 첫 상용화. 조선일보.\n안옥희 (2024). “영상, 카피, 음악, 척하면 척 광고도 AI 시대” 매거진한경.\n유승철 (2023). 생성형 AI시대의 마케팅, 어디로 가는가” 반론보도.\n정상봉 (2024). 대홍기획의 AI 시스템 ‘에임스’, 롯데 전체 마케터가 쓴다. 매일경제.\n최영호 (2024). “CARU, AI로 생성된 어린이 광고에 대한 새로운 지침을 엄격하게 시행한다. 매드타임즈.\n한정호 (2024). 엔비디아, KT 초거대 AI 모델 ‘믿음’ 개발·구축 지원. 아이티데일리.",
    "crumbs": [
      "3부: 인공지능 윤리와 현장",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>인공지능과 광고윤리</span>"
    ]
  },
  {
    "objectID": "fml.html",
    "href": "fml.html",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "",
    "text": "11.1 COMPAS 알고리즘\n수학적으로 정의된 공정성 개념에 쉽게 접근하기 위해서 COMPAS라는 범죄자 프로파일링 소프트웨어의 유명한 사례에 대해 먼저 논의해 보자. COMPAS (Correctional Offender Management Profiling for Alternative Sanction)는 Northpointe(현 Equivant)라는 기업에 의해 개발된 프로그램으로 범죄자의 신상 정보를 이용하여 재범 가능성을 예측하는 기능을 가지고 있다. 이는 미국 뉴욕, 위스콘신, 캘리포니아 등의 지방 법원에서 보석 결정 등을 내리는데 보조 도구로 사용되었다. 미국의 탐사보도언론 프로퍼블리카(Propublica)는 COMPAS에 의해 내려진 결정을 분석한 결과, COMPAS에 의한 자동화된 결정이 인종에 따라 편향되었다고 폭로하였다.\n아래 테이블은 &lt;알고리즘이 지배한다는 착각Outnumbered&gt;이라는 책에서 저자 데이티드 섬프터가 원 데이터를 재구성한 알기 쉬운 예이다.1\n위의 표에서 ‘고위험’이 의미하는 바는 COMPAS가 해당 범죄자에 대해 재범 가능성이 높다고 판정한 경우를, ’저위험’은 그 반대의 경우이다. 즉, 각 열(column)은 자동 분류 알고리즘의 예측 결과에 대응한다. 반면, 각 행(row)에 표현된 ’재범○’, ’재범✕’는 실제로 이후에 데이터에 포함된 범죄자들이 재범을 저질렀는지 여부를 의미한다. 즉, 이 테이블은 COMPAS 알고리즘의 재범 가능성에 대한 예측과 사후적으로 밝혀진 실제 재범 여부를 모두 제공하기에, 이를 이용해 알고리즘 예측의 정확성을 평가해볼 수 있는 지도학습 모형에 해당한다.\n프로퍼블리카는 COMPAS의 편향된 예측에 대한 우려를 표명하면서, 몇 가지 통계치를 제시하였다. 위의 데이터에서 가장 먼저 비판할 수 있는 바는, 흑인이 고위험군으로 판정될 확률은 60%(=2174/3615)인 반면, 백인이 고위험군으로 판정될 확률은 34.8%(=854/2454)에 불과하다는 것이다. 물론, 이는 실제 두 인종 간의 재범률 차이에서 기인한 것일 수도 있다. 하지만, 프로퍼블리카는 알고리즘의 정확도가 두 인종 간에 다르게 나타난다는 점에서 또 다른 우려를 표명한다. 예컨대, 흑인의 경우 데이터에 포함된 1,714명이 실제 재범을 저지르지 않았는데도 불구하고, 그 중 805명은 COMPAS가 고위험군으로 분류하여 46.9%%(=805/1714)의 위양성률(false positive rate)을 보인 반면, 백인은 23.5%(=349/1488)의 위양성률을 가지고 있다는 것이다. 즉, 위의 데이터에 따르면, COMPAS 알고리즘은 백인 범죄자에 대해서는 그들에게 불리한 오류를 발생시킬 확률이 더 적다.\n이에 대해 COMPAS를 개발한 Northpointe는 이와 다른 정확도 개념을 이용해 프로퍼블리카의 비판을 반박하였다. 예컨대, 고위험군으로 예측된 2,174명의 흑인 중 1,369명이 실제 다시 범죄를 저질러, 63%(=1,369/2,174)의 정확도를 보였으며, 고위험군으로 예측된 854명의 백인 중 505명이 재범을 저질러, 59%(=505/854)의 정확도를 보였기에, COMPAS알고리즘은 두 인종 집단 사이에서 정확도의 큰 차이를 보이지 않았다는 것이다.\n이와같이 혼란스러운 논란의 바탕에는 사실 공정성이라고 부를만한 다양한 기준이 있으며, 이들이 서로 모순될 가능성이 크다는 매우 근본적인 문제가 존재한다. 이를 좀 더 분명하게 이해하기 위해 이제 약간의 수학적 기호와 정의를 도입해보도록 하자.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#compas-알고리즘",
    "href": "fml.html#compas-알고리즘",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "",
    "text": "흑인\n고위험\n저위험\n합계\n\n\n\n\n재범○\n1,369\n532\n1,901\n\n\n재범✕\n805\n990\n1,714\n\n\n합계\n2,174\n1,522\n3,615\n\n\n\n\n\n\n백인\n고위험\n저위험\n합계\n\n\n\n\n재범○\n505\n461\n966\n\n\n재범✕\n349\n1,139\n1,488\n\n\n합계\n854\n1,600\n2,454",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#공정성의-수학적-정의",
    "href": "fml.html#공정성의-수학적-정의",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "11.2 공정성의 수학적 정의",
    "text": "11.2 공정성의 수학적 정의\nCOMPAS 사례를 통해 살펴본 공정성 개념들을 수학적으로 일반화하기 위해 우리는 몇 가지 확률변수(random variable)2들을 정의할 것이다. 먼저, 우리가 예측하고 싶은 ’진실’을 \\(Y\\)라고 부르자. COMPAS의 예에서 \\(Y\\)는 실제 재범 여부에 해당한다. \\(Y=1\\)라는 확률변수 \\(Y\\)의 실현값은 재범을 실제 저질렀음을, \\(Y=0\\)은 반대의 경우를 의미한다고 하자. 물론 COMPAS가 예측을 만들어내는 시점에는 재범 여부를 알 수 없지만, 위의 경우와 같이 사후적으로는 해당 데이터가 수집되어 \\(Y\\)의 실제 값을 알게되는 경우가 있고, 이를 통해 인공지능의 예측 결과의 정확성을 사후적으로 평가해볼 수 있다.\n\\(Y\\)와 달리, \\(R\\)은 알고리즘의 예측을 표현하는 확률변수라고 하자. 즉, 위의 예에서 COMPAS가 해당 범죄자를 재범 ’고위험군’이라고 평가한 경우에는 \\(R=1\\), ’저위험군’이라고 평가한 경우에는 \\(R=0\\)이라는 확률변수의 실현값으로 표현할 할 수 있을 것이다.\n이제 개개인이 가지고 있는 특성 역시 확률변수로 표현하자. 수학적 공정성의 정의는 먼저 ‘차별’이 발생할 수 있는 ’민감한 개인의 특성’, \\(A\\)와 그렇지 않은 일반적인 개인의 특성 \\(X\\)가 구분된다고 가정한다. COMPAS의 예에서 \\(A\\)는 인종에 해당할 것이다. 기호의 간결함을 위해 \\(A=a\\)는 개인이 흑인인 경우를, \\(A=b\\)는 개인이 백인인 경우를 의미한다고 가정하자. \\(X\\)는 민감속성 이외의 모든 다른 특성이 될 것이다. 지금까지의 정의를 모두 종합하면 아래와 같다.\n\n\\(Y \\in \\{0,1\\}\\): 예측하고자 하는 진실 (e.g. 실제 재범 여부)\n\\(R \\in \\{0,1\\}\\): 모형의 예측 (e.g. 고위험군/저위험군)\n\\(A \\in \\{a,b\\}\\): 민감한 개인의 특성 (e.g. 인종)\n\\(X\\): 민감하지 않은 개인의 특성 (e.g. 거주지역, 직업)\n\n물론 \\(Y\\), \\(R\\), \\(A\\)는 위와 같이 0/1 또는 a/b처럼 이분법적으로 구분될 필요는 없으며, 더 많은 카테고리가 존재하는 경우 (예컨대 거주지역), 또는 해당 변수들이 연속적으로 변화하는 숫자일 경우(예컨대 소득)로 확장할 수도 있다. 여기서는 설명의 간결함을 위해 위와 같이 단순하게 표현할 수 있는 경우에만 논의를 한정할 것이다.\n위에서 프로퍼블리카 제기했던 비판 중 첫 번째 비판-즉, 흑인을 고위험군으로 예측하는 경우가 백인을 고위험군으로 예측하는 경우보다 더 많다는 지적을 확률로 표현하면 다음과 같다.\n\\[P(R=1|A=a) &gt; P(R=1|A=b)\\]\n여기서 \\(P()\\)는 (괄호 안에 표시할) 어떤 사건의 확률을 타나내는 수학 기호이다. 예컨대 \\(P(R=1)\\)은 (인종을 막론하고) COMPAS가 고위험군이라고 예측할 확률을 의미한다. \\(P( | )\\)라는 조금 더 복잡한 기호는 \\(|\\) 기호 뒤에 표시된 어떤 조건 하에서의 확률, 즉, ’조건부 확률(Conditional Probability)’을 의미한다. 즉, \\(P(R=1|A=a)\\)은 ’인종이 흑인인 조건 하에서(\\(A=a\\)) 고위험군으로 예측(\\(R=1\\))될 확률’을, \\(P(R=1|A=b)\\)는 ’인종이 백인인 조건 하에서(\\(A=b\\)) 고위험군으로 예측(\\(R=1\\))될 확률’을 의미한다. 프로퍼블리카의 비판은 이 두 조건부 확률이 동등해야 ’공정한 알고리즘’이라는 생각을 전재한다. 이를 ’통계적 동등성(statistical parity)’로서의 공정성이라고 한다.\n프로퍼블리카의 COMPAS 알고리즘에 대한 두 번째 비판은 알고리즘의 ‘정확도’가 두 인종 사이에 달랐다는 점이었다. 구체적으로, ’위양성률(False Positive Rate)’, 즉, 실제 재범을 저지르지 않은 흑인들을 고위험군으로 잘못 분류하는 경우가 백인에게 동일한 오류가 발생하는 경우에 비해 지나치게 잦았다는 것이다. 이러한 비판을 조건부 확률의 기호로 표현하자면 다음과 같다.\n\\[P(R=1|Y=0, A=a) &gt; P(R=1|Y=0, A=b)\\]여기서 \\(P(R=1|Y=0,A=a)\\)는 ‘실재 재범을 일으키지 않았고(\\(Y=0\\)), 흑인이라는 조건(\\(A=a\\)) 하에서’ 고위험군으로 예측(\\(R=1\\))될 확률이, ‘실재 재범을 일으키지 않았고(\\(Y=0\\)), 백인이라는 조건(\\(A=b\\))’ 하에서 고위험군으로 예측(\\(R=1\\))될 확률보다 높다는 것이다.\n반면, Northpointe의 재반박, 즉, 고위험군으로 분류된 사람 중에 실재 재범을 일으킬 확률은 흑은과 백인 사이에 거의 같다는 주장은 어떻게 표현할 수 있을까? 다음의 수식을 살펴보자.\n\\[P(Y=1|R=1, A=a) \\approx P(Y=1|R=1, A=b)\\]\n여기서 \\(P(Y=1|R=1,A=a)\\) 라는 표현과 앞서 프로퍼블리카의 비판에서 보았던 \\(P(R=1|Y=0,A=a)\\)라는 표현을 비교해보자. \\(Y\\)에 해당하는 값이 0이냐, 1이냐, 즉, 재범을 일으키지 않은 경우에 주목하느냐, 재점을 일으킨 경우에 주목하느냐 라는 차이도 있지만, 더 중요한 차이는 \\(P(Y|R,A)\\)라는 표현은 \\(Y\\)가 조건문 앞에, \\(P(R|Y,A)\\)라는 표현은 \\(R\\)이 조건문 앞에 있어, 확률을 표현하고자 하는 사건과 조건 사이의 위치가 뒤바뀌었다는 것이다. 즉, 프로퍼블리카와 COMPAS는 두 개의 서로 다른 조건부 확률에 주목하고 있는 것이다. 프로퍼블리카는 재범으 저지르지 않았는데 고위험군으로 분류되는 ’억울한 일이 발생하는 오류’에 주목했다면, COMPAS는 고위험군이라는 분류가 얼마나 잘 재범자를 찾아낼 수 있는지를 말해주는 ’성능’에 주목했다고 볼 수 있다. 이렇게 다른 조건부 확률에 주목하게 되면 공정성에 대해 다른 평가를 하게 된다. 아니, 더 정확하게는 이 사태가 보여주는 것은 우리가 일반적으로 ’공정성’이라고 부르는 개념 속에는 여러 가지 다른 개념이 포함되어 있다는 것이다.\n앞서 우리는 적어도 3개의 공정성 개념을 본 셈이다. 이제, 수학 기호를 이용해 이를 조금 더 일반화해 보자. 이러한 일반화 작업을 통해 사실은 무수히 많은 공정성 개념이 존재할 수 있다는 사실을 알 수 있다. 더 나아가, 안타깝게도 대부분의 경우 이러한 다수의 공정성 개념들은 동시에 성립 가능하지 않는 경우가 많다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#엄밀한-정의",
    "href": "fml.html#엄밀한-정의",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "11.3 엄밀한 정의",
    "text": "11.3 엄밀한 정의\n다음의 공정성 정의들은 위의 프로퍼블리카와 COMPAS 간의 논쟁에서 등장한 수학적 공정성 개념을 조건부 확률을 통해 일반화한 것이다.\n\n11.3.1 비인지(Unawareness)\n\\[P(R=1|X,A)=P(R=1|X)\\]\n이 수학적 정의에서 중요한 것은 등호 왼편에는 조건부 확률의 조건분에 \\(A\\)가 포함되어 있는 한편, 오른쪽에는 그렇지 않다는 것이다. 이것이 의미하는 바는, \\(A\\)라는 민감한 속성이 포함되어 있든 포함되어있지 않든, 알고리즘의 예측 결과는 같아야 한다는 것이다. 다시 말해, 알고리즘이 민감 속성을 ‘모른 채(unaware)’ 예측을 생산해야 한다는 것이다.\n이러한 공정성 요건은 매우 직관적이다. 특정한 민감 속성을 고려한 차별적 예측이 문제를 발생시키는 것이라면, 해당 민감 속성으 고려하지 않으면 되는 것이 아닐까? 이러한 직관성으로 인해, 비인지 조건은 기존 공정성과 관련된 여러 법률들과도 잘 어울린다. Barocas와 Selbst가 매우 영향력 있는 2016년 논문에서 지적한 바와 같이, 자동화된 알고리즘 이전부터 내려온 공정성과 관련된 여러 법률들은 이미 예측에서의 불평등한 대우(disparate treatment) 또는 예측의 불평등한 효과(disparate impact)를 규제해왔다. 여기서 전자는 예측 또는 판단 ’과정’에서 여러 집단을 불평등하게 다루어서는 안 된다는 것을 의미하기에, 실질적으로는 예측 대상이 특정 민감 속성을 가졌는지를 예측 과정에 포함시켰을 경우 이를 불평등 대우로 여기고 처벌하는 방식의 법 논리를 가지게 된다. 즉, 민감속성의 비인지를 요구하는 것이다. 이는 과거 법을 인공지능을 위한 예측에 그대로 활용할 수 있다는 대단히 큰 장점이 있다. 이러한 의미에서 비인지 조건은 우리가 지금까지 사용해온 용어 중 절차적 공정성에 대체로 상응한다.\n그러나, 이러한 공정성의 요구에는 치명적인 단점이 있다. 바로 민감 속성과 통계적으로 상관관계를 갖는 대리변수들이 존재할 수 있다는 것이다. 예컨대, 특정 지역에 특정 인종이 집중적으로 거주한다면 (전설적인 힙합 아티스트 에미넴이 출현한 영화 &lt;8마일&gt;에 등장하는 디트로이트를 떠올려보자), 우편번호라는 전혀 민감해보이지 않는 속성은 인종이라는 민감한 속성을 ‘높은 확률로’ 알아낼 수 있는 ‘대리변수(proxy)’가 된다. 특히, 과거에 비해 훨씬 더 많은 변수를 예측에 한꺼번에 사용할 수 있는 ’빅데이터’를 통한 예측 환경에서는 매우 많은 대리변수가 존재해서 민감 속성으 거의 정확하게 추론할 수 있는 경우가 많다. 따라서, 단순히 민감속성을 예측에 사용해서는 안 된다는 비인지 조건은 알고리즘의 공정성을 달성하기에는 ’지나치게 약한’ 조건일 가능성이 크다. 그 때문에, 많은 현대의 공정성 조건들을 이러한 대리변수의 존재를 전제한 경우가 많다.\n\n\n11.3.2 통계적 동등성(Statistical Parity)\n\\[P(R=1|A=a)=P(R=1|A=b)\\]\n통계적 동등성은 문헌에 따라서, 독립성(Independence), 또는 인구적 동등성(demographic parity), 집단 공정성(group fairness)라고 부르기도 한다.\n이 정의는 프로퍼블리카의 COMPAS에 대한 첫번째 비판에서 이미 본 것으로, 집단 사이에 예측의 차이가 없어야 한다는 것을 의미한다. COMPAS와 조금 다른 예를 들자면, 자동화된 알고리즘이 대출 승인을 해 주는 경우, 대출 확률이 남성과 여성 간에 차이가 없어야 한다는 것이다. 이는 앞서 Barocas와 Selbst가 구분했던 공정성 법률 규제의 두 가지 대상 중 두 번째에 해당하는 ‘불평등한 효과disparate impact’와 관련된다. 즉, 예측 과정이야 어떻든, ’결과적으로’ 집단 간에 차이가 없어야 한다는 주장이다.\n많은 경우, 이러한 요건은 글자 그대로 적용된다기 보다는, 근사식 형태로 요구된다. 즉, 두 집단 사이의 예측치 차이가 ’지나치게 커서는 안 된다’는 식이다. 이러한 방식은 기존 법안의 공정성 법률에서 종종 관찰되는데, 미국 법률의 four-fifth rule에 해당한다. 이는 80%의 법칙이라고 옮겨도 좋을텐데, 어떤 집단도 가장 선정 확률이 높은 집단에 비해 선정확률이 80%이하로 내려가서는 안 된다는 것이다. 즉, 대출 승인률이 남성에게 50%라면 여성에게 적어도 40%는 되어야 한다는 것이다.\n어떤 과정을 통해서 예측을 하던간에 집단 간 같은 정도의 예측 결과를 요구한다는 점에서 통계적 동등성은 매우 강한 공정성 요건인 것처럼 보인다. 하지만, 해당 요구사항에는 몇가지 문제점이 있다. 가장 쉽게 생각할 수 있는 문제는, 예측하고자 하는 진실 \\(Y\\)와 민감한 속성 \\(A\\) 사이에 실제 상관관계가 있을 수 있다는 점을 고려하지 않는다는 것이다. 예컨대, 흑인과 백인 사이의 ‘평균적인’ 대출상환능력에는 실제 20% 이상의 차이가 있을 수도 있다. 이에 대해 미리 20%의 상한을 결정하는 것은 은행으로 하여금 손해를 감수할 것을 요구하는 셈이다. 이는 사실 현실에 이미 (예컨대 대출상환능력에서의) 불평등이 존재한다면, 이윤을 추구하는 기업이 손해를 감수하고 이를 교정하기 위한 판단을 내리도록 요구할 수 있는가라는 근본적인 문제를 제기한다. 미국에서 이를 명시적으로 요구하였던 정책이 소수집단 할당제(affirmative action)으로 시행되어 왔으며, 한국에서도 ’할당제’라는 이름으로 여러 부문에서 제도화되어 왔는데, 이는 최근까지도 많은 사회적 논란을 만들어내고 있다. 그렇다면 인공지능의 예측이나 판단에도 할당제와 같이 ’현실을 교정’하는 판단 기준이 내재되어 있어야 공정하다고 할 수 있는가? 물론, 이는 장기적인 사회적 변화를 위해서 기업이 감수해야 할 몫이라고 합의할 수 있는 부분이기도 하지만, 그러한 합의가 쉽게 이루어지지는 않을 것이다.\n또 하나 중요한 문제는 이른바 예측의 ‘laziness’라는 문제이다. 즉, 평균적으로 예측치의 비율만 집단간 유사하게 맞추면 되기 때문에 예측이 얼마나 중요한지는 중요하지 않다는 것이다. 예를들어, 통계적 동등성에 따라 앞서의 예와 같이 40%의 대출 승인 비율을 준수해야 하는 집단이 있다면, 이들에게 얼마나 상환 능력에 따라 대출 승인을 해야 하는지는 통계적 동등성과 아무런 관련이 없다. 극단적으로 40%의 확률로 윗면이 나오는 동전을 던져서 랜덤하게 대출승인을 해 준다고 해도 통계적 동등성은 달성할 수가 있는 것이다. 이러한 문제점을 해결하기 위해서는 ’정확성’ 개념이 공정성 안으로 들어올 필요가 있는데, ’정확성’이 예측 결과(\\(R\\))가 실제(\\(Y\\))를 얼마나 잘 맞추는가에 대한 측정치인 이상, 공정성 정의 안에 \\(Y\\)가 포함되어야 한다는 것으 의미한다.\n통계적 동등성 개념에 대한 비판의 핵심을 좀 더 일반화하자면 두 가지 회피하기 어려운 문제로 귀결된다. 첫째, 사회에는 이미 불평등이 존재하고, 이를 그대로 받아들일 것이는 것이 공정한 것인가 아니면 교정하려는 시도가 공정한 것인가라는 해소되기 어려운 물음이 존재한다. 둘째, 현실의 불평등을 반영한 공정성 개념을 위해서는 현실의 정보에 변수가 공정성의 수학적 정의 안에 포함되어야 한다. 사후적으로라도 알게된 진실, 즉 \\(Y\\) 가 그러한 변수에 해당하며, \\(Y\\)가 포함되는 즉시 공정성 개념은 앞으로 볼 것들처럼 인공지능 예측의 ’정확성’에 기반한 버전이 된다.\n\n\n11.3.3 분리성(Seperation)\n분리성으로서의 공정성 개념은 다음과 같이 사후적으로 알게된 진실(\\(Y\\) )에 예측치가 얼마나 잘 조응하는가를 표현하는 일련의 공정성 개념을 포함한다. 분리성은 맥락에 따라, equalized odd, 정확도 동등성(accuracy parity)이라고 불리기도 한다.\n\n참양성 동등 \\[P(R=1|Y=1, A=a)=P(R=1|Y=1, A=b)\\]\n위양성동등 \\[P(R=1|Y=0, A=a)=P(R=1|Y=0, A=b)\\]\n참음성 동등 \\[P(R=0|Y=0, A=a)=P(R=0|Y=0, A=b)\\]\n위음성동등 \\[P(R=0|Y=1, A=a)=P(R=0|Y=1, A=b)\\]\n\n조건부 확률을 이용한 위의 정의들에서 조건문에 \\(Y\\)가 등장했다. 이는 특정 정확도, 즉 진실에 대비해서 얼마나 예측치가 정확하게 결과를 알아냈는지에 대한 비율을 표현하는 네가지 정확도, 참양성률(True Positive Rate; TPR), 위양성률(False Positive Rate; FPR), 참음성률(True Negative Rate; TNR), 위음성률(False Negative Rate; FNR) 등이 집단간에 동등할 것을 요구하는 공정성의 정의이다.\n예컨대 참양성 동등의 경우, 민주주의에서 강조하는 특정한 공정성 개념, ’기회의 평등’과 직접적인 연관을 갖는다. 왜냐하면, 실재로 맞춤한 능력을 갖는 사람(\\(Y=1\\))이 알고리즘에 의한 예측에 의해 기회를 얻을(\\(R=1\\))확률이 집단 간에 동일(\\(P(R=1|Y=1,A=a)=P(R=1|Y=1,A=b)\\))해야 한다는 것이 바로 기회의 평등의 개념이기 때문이다.\n또, 알고리즘 공정성을 침해한 사례중 하나로 자주 인용되는 것 중, 구글 검색 결과 백인의 이미지 검색은 잘 작동한 반면, 흑인의 이미지 검색 결과는 유인원을 다수 포함하고 있었다는 사례를 생각해보자. 이는 검색 대상이 사람인데, 사람이 아니라고 생각하는 위음성률이 흑인에게만 높았다는 것을 의미하므로 (\\(P(R=0|Y=1,A=a) &gt; P(R=0|Y=1,A=b)\\)) 위음성 동등 위배 사례에 해당한다.\n문제는 잘 기계학습에서 이 네 가지 중 하나가 성립한다고 해서 다른 정확도가 함께 올라가리라는 보장이 없을 뿐더러, 어떤 경우에는 상호 충돌하여 한 가지 정확도가 개선되면 다른 정확도는 하락하는 트레이드오프(trade-off) 관계가 성립하는 경우가 많다는 것이다. 따라서, 실질적으로는 여러 정확도 개념 중에서 예측의 대상이 되는 사람들에게 ’억울한 오류’를 막기 위한 정확도, 또는 오류율 개념에 주목하는 경우가 많다. 예컨대, 재범 위험성이 큰 사람은 실재 재범을 저지르지 않음(\\(Y=0\\))에도 ’양성(positive; \\(R=1\\))’으로 예측되는 경우 ’억울한 피해’가 발생할 수 있으므로, 잘못된 양성 판정, 즉, 위양성률에 주목하여, 집단 간에 위양성률이 동등하도록 하는 것을 최우선으로 한다. 반면, 대출 상환 능력을 예측하여 이를 바탕으로 대출 승인을 하는 경우, 실재로 대출을 갚을 수 있는 능력이 있음에도(\\(Y=1\\)), 대출을 승인을 허용하지 않는 음성 판정(\\(R=0\\))을 하는 경우, 이는 대출 신청자 입장에서는 억울한 판정이므로, 이렇게 잘못 음성 판정을 내릴 확률, 즉, 위음성률을 집단간에 동등하게 하는 것이 공정한 알고리즘의 우선적인 선택이 된다.\n여기서 ’우선적 선택’이라는 표현을 이해하는 것이 중요하다. 알고리즘의 공정성을 확보하고자 하는 조직은 결국 여러 공정성 개념 중에서 더 중요한 개념과 덜 중요한 개념 사이에서 조직의 목표에 따라 균형을 맞추는 선택을 해야 한다는 것이다. 이는 파레토 원리에 따른 균형과 타협이라는 이 장의 마지막 테마와 직접적으로 연결된다.\n또한, 인공지능의 정확도에 기반하는 분리성 개념은 차별적 현실에는 관심을 갖지 않는다. 자동대출승인의 사례를 다시 이용하자면, 백인이든 흑인이든 각 개인이 가지고 있는 실제 대출 상환능력(\\(Y\\))과 일치 예측을 내어놓도록 하는 것이 중요하지, 각각의 인종집단이 얼만큼의 대출승인을 받아야 하는지에는 관심이 없다. 따라서, 분리성 개념에 기반한 ‘공정한’ 알고리즘 현실의 ’불공정’을 있는 그대로 받아들일 뿐, 그것을 개선하는 역할을 할 수는 없다.\n\n\n11.3.4 충분성(Sufficiency)\n프로퍼블리카의 비판에 대한 COMPAS의 반비판, 즉, 재범 위험이 크다고 예측한 사람 중에서 실재 재범을 일으킨 확률이 인종 집단 간에 크게 차이나지 않는다는 주장은 분리성과는 상이한 정확도 개념을 이용하여 공정성을 정의하는 관점에 바탕을 두고 있다. 즉, 알고리즘이 양성 또는 음성이라고 예측했을 때 그것이 얼마나 실재와 일치하는지, 일종의 ’예측의 성능/성과’에 초점을 맞추는 관점이다. 충분성은 문헌에 따라 예측률 동등(Predictive Rate Parity)라고 부른다. 이를 수식으로 표현하면 다음과 같다.\n\n양성예측동등(Positive Predictive Parity) \\[P(Y=1|R=1,A=a)=P(Y=1|R=a,A=b)\\]\n음성예측동등(Negative Predictive Parity) \\[P(Y=0|R=0,A=a)=P(Y=0|R=0,A=b)\\]\n\n기계학습에 대한 기초지식이 있는 독자라면, 사실 분리성과 충분석은 기계학습 모형평가에서 늘 사용하는 두 가지 정확서 개념, 즉, 재현율(recall)과 정밀도(precision)이 인구 집단 간에 동등한지를 묻는 것에 다름 아니라는 것을 알 수 있을 것이다. 그리고 기계학습에서 재현율과 동등성을 동시에 높이는 것이 어려운 것처럼, 분리성이 높다고 해서 동등성이 높아지는 것도, 그 반대도 성립하지 않는다. COMPAS의 예에서 프로퍼블리카와 Northpointe 간의 입장 차이는 바로 이러한 문제로부터 기인한다.\n또한, 충분성 역시 분리성과 유사하게 차별적 현실을 그대로 받아들이는 공정성 개념이라는 문제점이 존재한다.\n지금까지 논의한 공정성 개념이 가장 고전적인 논의에 속하는 것이라면, 최근에 더욱 많이 논의되는 새로운 공정성 개념들도 있다. 예컨대 마이크로소프트의 Cynthia Dwork(윤리적 알고리즘 분야에서 최고의 권위를 가지고 있는 미국의 여성 컴퓨터공학자)와 동료들이 제시한 개인적 공정성(Individual Fairness)은 비슷한 특성을 가지고 있는 개인이 다른 예측 결과를 경험해서는 안 된다는 의미에서의 공정성 개념으로, 우리가 앞서 살펴본 공정성 개념들이 집단간 비교를 통해 정의되는 것에 비해서, 각 개인에게 공정한 경험을 보장하려고 하는 시도라는 점에서 의의가 있다. 또한, 영국의 앨런 튜링 연구소의 Chris Russell과 동료들이 제안한 인과적 공정성(Causal Fairness)은 민감한 속성이 다른 비민감 속성의 ’원인’으로 영향을 미쳐 최종적으로 예측 결과에 영향을 미치는 메커니즘을 고려하고자 하는 공정성 개념이다. 이러한 분야는 여전히 많은 발전을 이루고 있고, 논쟁적인 측면이 커 여기서는 생략하도록 한다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#불가능성-정리-impossibility-theorem",
    "href": "fml.html#불가능성-정리-impossibility-theorem",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "11.4 불가능성 정리 (Impossibility Theorem)",
    "text": "11.4 불가능성 정리 (Impossibility Theorem)\n이렇게나 많은 공정성 개념이 존재한다는 것은 그 자체로 골치아픈 문제이지만, 코넬의 정보과학 대가인 Jon Kleignberg등이 명명한 바, 이른바 ’불가능성 정리’라고 부르는 수학적 결과는 더욱 더 큰 난점을 제기한다. 이 불가능성 정리에 따르면, 우리가 위에서 살펴본 공정성 정의들은 수학적으로 동시에 성립할 수 없다는 것이다. 이를 다시 표현하자면, 우리가 말하는 공정성이라는 용어 속에는 기실 다양한 차원의 공정성 개념들이 숨어 있는데, 이들은 본질적으로 서로 모순된다는 것이다. 따라서, 프로퍼블리카가 ’분리성’에 근거하여 제기했던 비판과 ’충분성’에 근거해서 Northpointe가 제시한 반박은 애초부터 엇갈릴 수밖에 없고, 그 입장 차이는 해소할 방법이 없다는 것이다!\n그렇다고 해서 이러한 결론이 공정성은 우리가 성취할 수 없는 목표라는 것을 의미하는 것일까? 그렇지 않다. 사실, 이렇게 서로 모순되는 공정성 개념이 우리의 ’공정성’이라는 용어의 일상적 용법에 숨어있었다는 것을 알아내는 것은 후퇴라기 보다는 한발 전진이라고 할 수 있다. 문제 해결의 전제는 우리가 마주한 문제가 무엇인지를 아는 것이기 때문이다. 이렇게 사회과학적 영역에서 수학적 조작화를 통해 인간 언어의 모호성을 극복하고 다양한 차원들이 존재함을 드러내서 더 나아가 일종의 불가능성 정리에까지 도달하는 것은 처음이 아니고, 오히려 사회과학의 진전에서 중요한 역할을 하였다. 예컨대, 노밸경제학 수상자이기도 한 케네스 에로(Kenneth Arrow)는 후생경제학의 불가능성 정리를 통해 어떤 상황에서도 모든 사람의 만족을 달성하는 민주적 방법이 없다는 것을 수학적으로 증명하였다. 이는 국가의 계획 없이 효율적 자원 배분이 가능하다는 경제학의 이상에 큰 도전을 가져왔는데, 후에는 이러한 불가능성을 받아들임으로써, 경제학을 정치적 결정과정을 설명하는 학문으로 확장하는 계기가 되었다. 또한 마르크스주의 정치경제학 비판에서도 전형문제, 즉, 노동에 기반한 가치(value)와 시장에 기반한 가격(price) 체계 사이의 수학적 불일치성 문제를 해결하려는 노력들은, 마르크스 경제학을 경험 연구의 영역으로 확장시키는 계기가 되었다. 마찬가지로, 공정성 개념의 다수성과 이들 사이의 동시 성립 불가능성 문제는, 공정한 인공지능이 성취 불가능하다는 절망으로 우리를 이끌기 보다는, 오히려 중요한 것은 인간들의 달성하고자 하는 공정성 목표의 합의와 합리적인 타협의 방식이라는 것을 분명하게 한다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#파레토-경계를-통한-균형-찾기",
    "href": "fml.html#파레토-경계를-통한-균형-찾기",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "11.5 파레토 경계를 통한 균형 찾기",
    "text": "11.5 파레토 경계를 통한 균형 찾기\n문제가 양립할 수 없는 다수의 목표들이라면, 결론은 결국 목표들 간의 취사선택 또는 타협이 될 것이다. 그리고 수학저 조작은 ‘체계적 타협’을 위한 도구까지 마련해준다(Kearns). 이러한 타협을 달성하는데 핵심적으로 사용하는 개념적 도구는 파레토 효율성(Pareto Efficiency)라는 개념이다. 이 개념은 19세기와 20세기초에 주로 활동한 빌프레도 파레토(Vilfredo Pareto)라는 이탈리아의 사회학자이자 경제학자의 이름을 딴 것으로, 파레토 효율성 말고도 파레토의 이름이 붙은 다른 유명한 개념들도 많이 있습니다. 파레토 효율성은 사회적 최적상태(optimality)를 규정하는 하나의 방식으로, 대략적으로 “어느 누군가의 희생 없이는 다른 사람들의 상태에 대한 개선이 불가능한 상태”를 의미한다고 볼 수 있습니다. 이를 조금 더 일반화하자면, “하나의 목표에 대한 양보 없이 다른 목표들의 개선이 불가능한 상태”라고 생각할 수 있습니다. 그런데, 이러한 개념에 대한 설명이 그다지 ’최적상태’ 또는 ‘효율성’ 등의 표현과 어울리지 않는 것처럼 보이는 것도 사실이다. 사실 파레토 효율이라고 부를 수 있는 상태는 무수히 많이 존재한다. 오히려 파레토 효율성의 의의는 파레토 효율성 관점에서 ‘열등한’ 사회적 선택을 찾아내는 기준이 된다는 점에 있다고 볼 수 있다. 이를 이해하기 위해서 다음 그래프를 살펴보자.\n아래 그림은 다시 그리는 것이 나을 수 있겠다…\n\n\n\n파레토 경계를 통한 균형 찾기\n\n\n출처: Ho, D. E. & Xiang, A. (2020). Affirmative algorithms: The legal grounds for fairness as awareness,\nhttps://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/\n위의 그림은 자동화된 알고리즘을 이용하는 가상의 조직이 정확도(중 하나의 지표)와 공정성(중 하나의 지표)를 자동화된 판단을 통해 달성하고자 하는 두 가지 목표라는 것을 가정하고 있다. 그래프 상의 각 점들은 그 조직이 선택할 수 있는 예측 알고리즘들이 성취할 수 있는 정확도와 공정성의 조합을 의미한다. 이제 가장 바깥에 있는 점들을 연결한 선을 살펴보자. 이 선 위에 존재하는 알고리즘은 ‘파레토 효율적’이다! 왜 그런가 예컨대 해당 선 위에 있는 알고리즘 A를 생각해보자. 이 알고리즘보다 공정성이 높은(즉, A보다 오른쪽에 위치한) 알고리즘을 선택하려면 어떻게 해야 할까? 일단, A가 위치한 경계선 오른쪽 바깥에 있는 알고리즘은 기술적인 한계로 인해 그 조직이 선택할 수 없는 알고리즘이다. 따라서, 즉, A에서 공정성과 정확성을 ’동시에’ 개선할 방법은 없다! 따라서, 공정성을 개선하기 위해서는 정확도를 희생하면서, 경계선 상 혹은 안쪽에 있는 다른 알고리즘을 선택하는 수밖에 없다. 같은 논리에 따라, 반대로 A알고리즘에서 정확성을 개선한 알고리즘을 선택하고 싶다면, 공정성을 다소 희생하는 수밖에 없다. 이제 “하나의 목표에 대한 양보 없이 다른 목표들의 개선이 불가능한 상태” 라는 파레토 효율성의 정의를 생각해보면, A는 정확히 해당 정의에 부합하는 알고리즘이라는 것을 알 수 있다. 사실, 이러한 논리는 경계선 상에 있는 모든 알고리즘에 적용된다. 따라서, 해당 경계선 상에 존재하는 알고리즘은 모두 ’파레토 효율적’인 알고리즘이다! 이러한 중요성 때문에 해당 경계선을 특별하게 ’파레토 경계(Pareto Frontier)’라고 부릅니다.\n이제 파레토 경계 안 쪽에 있는 알고리즘들, 예컨대 B와 같은 알고리즘에 대해 생각해보자. B와 같은 알고리즘은 정확도와 공정성을 ‘동시에’ 개선할 수 있다. 왜냐하면 B에서 오른쪽 상단 방향으로 파레토 경계 안 쪽에 더 좋은 알고리즘이 존재하기 때문이다. 이렇게 두 가지 목표를 동시에 개선하느 것을 ‘파래토 개선(Pareto Improvement)’라고 부른다. 이렇게 파레토 개선이 가능한 알고리즘은 어떠한 희생도 없이 개선이 가능하므로, 개선된 알고리즘에 비해 확실히 ’열등한’ 알고리즘이라고 볼 수 있다. 따라서, 파레토 개선이 가능한 알고리즘을 ’파레토 열등(Pareto Dominated)’하다고 한다. 이제, 파레토 효율적인 알고리즘(=파레토 경계상의 알고리즘)은 파레토 개선이 불가능한 알고리즘이라는 것을 알 수 있고, 파레토 경계 안 쪽에 존재하는 알고리즘은 확실히 파레토 열등한 알고리즘이라는 것을 알 수 있다. 따라서, 이러한 파레토적 사고 방식을 통해 단 하나의 최적 알고리즘을 알 수 없다고 하더라도, 적어도 파레토 열등한 알고리즘이 아니라, 파레토 곡선 상의 파레토 효율적 알고리즘 중에 선택이 이루어져야 한다는 것을 알 수 있다!\n그렇다면 파레토 경계상의 많은 알고리즘 중에는 어떠한 알고리즘을 선택해야 하는가? 여기서부터는 조직이 여러 목표들 사이의 경중을 어떻게 평가할 것인가에 대한 합의된 기준이 필요하다. 즉, 다수의 목표를 동시에 가장 최적 수준에서 달성할 수는 없으므로, 목표들 간의 ‘타협’이 이루어져야 하며, 이는 조직의 형태, 목적, 구성원의 생각, 제도 등에 따라 다를 수 있다는 것이다. 이러한 타협 역시 파레토 경계 위에서 단순히 ’감으로’ 이루어질 필요는 없다. 별도로 자세하게 다루어야 하는 내용이므로, 여기서 자세하게 다루기는 어렵지만, 해당 조직이 여러 목표 간에 어떻게 타협하는 것을 최선이라고 생각하는가, 혹은 그렇게 합의했는가를 일종의 조직의 선호를 나타내는 수학적 ’함수’로 표현할 수 있다. 그리고 그 함수는 다음과 같이 그래프로 표현 가능하다.\n\n\n\n무차별 곡선을 이용한 알고리즘 선택\n\n\n위의 그림에서 조직의 선호를 나타내는 곡선은 ‘무차별 곡선(indifference curve)’이라고 하는데, 해당 무차별 곡선의 형태는 정확성과 공정성 간의 타협에 대한 조직의 선호에 따라 달라진다. 이제 ’주어진 선호/합의’ 하에서 최적 알고리즘은 두 그래프가 만나는 점에서 결정된다. (경제학원론을 배운 학생이라면 이러한 사고 방식에 익숙할 것이다. 보지 못한 경우라도, 경제학원론 교과서의 소비자효용, 후생경제학 부분을 훑어보길 바란다. 이러한 선호 표현 방식과 그를 통한 의사 결정의 문제를 다루는 분야를 의사결정과학Decision Science이라고 부른다). 만약, 더 많은 정확성과 더 많은 공정성 개념을 고려하여 타협을 하고자 한다면, 위의 파레토 경계와 파레토 곡선은 더 이상 2차원의 그래프로 표현할 수는 없을 것이다. 그러나 시각화하기 어려운 고차원의 선택 문제라고 하더라도 동일한 논리를 통한 선택은 여전히 가능하다.\n지금까지 훑어본 다양한 공정성의 개념과 양립불가능성 정리, 그리고 파레토 효율성을 통한 알고리즘 선택 과정은 알고리즘 공정성에 대한 과학적 논의가 던져주는 부정적 전망과 부정적 전망을 동시에 보여준다. 모두 합리적으로 들리는 여러 차원의 공정성을 동시에 달성할 수 없다는 것, 그리고 공정성과 예측의 정확성 역시 자주 충돌한다는 것은 인간사회가 인공지능의 자동화된 예측과 결정에 의존하는데 위험성이 수반된다는 것을 의미한다. 하지만, 이러한 양립불가능성은 자동화된 결정시스템에 고유한 것이 아니다. 인간에 의해서 동일한 결정이 이루어진다고 하더라도, 동일한 문제는 발생한다. 다만, 그 전에는 이렇게 엄밀한 수학적 접근을 시도하지 않았기에 우리가 가지고 있는 딜레마를 정확히 보고 있지 못했을 뿐이다. 오히려 공정성의 다양한 정의를 밝히는 작업은 조직의 합의된 목표와 그 안에 포함된 타협을 체계적으로 알고리즘 선택에 반영할 수 있는 도구를 제공한다는 점에서 진보라고 할 수 있다. 우리에게 이러한 도구들이 있는 한, 선택은 늘 가능하다. 중요한 것은 사람과 조직이 좋은 목표를, 좋은 타협안을 만들어내는 것이다. 사람의 일을 기계가 대신 해 줄수는 없는 일이라는 교훈은 또 다시 반복된다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#남은-문제들",
    "href": "fml.html#남은-문제들",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "11.6 남은 문제들",
    "text": "11.6 남은 문제들\n우리가 위에서 살펴본 공정성 개념들은 ‘사후적으로라도’ 정답을 알 수 있는 기계학습, 즉, 지도학습(Supervised Machine Learning)을 사용하는 경우 예측 결과의 공정성을 개선하기 위해 사용할 수 있는 공정성 개념의 수학적 조작화라는 주제에 국한된 것이었다. 이외에도 인공지능 공정성을 다루는 다른 연구 영역들이 여전히 존재한다. 여기서는 교재의 목적상 이 모든 논의를 담지는 못하고, 그러한 논의들이 발전되고 있다는 것 정도를 지적하도록 한다. 관심있느 독자들은 챕터 말미의 ’더 읽을거리’와 ’참고문헌’을 참조하기를 바란다.\n\n11.6.1 비지도학습의 공정성\n가장 먼저 생각해볼 수 있는 것은 비지도학습(Unsupervised Machine Learning)에서의 공정성이다. 비지도학습은 우리가 위의 논의 속에서 사후적으로라도 알게 되는 것으로 가정했던 예측하고자 하는 진실, \\(Y\\)가 존재하지 않는 경우의 기계학습을 의미한다. 예컨대 온라인 소비자의 행동 데이터로부터 충성도가 높은 소비자를 찾아내는 상황을 생각해보자. ’충성도 높은 소비자’라는 것은 어차피 마케터의 관점에서 존재하는 관념일 뿐, 이를 명확하게 정의하기 어렵기 때문에, 사후적으로라도 ’충성도 높은 소비자’가 누구였는지를 알아내는 것은 어렵다. 이러한 경우에는 비슷한 행동을 보이는 소비자들끼릴 묶어내는 ’군집화(Clustering)’라는 비지도학습 기술을 사용한다.\n또 최근 주목받는 예로 인공지능에 의한 기계번역을 하는 경우를 떠올려보자. 언어의 요소(예컨대 단어)를 다차원의 수치로 표현한다음, 이 수치가 가까운 다른 언어의 요소로 대체하는 방식을 이용한다. 예컨대, ’개’와 ’dog’는 다른 언어에 속한 단어이지만, 숫자로 표현된 두 단어의 거리는 가깝기에 이를 이용한 컴퓨터는 번역을 할 수 있는 것이다. 이렇게 언어를 다차원의 수치로 표현한 것을 ’임베딩(Embedding)’이라고 하는데, 단어의 임베딩을 찾아내는 작업 역시 비지도학습에 해당한다. 사실 임베딩은 자동 번역뿐 아니라, 언어를 음악으로, 그림으로, 동영상으로 ’번역’하는 기술에도 다양하게 사용된다.\n정확성에 기반한 공정성 개념이였던 분리성, 정확성의 수식에 \\(Y\\)가 포함되었었다는 것을 상기해보면, 그러한 공정성 개념은 \\(Y\\)가 존재하지 않는 비지도학습에는 적용되기 어려울 것이다. 그럼에도 불구하고, 비지도학습에는 다양한 공정성 문제가 발생한다. 예컨대. ’여성’이라는 단어의 임베딩은 ’간호사’의 임베딩에 더 가깝고, ’남성’이라는 단어의 임베딩은 ’의사’의 임베딩에 더 가깝게 도출된다면, 이는 임베딩에 편향이 반영되어있다는 것을 의미한다. 그리고 이는 인공지능이 이용하는 학습데이터, 즉, 인간이 만들어낸 데이터가 편향된 이상 인공지능을 자동으로 해결해주지 않는다. 따라서, 최근에는 이를 해결하기 위한 다양한 공학적 시도들이 있다. 관심있는 독자들은 키언스의 책이나, Bolukbasi 등 (2016) 등의 논문을 살펴보기 바란다.\n\n\n11.6.2 데이터의 편향성\n또 한가지 중요한 주제는 데이터 그 자체가 가지는 편향이다. 앞서 언급한 바와 같이, 인공지능은 인간의 오류와 편견을 그대로 반복하거나, 더 나쁘게는 확대재생산한다. 이렇게 인간의 편향이 인공지능의 편향에 스며드는 가장 영향력 있는 통로는 알고리즘의 복잡한 디자인이라기 보다는 데이터 그 자체이다. 2016년, Barocas와 Selbst는 California Law Review에 데이터에 편향이 숨어드는 경로에 대한 매우 영향력 있는 논문을 게재하였다. 그러한 경로는 매우 다양해서, ‘좋은 근로자’, ’높은 금융 신용도’와 같이 알고리즘을 통해 예측하고자 하는 목표 그 자체를 정의하는데 있어 편향이 포함될 수 있다. 예컨대 ’좋은 근로자’를 정의하면서 흔히 남성성과 결부되는 속성을 포함시키는 경우가 그러하다. ’좋은 근로자’가 중립적으로 잘 정의되었다고 하더라도, 기존 사원 기록에서 특정 근로자를 좋은 근로자로, 특정 근로자를 부족한 근로자로 인간이 ’레이블링’하는 인간의 판단에도 편향이 포함될 수 있다. 예컨대, 과거 인사평가 기록을 인공지능의 학습데이터로 이용했는데, 남성 인사 평가자가 남성 사원에 대해 유리한 판단을 해 왔다면, 이러한 학습데이터를 이용한 인공지능은 편향적인 결정을 생산할 것이다. 또한, 데이터 수집 과정에서 특정 그룹이 과도하게 많이 데이터에 포함되거나, 적게 포함된 경우에도 데이터가 편향을 가지게 된다. 예컨대, 특정인종의 거주지역에 집중적으로 순찰을 수행하면서 발생한 기록을 인공지능의 학습 데이터로 이용했을 때(과대표집), 해당 인종의 범죄 위험을 높게 평가한다든지, 소수인종의 인터넷 이용 데이터가 다수인종에 비해 상대적으로 적어(과소표집) 소수 인종에 대한 예측이 지나치게 부정확한 예가 그렇다. 또, 직접적을 차별을 하지 않더라도, 특정 집단에게 유리한 개인 특성(예컨대, 출신대학, 출신지역 등)을 학습에 과도하게 많이 포함시키는 경우에도 편향이 발생할 수 있다.\n\n\n11.6.3 개선 방법 (Fair Algorithm)\n마지막으로, 인공지능의 편향 위험이 관측되었을 때, 이를 수정하기 위한 방법에 대한 논의 역시 활발하다. 실질적으로 어떻게 인공지능을 어떻게 더 공정하게 만들 수 있을 것인지에 관한 논의이므로, 최근 컴퓨터공학자들의 노력은 이 부분에 집중되고 있다. 가장 기초적으로는 데이터에 대한 사전작업을 통해서 데이터에 숨은 편향성을 미리 교정한 후(전처리; preprocessing) 인공지능을 학습시키는 방법 (대표적으로는 Zemel et al.의 2013년 ICML 페이퍼), 인공지능 학습시, 위의 논의에서 정의한 공정성 요구조건을 일종의 학습의 제약으로 포함시키는 방법 (대표적으로는 Zafal et al.의 AISTAT 2017년 페이퍼), 인공지능의 학습이 종료된 후에 공정성 정의를 이용하여 학습 내용을 수정하는 방법 (이른바 후처리;postprocessing)으로 나눌 수 있다. 이 분야는 기술적인 지식을 요구하므로, 본 챕터에서 자세하게 다룰 수는 없지만, 기계하습의 원리에 대한 다소간의 이해가 있다면, 위에서 논의한 공정성의 정의들이 이러한 ‘공정한 알고리즘’ 개발에 직접적으로 사용되고 있다는 것을 알 수 있다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#결론",
    "href": "fml.html#결론",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "11.7 결론",
    "text": "11.7 결론\n이 장에서는 활발하게 공정한 인공지능(FML) 논의에서 ’지도학습 맥락에서 공정성의 수학적 정의’를 주로 살펴보았다. 이러한 공정성 개념에 대한 수학접 접근을 통해 알게된 결론은, 우리가 일상적으로 사용하는 ’공정’의 개념에는 서로 모순되는 다른 차원의 공정성 개념이 포함되어 있다는 것이다. 이는 일견 우리가 일반적으로 옳다고 여기는 가치를 동시에 성취 불가능하다는 부정적 결론으로 보일 수 있지만, 꼭 그렇지는 않다. 오히려 수학적 접근을 통해 우리가 이미 가지고 있던 개념의 모호성을 더욱 분명하게 함으로써 논의를 한걸음 진전시키는 것이라고 볼 수 있다. 구체적으로, 공정한 인공지능 논의를 통해 공정성은 더 발달한 인공지능이 기술적으로 해결할 수 있는 것이 아니라, 모순되는 목표들 간에 인간이 도출해내야 하는 타협과 합의를 통해서 성취가능한 것이라는 것을 알게 되었으며, 그러한 타협과 합의가 존재한다면 공정성의 수학적 정의는 이를 컴퓨터가 이해할 수 있는 언어로 번역함으로써 알고리즘을 인간의 결정에 부합하도록 체계적으로 개선할 수 있는 통로를 마련해주며, 대안으로 개발되고 있는 공정한 인공지능 알고리즘들은 대부분 이러한 수학적 정의에 의존하고 있다.\n지금까지 본 바와 같이 인공지능의 편향은 인간 세계에 존재하는 편향이 반영된 것이며, 이를 해결하기 위한 방법 역시 인간의 합의에 달려있다. 수학적/공학적 논의는 인간 세계가 생각하는 공정을 기계가 이해할 수 있도록 하는 통로를 마련하는 것에 불과하다. 단, 이렇게 인공지능의 공정성을 인간의 불공정성의 단순한 반영으로 생각하는 일종의 수동적 접근을 전제한다. 이러한 수동적 접근은 분리성, 충분성의 정의에서 본 것과 같이 이미 세상에 존재하는 불평등은 주어진 것으로 받아들이는 경향이 있다. 그러나, 인간 세계의 불평등을 그대로 받아들이고 있는 인공지능이 인간의 개입없이 판별하는 영역이 늘어나면 늘어날 수록 그러한 불평등은 확대재생한 될 공산이 크다. 즉, 적극적인 교정을 위한 장치가 존재하지 않는 자동화된 결정은 현상 유지를 넘어서 사태를 악화시킬 가능성이 크다는 것이다. 따라서, 최근에는 affirmative action을 취하는 공정한 인공지능에 대한 논의가 발전하고 있기도 하다. 그러나 이렇게 공정한 인공지능과 관련된 개념이 새밀하게 발전하면 발전할수록 분명해지는 것은 공정한 인공지능은 인간과 사회의 결정과 합의에 의해서 지속적으로 근접할 수 있는 것이지, 훌륭한 솔루션에 의해 기술적으로 해결할 수 없는 문제라는 것이다. 오히려 지금까지 논의한 공정성의 수학적 정의는 인간과 인공지능이 사회적으로 합의된 바를 소통하고 그러한 목표를 위해 ’연합’하는 통로를 제공해주는 도구라고 이해하는 것이 필요하다. 인공지능이 편만한 세계에서 더이상 인간이 홀로 공정한 세계를 달성할 수는 없는 일이지만, 그러한 현실을 받아들이는 것이 인간이 해야 할 일을 회피해도 좋다는 것을 의미하는 것은 아니라는 점을 명심해야 한다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#더-읽을거리",
    "href": "fml.html#더-읽을거리",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "11.8 더 읽을거리",
    "text": "11.8 더 읽을거리\n마이클 키언스, 아론 로스 (2021). &lt;알고리즘 윤리: 안전한 인공지능 설계 기법&gt;. 에이콘 출판사.\n데이비드 섬프터 (2022). &lt;알고리즘이 지배한다는 환상: 수학으로 밝혀낸 빅데이터의 진실&gt;. 해나무.\nBarocas, S., Hardt, M., & Narayanan, A. (2023).  Fairness and machine learning: Limitations and opportunities. MIT Press. https://fairmlbook.org/\nBarocas, S., & Selbst, A. D. (2016). Big data’s disparate impact. California Law Review, 671-732.\nBolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#참고문헌",
    "href": "fml.html#참고문헌",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "11.9 참고문헌",
    "text": "11.9 참고문헌\n홍찬숙 (2021) 청년의 무엇이 ‘성평등 프레임에서 젠더갈등과 공정성 프레임으로’ 변화한 것인가?\nBarocas, S., & Selbst, A. D. (2016). Big data’s disparate impact. California law review, 671-732.\nBolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.\nChouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2), 153-163.\nDwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012, January). Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference (pp. 214-226).\nKleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807.\nRussell, C., Kusner, M. J., Loftus, J., & Silva, R. (2017). When worlds collide: integrating different counterfactual assumptions in fairness. Advances in neural information processing systems, 30.\nhttps://afraenkel.github.io/fairness-book/content/05-parity-measures.html\nhttps://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb\n\n\n\n\nBarocas, Solon, 와/과 Andrew D Selbst. 2016. “Big data’s disparate impact”. California law review, 671–732.\n\n\nDwork, Cynthia, Moritz Hardt, Toniann Pitassi, Omer Reingold, 와/과 Richard Zemel. 2012. “Fairness through awareness”. In Proceedings of the 3rd innovations in theoretical computer science conference, 214–26.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "fml.html#footnotes",
    "href": "fml.html#footnotes",
    "title": "11  공정한 인공지능을 위한 기술적 해법",
    "section": "",
    "text": "프로퍼블리카는 기사와 함께 그들이 정보공개청구를 통해 취득하여 보도에 사용한 데이터를 모두 공개하고 있다. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing 참조.↩︎\n특정 확률에 따라 그 값이 결정되는 변수를 확률변수라고 한다.↩︎",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>공정한 인공지능을 위한 기술적 해법</span>"
    ]
  },
  {
    "objectID": "xai.html",
    "href": "xai.html",
    "title": "12  설명 가능한 알고리즘",
    "section": "",
    "text": "12.1 설명하기 쉬운 알고리즘\n먼저, 인공지능의 근간이 되는 기계학습, 그 중에서도 지도학습(supervised learning) 방법은 분류(classification) 문제에 대해 복습해보도록 하자. 분류의 문제는 어떠한 특성(feature)을 이용해 타겟(target)을 예측하는 문제를 의미한다. 기계학습을 처음 배울 때 하는 이미지 특성을 이용해 개와 고양이를 자동으로 나누는 알고리즘, 손글씨 이미지를 이용해 숫자를 파악하는 알고리즘 같은 것들이 모두 분류 알고리즘이다. 설명의 편의를 위해, 여기서는 가장 간단한 알고리즘을 생각해보자. 즉, 두 가지의 특성을 이용해 0 또는 1의 판단을 하는 알고리즘이다. 첫번째 특성을 \\(X_1\\), 두번째 특성을 \\(X_2\\)라고 하고 이들 각각은 연속적인 숫자를 그 값으로 가질 수 있다고 가정할 것이다. 마지막으로, 예측하고자 하는 타겟, \\(Y\\)는 0 또는 1의 값을 갖는데, 여기서 우리가 생각하는 단순한 분류 알고리즘은 이 타겟에 대한 예측, \\(R\\)을 만들어낸다. 즉, 실제로 타겟이 \\(Y=1\\)일 때, 예측 역시 \\(R=1\\), \\(Y=0\\)일 때 예측 역시 \\(R=0\\)이라면 좋은 분류알고리즘이라고 할 수 있을 것이다. 비근한 예로, 소득(\\(X_1\\))과 신용점수(\\(X_2\\))를 이용해 대출을 상환할 수 있는지(\\(Y=1\\)), 그렇지 않은지(\\(Y=0\\))를 판단하는 알고리즘을 만드는 상황을 생각해 볼 수 있다.\n이러한 경우에 적용할 수 있는 가장 간단한 분류 알고리즘 중 하나로 이야기되는 것 중 하나는 ’로지스틱 회귀(Logistic Regression)’라는 방식이다. 두 개의 특성, \\(X_1\\), \\(X_2\\)와 예측치 \\(Y\\)가 0 혹은 1의 값을 갖는 우리의 분류 문제를 해결하기 위해 로지스틱 회귀 모형을 적용하면, 그 결과는 다음 그림과 같이 시각화할 수 있다.\n즉, 위의 맥락에서 좋은 로지스틱 회귀 알고리즘이라 함은, 첫번쨰 특성과 두 번째 특성을 통해 2차원 평면 위에 표현된 데이터를 \\(Y=1\\) (+로 표현됨)과 \\(Y=0\\)(-으로 표현됨)을 가장 정확하게 구분하는 직선 하나를 긋는 문제이다. 이러한 모형은 매우 단순해서 사람들에게 쉽게 ’설명’할 수 있을 것이다. 즉, 첫 번째 특성과 두 번째 특성의 값이 모두 클 수록 \\(Y=1\\)로 예측될 가능성이 크다. 자동 대출 승인의 예를 다시 사용하자면, 소득과 신용점수가 클 수록, 대출을 갚을 것이라고 예측할 가능성이 크다(즉, 자동대출승인이 될 가능성이 크다).\n이러한 로지스틱 모형을 수식으로 표현하면 다음과 같다.\n\\[\nP(y=1) = \\frac{1}{1+ e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}}\n\\] 복잡해 보이지만, 위의 수식에서 \\(-\\beta_1/\\beta_2\\)가 위 그래프의 기울기에 해당한다는 아주 단순한 직관을 가지고 있다. 따라서, 로지스틱 회귀는 쉽게 `설명 가능한’ 모형에 속한다고 할 수 있다.\n그런데, 이런 간단한 분류 방법이 늘 현실적이지는 않다. 다음과 같은 \\(Y\\)의 분포에서는 0과 1을 분류하는 경계선이 곡선인 편이 훨씬 더 좋은 예측 성능을 낼 것이기 때문이다.\n이러한 분류 알고리즘은 비선형성(nonlinearity)을 갖는다고 하며, 특히 이미지 인식을 이용한 분류, 텍스트의 분류와 같이 일상에서 인공지능이 다루어야 하는 데이터의 처리는 대부분 비선형 분류 알고리즘을 바탕으로 한다. 이렇게 분류 알고리즘에 비선형성을 허용하기 위해서는 로지스틱 회귀 모형의 수식보다 훨씬 더 복잡한 수식을 바탕으로 한 알고리즘이 필요할 것이다. 그런 경우에는 모형의 모수(parameter; 로지스틱 회귀 모형에서 \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\)와 같이 알고리즘이 학습(learning)을 통해 찾아내는 숫자들)의 수도 훨씬 많고, 로지스틱 회귀 모형의 모수와 같이 직관적인 의미를 갖지 않는다. 즉, 이러한 경우에는 분류 알고리즘이 학습한 결과를 확인한다고 하더라도, 그것의 직관적 의미를 인간이 이해할 수는 없다는 것이다. 이럴 때, 분류 알고리즘, 더 일반적으로 기계학습 모형이 블랙박스와 같은 성격(blackboxedness)를 갖는다고 말한다. 인공지능, 빅데이터의 사용과 관련하여 가장 자주 언그되는 분류 알고리즘이라고 할 수 있는 인공신경망(artificial neural network; ANN)이나 그에 그반한 딥러닝 모형들 역시 블랙박스와 같은 성격을 갖는다. 즉, 해당 알고리즘들은 모형의 복잡도를 상승시켜 더 높은 유연성을 허용하고, 그를 통해 모형의 성능을 높일 수 있었는데, 그 결과 ‘설명할 수 없는(unexplainable)’ 알고리즘이 되었다는 것이다.\n결국, 인공지능 발전의 바탕에는 하나의 딜레마가 놓여있다. 모형의 성능과 설명가능성을 동시에 취할 수 없고, 하나를 얻으면 하나를 포기해야 한다는 것이다. 이러한 딜레마를 다른 말로는 예측력과 설명가능성 사이의 트레이드오프(trade-off) 관계라고도 한다. 이를 그림으로 표현하면 다음과 같다.\n선형/로지스틱 회귀 모형 이외에도, 비교적 설명 가능성이 높은 분류 알고리즘으로 규칙 기반(Rule-Based) 알고리즘, 의사결정 나무(Decision Tree). 이러한 알고리즘은 인공지능 개발 초창기에는 중요하게 다루어졌으나, 부스팅, 딥러닝과 같이 복잡한 모형이 등장한 후로는 상용화된 인공지능에서 사용되고 있다고 보기는 어렵다(물론, 여전히 많은 복잡한 알고리즘들이 이와 같이 간단한 모형에 기반을 두고 있다). 따라서, 최근 사용되는 대부분의 알고리즘은 설명 가능성이 낮은 알고리즘이라고 할 수 있다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>설명 가능한 알고리즘</span>"
    ]
  },
  {
    "objectID": "xai.html#설명하기-쉬운-알고리즘",
    "href": "xai.html#설명하기-쉬운-알고리즘",
    "title": "12  설명 가능한 알고리즘",
    "section": "",
    "text": "로지스틱 회귀를 이용한 분류\n\n\n\n\n\n\n\n\n\n비선형 분류문제\n\n\n\n\n\n\n\n복잡성과 설명가능성 간의 트레이드오프",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>설명 가능한 알고리즘</span>"
    ]
  },
  {
    "objectID": "xai.html#설명-가는성을-높이는-기술-xai",
    "href": "xai.html#설명-가는성을-높이는-기술-xai",
    "title": "12  설명 가능한 알고리즘",
    "section": "12.2 설명 가는성을 높이는 기술: XAI",
    "text": "12.2 설명 가는성을 높이는 기술: XAI\n그렇다면, 지금까지의 논의가 예측력을 높은 알고리즘, 또는 뛰어난 성능의 인공지능을 가지기 위해서는 설명을 포기해야 한다는 것을 의미할까? 꼭 그렇지만은 않다. 많은 인공지능 연구자들은 예측력과 복잡도를 보존하면서도 여전히 설명하기 위한 기술들을 개발하기 위해 노력하고 있다. 마치 위에서 설명한 근본적 딜레마와 배치되는듯한 이러한 노력의 바탕에는 에측을 위한 모형을 그대로 둔 채로, ’설명을 위한 장치’를 개발한다는 아이디어가 있다. 즉, ’설명가능한 인공지능(eXplainable AI; XAI)’라고 부르는 영역에서 활발하게 논의되고 개발되고 있는 모형들은 그 자체로 설명이 쉬운 기계학습 모형의 개발이 목적이라기 보다는, 예측모형에 덧붙일 수 있는 설명 장치들이라고 할 수 있다.\n이러한 설명 장치들도 몇 가지 유형으로 구분할 수 있다. 그 첫번째는 모형의존형(model-specific) 기술과 모형불문형(model-agnostic) 기술의 구분이다. 모형의존형 기술은 인공지능에 사용되는 예측 모형이 특수한 기술을 따를 때만 작동할 수 있는 모형 설명 방식이다. 예컨대, 영상/이미지 관련 인공지능에 자주 사용되는 CNN (Convolutional Neural Network) 알고리즘에서 작동하는 GRAD-CAM, Score CAM, Grad-CAM++ 등의 기술이 그것이다. 모형의존형 기술은 주예측모형의 작동 방식을 응용하여 만들어진 것이므로, 모형-맞춤형 설명이 가능하다는 장점이 있다. 그러나 이러한 모형들은 예측모형에 바로 응용할 수 없을 뿐만 아니라, 주모형과 직접 연동되어 작동하는 경우가 많아, 주모형의 속도를 느리게 하는 단점을 갖는 경우가 많다. 반면, 모형불문형 기술은 주모형과 독립적으로 설계되고 작동하기 때문에 범용성이 높고, 주모형의 퍼포먼스를 하락시키지 않는 경우가 많다. 이 때문에 최근이 개발 경향은 범용성 높은 모형불문형 기술을 개발하면서 설명력을 가능한 모형의존형 기술에 근접시키도록 하는 방향으로 발전하고 있다. 따라서, 이 장에서는 모형불문형 기술에 초점을 맞출 것이다.\n또 다른 구분으로는 전역적(global) 설명과 국소적(local) 설명 기술의 구분이 있다. 전역적 설명은 이용자가 복잡한 모형 전체의 구조를 이해할 수 있도록 단순화하는 방식을 의미하는 반면, 국소적 설명은 인공지능에 의해 내려진 특정한 판단의 이유를 제공하는 방식을 의미한다. 기계학습 모형을 이용하면서 이를 유지, 개선하기 위한 운용 목적이나, 보안상의 문제를 발생시킬 수 있는 외부 공격에 대해 강건한 모형을 만들기 위한 모니커링 목적으로는 전역적 설명이 자주 사용된다. 그에 반해, 인공지능이 내린 판단에 영향을 받는 일반 이용자들이 예컨대 자동화된 대출 결정을 받거나, 국경에서 보안 검색의 대상이 되었을 때, 그러한 결정에 대한 설명을 요구하거나 이의제기를 하려는 경우라면 모형 전체에 대한 설명은 일반인들에게 그다지 유용하지도 않을 것이며, 더 나아가 주어진 설명이 ’이해가능(interpretable)’하지 않을 수도 있다. 이러한 경우에는 특정 결정에 대한 국소적 설명이 더욱 유용할 수 있을 것이다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>설명 가능한 알고리즘</span>"
    ]
  },
  {
    "objectID": "xai.html#대표적인-xai-기술",
    "href": "xai.html#대표적인-xai-기술",
    "title": "12  설명 가능한 알고리즘",
    "section": "12.3 대표적인 XAI 기술",
    "text": "12.3 대표적인 XAI 기술\nXAI는 빠르게 발전하고 있는 분야이기 때문에, 지금도 새로운 기술이 계속해서 개발되고 있지만, 여기서는 대표적으로 알려진 몇 가지 기술을 중심으로 전반적인 개념을 이해하고자 한다. 최근에도 자주 인용되는 XAI 기술을 전역적, 국소적 설명의 구분 방식에 따라 일별해 보면 다음과 같다.\n\n\n\n분류\nXAI 기술\n특징\n\n\n\n\n전역설명\nPartial Dependence Plot (PDP)\n특성의 변화가 기계학습 모델의 예측 결과에 반영되는 영향을 그래프로 표시\n\n\nAccumulated Local Effect (ALE)\n특성의 변화가 ‘평균적으로’ 예측 결과에 반영되는 영향을 표현한 것으로 PDP와 유사하지만, 조금 더 빠르게 계산 가능\n\n\nPermutation Importance\n특성을 무작위로 재정렬해 모델 예측 오차가 증가하는 것을 관찰하는 방식으로 특성과 결과의 관계를 밝힘\n\n\nGlobal Surrogate\nAI모형의 예측 방식을 유사하게 학습한 해석가능한 모형으로 대리 설명. 의사결정 트리가 자주 사용됨\n\n\n국소설명\nLocal Surrogate (LIME)\n이미지나 텍스트를 포함한 다양한 데이터에 대해 임의의 판별 AI모형의 예측을 선형 근사로 설명.\n\n\nShapley Additive exPlanations (SHAP)\n각종 데이터에 대응하는 AI모형의 예측에 대해 특성의 공헌도를 게임이론적 지표를 이용해 설명.\n\n\n반사실적 설명\n현재의 판단 결과를 바꿀 수 있는 가장 작은 특성값의 변화를 이용해 현재 판단 결과의 ‘원인’을 설명.\n\n\n\n\n12.3.1 전역적 설명\n여기서는 많은 전역적 설명 방식 중, Permutation Importance 방식과 Global Surrogate에 대해 설명해 보도록 하겠다. Permutation Importance는 많은 전역적 설명 방식과 유사하게, 예측 결과를 도출하기 위한 근거가 되는 특성(feature)들의 상대적 중요도(importance)를 계산하는 것을 목표로 한다. 설명 방식은 다음과 같다. 가장 먼저, 중요도를 측정하기 위한 특성을 먼저 선택한다. 그 다음 다른 특성의 값들은 주어진 데이터 그대로 둔 채, 선택된 특성값만을 무작위로 재배열한다. 이를 그림으로 표현하면 다음과 같다.\n\n\n\n무작위 배열의 예\n\n\n모든 다른 특징량은 1, 2, 3 등 원데이터에 주어진대로 배열되어 있지만, 중요도를 측정하고자 하는 특징량 E는 무작위 배열되어 10, 94, 48와 같이 규칙을 갖지 않는 순서로 재배열되어 있다. 이제 이렇게 (특징량 E만) 변형된 데이터를 이용해 설명하고자 하는 모형을 추정한다. 모든 모형 추정은 ‘오류율’을 생산하는데, 이렇게 변형된 데이터를 바탕으로 한 모형의 오류율과, 변형하지 않은 원 데이터를 바탕으로 추정한 같은 모형의 오류율을 비교해보면 전자의 오류율이 클 것이다. 이는 위의 예에서 특징량E에 포함된 예측에 사용되어야 할 유용한 정보를 무작위 배열을 통해 인위적으로 무용하게 만든 것이나 다름 없기 때문에, 더 적은 정보(변형된 데이터)를 가지고 추정한 모형이 더 많은 정보(원데이터)를 이용해 추정한 모형보다 정확하지 않을 것이라는 것을 생각해보면 납득이 갈 것이다. 이제 변형 데이터로부터 도출된 오류율과 원데이터로부터 도출된 오류율의 비율, 즉, 특정 특징량을 무작위 배열함으로써 ’증간한 오류의 양이 얼마인가’를 측정하면, 반대로 그것은 해당 특성이 얼마나 예측에서 중요한 역할을 하고있었는가를 나타내는 지표가 된다. 이러한 과정을 하나의 특성(위의 예에서는 ’특징량 E’) 뿐만 아니라, 모든 특성에 대해 반복하면, 각 특성의 중요도를 파악할 수 있게 되는 것이다.\n이러한 과정을 통해, 다음과 같이 각 특성의 중요도를 시각화할 수 있다.\n\n\n\n특성별 중요도 표현\n\n\n위와 같은 시각화는 각각의 특성이 해당 모형의 예측에서 얼마만큼 중요한 역할을 하는지를 표현해주기에, 예측 모형이 아무리 복잡하더라도 그 모형이 작동하는 방식을 대체로 이해하는데 도움을 준다. 또한, 위의 중요도 계산 방식은 어떠한 예측 모형을 사용하는가와 전혀 관계없이 수행할 수 있다는 점에서, 모형불문형 기술이라고 할 수 있다. 더 나아가, 중요도 계산에 이용하는 오류율은 사용하는 모형, 데이터와 관계 없이 같은 단위(unit)를 가지기 때문에, 예측 모형 간의 비교 분석을 수행하는데에도 유용하다.\n물론 이러한 전역적 설명 방식은 고유한 한계를 갖는데, 다수의 다른 특성들 사이의 예측에 있어서의 의존관계를 파악하는데 도움을 주지 못하며, 각 특성이 특정 영역에서는 큰 중요도를 가지다가 다른 영역에서는 중요도가 떨어지는 등 ‘비선형적’ 중요도를 갖는 경우에도 이를 파악하는데 도움을 주지 못한다. 전역적 설명 방식은 본질적으로 복잡한 모형 전체를 단순화하여 표현하는 접근방식이기에 피하기 어려운 단점이다.\n또 다른 전역적 설명방식으로 Global Surrogate을 들 수 있다. 여기서 Surrogate이라고 함은 대리 모형, 즉, 복잡한 모형에 대한 조금 더 단순한 근사 모형을 의미한다. Global Surrogate이 사용하는 근사모형은 특정한 모형으로 미리 정해져 있지 않고, 근사모형의 이용자가 쉽게 이해할 수 있다고 믿는 한, 어떤 모형이라도 가능하다. Global Surrogate의 개략적인 아이디어를 그림으로 표현하면 다음과 같다.\n\n\n\nGlobal Surrogate의 간단한 예\n\n\n즉, 실제 모형은 예측의 정확도를 높이기 위해 0과 1을 구분하는 구불구불한 곡선이라면, 이를 이해하는 것이 쉽지 않으므로, 가능한 비슷한 예측결과를 만들어 낼 수 있는 선형 근사모형을 추정하여, 이를 이용해 모형 전체에 대략적인 설명을 한다는 아이디어이다. 물론, 대리 모형이 반드시 선형 모형일 필요는 없고, 위의 트레이드오프 그림에 등장하는 의사결정 트리와 같이 설명가능성이 높은 모형이라면 어떤 것이든 사용할 수 있다.\n이러한 설명 방식 역시 주 모형의 작동 방식을 알지 못하더라도 구축 가능하다는 점에서 큰 장점을 갖지만, 근사 모형이 주 모형으로부터 대단히 멀 수 있다는 단점이 있다는 것을 위 그림에서 즉각적으로 알 수 있다. 근사 모형을 주 모형과 가깝게 하기 위해서는 근사 모형 역시 유연하게 만들어야 할 것이고, 그렇게 되면, 결국에는 근사 모형 역시 설명이 어려워진다는 애초의 딜레마에 빠져들게 된다.\n\n\n12.3.2 국소적 설명\n이에 반해, 국소적 설명은 예측 모형 전체를 일반 사용자에게 이해시키기 위한 목적이라기 보다는, 인공지능이 해당 사용자에게 부여한 특정 예측, 또는 결정이 이루어진 이유를 제공하기 위한 목적을 가지고 있다. 이미 논의한 바와 같이, 일반 이용자가 쉽게 이해할 수 있을 정도로 전역적 설명 기술을 적용하기 위해서는 복잡한 모형을 지나치게 단순화해야 한다는 문제점이 있으므로, 해당 이용자가 관심을 갖는 특수한 사안으로 설명의 범위를 국한하는 국소적 설명 방식이 최근 각광받고 있다. 또한, 국소적 설명 방식은 기업의 특정 결정에 대해 설명을 요구할 권리를 보장하는 기존의 소비자 보호 법안의 취지에 잘 부합한다는 장점이 있다. 이 장에서는 최근 각광받는 국소적 설명 기술인 LIME (Local Interpretable Model-agnostic Explanations)와 반사실적(Counterfactual) 설명 방식에 대해 간단히 논의해 보도록 할 것이다.\nLIME은 Global Surrogate 방식과 유사하게 대리/근사 모형(Surrogat)을 이용한 설명 기술이다. 그러나 Global Surrogate과의 결정적인 차이는, 근사 모형을 특정 예측치 근방(locality 또는 neighborhood)에서 생성한다는 것이다. 이 점을 더 쉽게 이해하기 위해서 다음과 같은 분류 문제와 근사 모형을 생각해 보자.\n\n\n\n국소적 근사모형\n\n\n여기서 회색과 붉은색으로 표시된 영역은 예측을 위한 기계학습 모형(주모형)이 -1(회색) 또는 +1(붉은색)으로 예측하는 구간을 의미한다. 두 영역의 경계를 보면 알 수 있듯이 그 경계는 매우 비선형적이다. 이러한 비선형적인 모형이 만들어낸 예측결과는 ’특징량 1과 특징량 2의 수치가 어떠어떠하기 때문에 +1 또는 -1로 예측하였다’와 같은 방식으로 설명하기 어렵다. 이 때문에, 이 불규칙해 보이는 경계 전체를 선형적인 모형으로 근사한다면(즉, Global Surrogate을 도입한다면), 지나친 단순화로 인해 좋은 근사모형을 만들어낼 가능성이 낮다.\n때문에, LIME은 전체 모형에 대한 근사 모형을 포기하고, 특정 위치에서의 근사 모형만을 생성한다. 예컨대 위의 그림에서 별표에 해당하는 어떤 예측치가 있었다고 하자. 이 예측치는 회색 영역에 위치해 있으므로, -1이라는 값을 가질 것이다. 좀 더 쉬운 이해를 위해 구체적인 시나리오를 도입하자면, 위의 그림이 소득(특징량1)과 신용점수(특징량2)의 특정한 조합(별표) 때문에 해당 소비자는 신용카드 발급이 자동적으로 거절(-1)된 경우를 시각화 하고 있다고 생각해보자. 이 때, 해당 소비자는 본인에게 불리한 판정(더 정확하게는 신용 위험이 높다는 ‘예측’)이 이루어졌으므로, 그 이유를 알려달라고 요구할 수 있을 것이다. 이 때, LIME은 근사 모형을 형성하되, 해당 소비자가 위치한 그 근방에서 예측 모형을 생성하여 해당 판단의 근거를 설명하자는 접근이다. 이 근사 모형은 위의 그림에서 점선으로 표현되어 있다. 해당 점선은 전체적인 불규칙한 예측 경계에 대해서는 (즉, Global Surrogate으로서는) 나쁜 근사모형이지만, 별표로 표시된 특정 예측치 ‘근방에서만큼은’ 상당히 좋은 근사모형이라는 것을 알 수 있다. LIME을 수행하는 구체적인 알고리즘은 다음과 같다.\n\n첫째, 설명 대상 데이터(별표)에 무작위 오류를 더하여, 설명 대상 데이터 근방의 가상 데이터를 만들어낸다 (이는 위의 그림에서 점들의 ’위치’로 표현되어 있다). 이를 기술적으로는 설명 대상 데이터를 섭동(perturbation)한다고 한다.\n둘째, 가상 데이터를 ’주모형’에 삽입하여 가상 데이터에 대한 AI 모형의 예측 결과를 얻는다 (위의 그림에서 예측 결과는 가상 데이터의 색깔에 해당한다).\n셋째, 가상 데이터와 그에 대한 예측결과를 이용하여, 비교적 단순한 근사모형을 만든다.\n\n이러한 방식을 이미지에 분류 문제에 적용하면, 다음과 같은 재미있는 결과를 얻을 수 있다.\n\n\n\n늑대로 잘못 분류된 허스키 사진과 그 이유\n\n\n이는 LIME을 처음 제시한 Ribeiro 등의 논문(2016)에 수록된 것으로, 특정 오류에 대해서 근사모형을 만들어 보니, 허스키와 늑대를 구분하는 경계가 배경에 해당하는 영역이었다는 것을 알 수 있었다는 예시이다. 즉, 분류할 대상인 동물 이미지의 형태에 따라 허스키와 늑대를 오해한 것이 아니라, 배경의 눈 때문에 늑대라고 생각했다는 것이다. 이는 예측에 이용된 모형이 시베리안 허스키와 늑대을 구분하기 위해 일반적으로 받아들일 수 있을만한 기준에 따라 분류를 생산하고 있지 않기 때문에, 해당 이미지에 대해서 잘못된 판단을 내렸다는 것을 이해하기 쉽게 보여준다.\n마찬가지의 오류가 대출의 상환 능력에 대한 인공지능의 판단에서도 이루어졌다고 생각해 보자. 대출 신청자가 인공지능에 의해 내려진 대출 거절 결정에 대해 이의를 제기했을 때, LIME을 이용한 검증 결과 신청자의 거주지, 또는 성별과 같이 합리적으로 받아들일 수 없는 결정 근거를 이용했거나 차별적 대우로 이해할 수 있는 결정이었음이 드러난다면, LIME의 결과는 신청자가 인공지능의 결정에 대해 재고를 요청할 수 있는 근거를 제공할 수 있을 것이다. 즉, 이전의 장에서 우리가 논의했던 인공지능의 ’불공정성’을 입증할 때도 국소적 설명방식은 유용하게 사용될 수 있는 것이다. 단, 염두에 두어야 할 점dms LIME의 이러한 설명이 모형 전체가 그러한 오류를 가지고 있다는 것을 의미하지는 않는다는 것이다. LIME이 국소적 설명방식인 이상, 특정 오류에 대한 설명을 제공할 뿐이다.\n마지막으로 반사실적 설명방식에 대해서 알아보자. 반사실적 설명은 논리학에서 사용하는 반사실적 조건문(counterfactual conditional)을 이용한 인과성의 이해방식에 근거한다. 반사실적 조건문은 최근 과학적 ’인과성’에 관한 논의에서 가장 중요하게 여겨지고 있는 개념으로, “X라는 사건이 일어나지 않았다면 사건 Y는 발생하지 않았을 것이다”와 같은 진술문을 의미한다. 예컨대, 뜨거운 커피를 마신 후 혀에 화상을 입었다고 해 보자. 직관적으로 뜨거운 커피를 마신 시간적으로 선행한 사건이 혀에 화상을 입은 후행한 ’결과’의 ’원인’인 것 같지만, 잘 생각해보면 꼭 그런 것은 아닐 수도 있다. 뜨거운 커피를 마시기 전후에 다른 뜨거운 음식을 먹었을 수도 있고, 두 사건 사이의 시간 간격이 대단히 먼 경우도 있을 수 있다. 그렇다면, 언제 뜨거운 커피를 마신 것이 혀에 화상을 입은 것의 인과적 원인이라고 할 수 있을까? 반사실적 조건문을 통한 인과성을 옹호하는 사람들은 다음과 같은 진술문이 가능하다면, 뜨거운 커피를 마신 것이 혀에 화상을 입은 사건의 원인이라고 말할 수 있다고 본다.\n\n“만약 내가 뜨거운 커피를 한 모금도 마시지 않았다면, 나는 내 혀에 화상을 입지 않았을 것이다”\n\n이 문장에서 ‘뜨거운 커피를 한 모금도 마시지 않았다면’ 이라고 하는 조건은 뜨거운 이미 커피를 마신 현실(사실)과 반대되는 상황, 즉, 반사실적 조건이다. 만약, 그러한 반사실적 조건 하에서는 혀에 화상을 입 ’결과’를 경험하지 않았을 것이라면, 반대로, 뜨거운 커피를 마신 ’사실’은 혀에 화상을 입은 ’원인’이 된다는 것이다. 즉, 반사실적 설명은 어떠한 ’결과’의 ’원인’을 찾을 때, 지금의 결과를 얻지 않기 위해서는 어떠한 ’반사실’을 가져야 하는가라는 물음으로 부터 사실에서의 ’원인’을 찾는 접근법이다.\n이러한 인과성에 대한 이해가 ‘인과성’이라고 하는 관념을 온전히 담아내는지에 대해서는 매우 활발한 논의가 일어나고 있다.1 그 타당성을 논하는 것은 이 교재의 논의 범위를 벗어나므로, 지금은 이러한 방식이 상당히 우리가 가지고 있는 ’인과’ 또는 ’원인과 결과’에 대한 직관적인 이해에서 크게 벗어나지 않는다는 점, 그러한 이유로 반사실적 설명은 인과성을 통계적으로 찾아내는 방법론을 개발하는데 있어 중추적인 역할을 하고 있다는 점을 받아들이고 지나가도록 하자.\n반사실적 설명방식의 유용성을 받아들인다면, 인공지능의 특정 판단이라는 ‘결과의 원인을 설명’하기 위해서, 우리가 알아야 하는 것은 인공지능이 이미 내린 결정과 다른 결정이 일어났을 ’반사실적 조건’이 무엇인지 일것이다. 그러나 이는 쉬운 일이 아니다. ’반사실적 조건’은 그 정의상 실제로 일어나지 않은 일, 그리고 현실의 결과와는 다른 결과를 얻게되는 일종의 ’평행세계’을 의미하기 때문이다. 우리가 계속해서 사용하고 있는 인공지능에 의해 대출이 거절된 결과와는 다른 결과, 즉, 대출이 승인된 결과를 얻기 위해서는 우리는 어떤 ’평행세계’ 속에 있어야 하는가? 반사실적 설명방식은 이러한 질문으로 인공지능의 판단에 대한 국소적 설명을 대신한다. 그러나, 타임머신이 없는 이상 그러한 반사실을 알아낼 수는 없다. 그렇다면, 반사실적 설명 방식은 어떻게 XAI에 응용될 수 있다는 것일까?\nXAI에서 사용되는 반사실적 설명방식은 평행세계로 봐도 좋을만한 평행세계의 근사치를 계산해 내는 것이다. 이러한 근사치는 ’지금의 결과와는 다른 결과를 얻었을 최소으로 다른 조건’을 의미한다. 즉, 다시 대출의 예로 돌아가보자면, 대출이 거절된 결과를 대출이 승인되는 결과로 바꾸는 ’최소한의 조건 변화’를 찾아내는 것이다. 이를 그림으로 시각적으로 표현하면 다음과 같다.\n\n\n\n반사실적 설명의 예\n\n\n위 그림의 (a)에서 보는 것은 인공신경망 알고리즘이 7이라는 숫자를 9라는 숫자로 새로 분류하기 위한 최소한의 변화, 즉, 반사실적 조건을 보여준다. 여기서 자세히 논하지는 않겠지만, 이러한 반사실적 조건의 근사치를 찾아내는 것은 인공지능의 자동화된 결정의 근거가 되는 특성이 최소한으로 변한다는 ‘제약하에서’ 현실에서의 결과와 평행세계에서 얻었으면 하는 결과 사이의 거리를 좁히는 일종의 ‘제약하의 최적화(constrained optimization)’ 알고리즘으로 표현할 수 있다 (Wachter et al., 2017). 더 비근한 예로, 위의 그림에서 (b)는 대출거절(또는 높은 금융 위험)을 대출승인(낮은 금융 위험)이라는 결정으로 바꾸기 위해서는 현재의 조건에서 최소한으로 바뀌어야 하는 반사실적 조건인지, 그 근사치를 찾은 결과를 표현한다. 예컨대, ‘당신은 만약 지금과 달리 결혼을 했거나, 현재보다 연소득이 5,000달러 많았다면 대출승인이 되었을 것입니다’ 라고 알려주는 것이다.\n이러한 설명방식은 아마도 소비자가 받아들일 수 있는 인공지능의 결정에 대한 가장 쉬운 설명 방식일 것이다. 특히 LIME이 제공하는 ’당신의 위치에서 우리의 근사 모형은 이러한 함수입니다’라는 식의 설명만 하더라도, 소비자에게는 그다지 직관적으로 다가오지 않을 공산이 크다. 그렇다면, 만약 인공지능의 자동화된 결정으로부터 소비자를 보호하는 법안이 소비자에게 ’충분한 설명’을 제공하는 것이라고 한다면, LIME이 제공하는 설명 역시 전역적 설명보다는 나을지 몰라도, ’충분한’이라는 표현에 적합할지는 논란의 여지가 있을 것이다. 반면, 반사실적 설명은 이론의 여지 없이 직관적 설명이라는 점에서, 대단한 강점을 가지고 있다.\n물론, 그렇다고 반사실적 설명이 능사는 아니다. 무엇보다, 반사실적 설명을 제공하는 XAI는 반사실적 조건 또는 평행세계의 ‘근사치’를 찾는 것이지, 평행세계 그 자체를 보여주는 것이 아니라는 점을 유념해야 한다. 따라서, 그 설명이 정말 인과적인 설명인지를 따지는 데에는 여러 논란의 여지가 있을 것이며, 그러한 설명이 특정 법적 의무를 다하기 위한 것이라면, 그러한 부정확함은 더더욱 문제가 된다. 더군다나, 근사치를 만들어 내는 알고리즘에는 여러가지가 있고, 여전히 새로운 방법이 만들어지고 있다는 문제가 있다. 이렇게 여러개의 알고리즘이 존재한다면, 우리는 여러개의 평행세계의 근사치 후보를 가지게 될 것이며, 그 중 어느 것이 더 나은 근사치인지 알 수 없는 문제가 있다. 마지막으로, 반사실적 설명 방식이 제공하는 특정 결정에 대한 직관적인 설명은 인간의 인공지능 작동 방식에 대한 이해를 높인다고 볼 수 없다. 따라서, 소비자 보호 규제의 목적이 인간의 이해를 늘리는 일종의 ’교육’ 목적을 가지고 있다면, 반사실적 설명 방식은 그러한 목적에 적합한 XAI 기술이라고 볼 수는 없겠다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>설명 가능한 알고리즘</span>"
    ]
  },
  {
    "objectID": "xai.html#설명가능성과-법안과의-연관성",
    "href": "xai.html#설명가능성과-법안과의-연관성",
    "title": "12  설명 가능한 알고리즘",
    "section": "12.4 설명가능성과 법안과의 연관성",
    "text": "12.4 설명가능성과 법안과의 연관성\nXAI에 대한 설명을 마무리하기에 앞서, 앞선 논의가 반사실적 설명 방식 그 자체의 장·단점이라기 보다는 제도가 요구하는 바와의 ’적합성’이라는 논의로 끝났다는 점을 강조해야 하겠다. 즉, 지금까지 논의한 다양한 XAI 기술은 공학자의 시각에서 그 자체 성능의 우열이 있을 수 있으나, 인간과 인공지능이 공존하는 사회를 고민하는 입장에서는 우리가 원하는 바와의 ’적합성’이라는 차원에서 평가해야 한다는 것을 반드시 유념해야 한다.\n그런데, 인공지능을 이용한 서비스를 제공하는 기업에게 이러저러한 XAI 기술을 함께 제공할 것을 요구함에 앞서, 우리는 어떠한 경우에 어떠한 설명을 법적으로 요구하고 있는지, 또 요구해야 하는지에 대해서 여전히 분명하게 알고있지 않다. 예컨대, 최근 인공지능의 투명성을 높이기 위한 알고리즘 사전 등록제와 같은 아이디어가 종종 언급되고 있는데, 이때, 미리 등록되어야 하는 알고리즘의 작동 방식에 대한 ’설명’은 아마도 국소적 설명과는 거리가 있을 것이다. 그렇다면, 국소적 설명과는 다른 어떠한 설명이 필요한 것일까? 예컨대, 인공지능의 특정 판단으로 인해 피해에 대한 책임을 사후에 묻기 위해 인공지능이 ’어떻게 작동해야 하는가’를 미리 명문화 하는 것이 사전 등록의 목적이라면, 인공지능의 작동 방식에 대한 ’대중의 이해도’를 높이기 위한 목적인 경우에 비해 훨씬 더 상세한 설명을 필요로 할 것이다. 여러 정치적인 타협에 의해 명문화된 법안이 어떠한 목적을 달성하고자 하는가를 판단하는 것은 간단한 문제가 아니기 때문에 (미국 수정헌법에서 ’민병대’와 관련된 조항들의 ’목적’에 대한 해석에 관해 총기규제와 관련하여 수많은 논쟁이 있었다는 점을 떠올려보자), 기술과 법 사이에서 연구를 수행하고 있는 많은 법학자, 사회과학자들은 이미 수많은 논의들을 쏟아내고 있다.\n또 하나, 반드시 이해해야 할 것은 XAI는 단지 인공지능을 규제하기 위해서만 사용되지는 않는다는 것이다. 대규모언어모형(LLM)의 발전에 따라, 인공지능과 인간의 연속적인 대화가 일상 서비스에서 가능해진 이상, XAI 기술은 인간과 인공지능간의 소통, 더 나아가 협업을 위해서 반드시 필요한 분야이기도 하다. XAI가 작동할 때, 인간은 인공지능이 한 번 내린 결과를 살펴보고 받아들일지, 받아들일지를 결정하는 것이 아니라, ‘왜 그러한 결정을 내렸는지’ 인공지능에게 합리적 설명(reasoning)을 요구하고, 그것이 받아들일만한 설명이 아닐 때, 새로운 판단 근거에 따른 새로운 결정을 요구해보는 반복적인 상호작용을 통해 더욱 합리적인 결정으로 이동해갈 수 있게 될 가능성이 있는 것이다.\n아직 불확실한 분야이지만, 아주 가능성이 큰 분야이기에, 우리가 원하는 협업과 규제에 적합한 XAI, 그리고 그러한 기술과 인간이 원하는 것 사시의 일치와 불일지, 어떻게 기술적으로, 제도적으로, 사회적으로 불일치들을 메꾸어야 할지에 대해서 주목해야할 기회가 넘치는 분야이기도 하다.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>설명 가능한 알고리즘</span>"
    ]
  },
  {
    "objectID": "xai.html#더-읽을거리",
    "href": "xai.html#더-읽을거리",
    "title": "12  설명 가능한 알고리즘",
    "section": "12.5 더 읽을거리",
    "text": "12.5 더 읽을거리\n\n고학수·김용대·윤성로·김정훈·이선구·박도현·김시원 (2021). &lt;인공지능 원론&gt;. 서울: 박영사.\n나카무로 마키코, 쓰가와 유스케 (2018). &lt;원인과 결과의 경제학: 넘치는 데이터 속에서 진짜 의미를 찾아내는 법&gt;, 리더스북.\n안재현 (2020). &lt;XAI 설명 가능한 인공지능, 인공지능을 해부하다&gt;, 위키북스.\n오오쓰보 나오키 외 (2022). &lt;XAI, 설명 가능한 AI&gt;. 비제이퍼블릭. 인터넷 교재\nMolnar, C. (2023). Interpretable machine learning: A guide for making black box models explainable. https://christophm.github.io/interpretable-ml-book/",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>설명 가능한 알고리즘</span>"
    ]
  },
  {
    "objectID": "xai.html#참고문헌",
    "href": "xai.html#참고문헌",
    "title": "12  설명 가능한 알고리즘",
    "section": "12.6 참고문헌",
    "text": "12.6 참고문헌\nEdwards, L., & Veale, M. (2017). Slave to the algorithm? Why a’right to an explanation’is probably not the remedy you are looking for. Duke L. & Tech. Rev., 16, 18.\nRibeiro, M. T., Singh, S., & Guestrin, C. (2016, August). ” Why should I trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144).\nWachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech., 31, 841.\nZhang, S., Chen, X., Wen, S., & Li, Z. (2023). Density-based reliable and robust explainer for counterfactual explanation. Expert Systems with Applications, 226, 120214.",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>설명 가능한 알고리즘</span>"
    ]
  },
  {
    "objectID": "xai.html#footnotes",
    "href": "xai.html#footnotes",
    "title": "12  설명 가능한 알고리즘",
    "section": "",
    "text": "반사실적 인과성을 주제로 한 책이 아직 한국에 많이 번역되어 있지는 않으나, 일본인 연구자 나카무로 마키코, 쓰가와 유스케가 쓴 &lt;원인과 결과의 경제학: 넘치는 데이터 속에서 진짜 의미를 찾아내는 법&gt;을 참조할만 하다.↩︎",
    "crumbs": [
      "4부: 인공지능 윤리의 기술적 접근",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>설명 가능한 알고리즘</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barocas, Solon, and Andrew D Selbst. 2016. “Big Data’s Disparate\nImpact.” California Law Review, 671–732.\n\n\nDwork, Cynthia, Moritz Hardt, Toniann Pitassi, Omer Reingold, and\nRichard Zemel. 2012. “Fairness Through Awareness.” In\nProceedings of the 3rd Innovations in Theoretical Computer Science\nConference, 214–26.",
    "crumbs": [
      "References"
    ]
  }
]